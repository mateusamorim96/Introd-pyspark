[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to pyspark",
    "section": "",
    "text": "Hello! This is the initial page!"
  },
  {
    "objectID": "Chapters/01-intro.html",
    "href": "Chapters/01-intro.html",
    "title": "2  Preface",
    "section": "",
    "text": "In essence, pyspark is a python package that provides an API for Apache Spark. In other words, with pyspark you are able to use the python language to write Spark applications and run them on a Spark cluster in a scalable and elegant way. This book focus on teaching the fundamentals of pyspark, and how to use it for big data analysis.\nThis book, also contains a small introduction to key python concepts that are important to understand how pyspark is organized and how it works in practice, and, since we will be using Spark under the hood, is very important to understand a little bit of how Spark works, so, we provide a small introduction to Spark as well.\nBig part of the knowledge exposed here is extracted from a lot of practical experience of the author, working with pyspark to analyze big data at platforms such as Databricks1."
  },
  {
    "objectID": "Chapters/01-intro.html#some-conventions-of-this-book",
    "href": "Chapters/01-intro.html#some-conventions-of-this-book",
    "title": "2  Preface",
    "section": "2.2 Some conventions of this book",
    "text": "2.2 Some conventions of this book\n\n2.2.1 Python code and terminal commands\nThis book is about pyspark, which is a python package. As a result, we will be exposing a lot of python code across the entire book. Examples of python code, are always shown inside a gray rectangle, like this example below.\nEvery visible result that this python code produce, will be shown outside of the gray rectangle, just below the command that produced that visible result. Besides that, every line of result will always be written in plain black. So in the example below, the value 729 is the only visible result of this python code, and, the statement print(y) is the command that triggered this visible result.\n\nx = 3\ny = 9 ** x\n\nprint(y)\n\n729\n\n\nFurthermore, all terminal commands that we expose in this book, will always be: pre-fixed by Terminal$; written in black; and, not outlined by a gray rectangle. In the example below, we can easily detect that this command pip install jupyter should be inserted in the terminal of the OS (whatever is the terminal that your OS uses), and not in the python interpreter, because this command is prefixed with Terminal$.\nTerminal$ pip install jupyter\nSome terminal commands may produce visible results as well. In that case, these results will be right below the respective command, and will not be pre-fixed with Terminal$. For example, we can see below that the command echo \"Hello!\" produces the result \"Hello!\".\nTerminal$ echo \"Hello!\"\n\"Hello!\"\n\n\n2.2.2 Python objects, functions and methods\nWhen I refer to some python object, function, method or package, I will use a monospaced font. In other words, if I have a python object called “name”, and, I am describing this object, I will use name in the paragraph, and not “name”. The same logic applies to functions, methods and package names.\n\n\n2.2.3 Be aware of differences between OS’s!\nSpark is available for all three main operational systems (or OS’s) used in the world (Windows, MacOs and Linux). I will use constantly the word OS as an abbreviation to “operational system”.\nThe snippets of python code shown throughout this book should just run correctly no matter which one of the three OS’s you are using. In other words, the python code snippets are made to be portable. So you can just copy and paste them to your computer, no matter which OS you are using.\nBut, at some points, I may need to show you some terminal commands that are OS specific, and are not easily portable. For example, Linux have a package manager, but Windows does not have one. This means that, if you are on Linux, you will need to use some terminal commands to install some necessary programs (like python). In contrast, if you are on Windows, you will generally download executable files (.exe) that make this installation for you.\nIn cases like this, I will always point out the specific OS of each one of the commands, or, I will describe the necessary steps to be made on each one the OS’s. Just be aware that these differences exists between the OS’s."
  },
  {
    "objectID": "Chapters/01-intro.html#sec-open-terminal",
    "href": "Chapters/01-intro.html#sec-open-terminal",
    "title": "2  Preface",
    "section": "2.3 Opening the terminal of your OS",
    "text": "2.3 Opening the terminal of your OS\nEvery OS comes with a terminal (or command prompt), and you will find this tool very useful for a number of things. I will expose many terminal commands in this book, and these commands should be used inside these terminals that comes with your OS.\nThe next sub-sections will show you how to open a terminal in each one of the OS’s where Spark is available. Lets begin with Windows.\n\n2.3.1 Opening a terminal on Windows\nThere are some different approaches to do this, but, the one that I find the most useful, is to open a terminal from inside a Windows folder, using the default File Explorer program of Windows. When we use the terminal, we usually want to affect or use a file that is stored inside a folder in our computer, with a terminal command.\nBecause of that, when we open a terminal from inside a folder, the terminal opened is already rooted inside the folder where our file is stored. In other words, we already have easy access to the file that we want to affect/use in our command, and we do not have the work to change or adjust directories in this terminal.\nFor example, lets suppose that we want to use a terminal command to use a file called hello.py, and, that this hello.py file is stored inside a folder called HelloPython. You can see in Figure 2.1, that this folder is in path C:\\Users\\pedro\\Documents\\HelloPython. So, the first step, is to use the File Explorer of Windows, to open this folder, like in Figure 2.1.\n\n\n\nFigure 2.1: Opening the HelloPython folder in Windows\n\n\nAfter you opened this folder, substitute the path to the folder in the search box with the word “cmd”, like in Figure 2.2, and them, press Enter in the keyboard.\n\n\n\nFigure 2.2: Opening a terminal inside a Windows folder - Part 1\n\n\nAs a result, a new terminal will open. See in Figure 2.3, that this new terminal is already looking to (or is already rooted on) this HelloPython folder. This way, we can easily access the files stored inside this folder, like the hello.py file.\n\n\n\nFigure 2.3: Opening a terminal inside a Windows folder - Part 2\n\n\n\n\n2.3.2 Opening a terminal on Linux\nIs fairly easy to open a terminal on a Linux distribution. Again, is very useful when you open the terminal from inside the folder you are interested in. Because you will have an easier access to all the files that are stored inside this folder.\nTo do this in Linux, you use the built-in File Explorer to open the folder where you want to root your terminal. At the moment, I am using an Ubuntu distribution. I just opened the same HelloPython folder, with the same hello.py file, in the File Explorer of Linux. As shown in Figure 2.4:\n\n\n\nFigure 2.4: Opening the HelloPython folder in Linux\n\n\nAfter you opened the folder, just click with the right button of your mouse, and select the “Open in Terminal” option, and a new terminal should appear on your screen. See in Figure 2.5, that the new terminal is already looking to the HelloPython folder, as we expected.\n\n\n\nFigure 2.5: Opening the terminal in Linux\n\n\n\n\n2.3.3 Opening a terminal in MacOs\nUnfortunately, I do not have a Mac machine in my possession, so I cannot easily show you how to open a terminal in MacOs. But there a lot of articles available in the internet discussing how to open a terminal in MacOs. For example, there is a article from the support of Apple2, or this other article from iDownloadBlog3."
  },
  {
    "objectID": "Chapters/01-intro.html#sec-install-software",
    "href": "Chapters/01-intro.html#sec-install-software",
    "title": "2  Preface",
    "section": "2.4 First step: install the necessary software",
    "text": "2.4 First step: install the necessary software\nBefore we start, is really important that you have all the software you need already installed on your machine and ready to go. Spark and, as a result, pyspark, are both available for Windows and UNIX-like systems (e.g. Linux and MacOs). To use pyspark you need to install: 1) Spark; 2) Python; 3) and the python package called pyspark.\nInstall and manage Spark in Windows is a little harder than in a UNIX-like machine. But it’s perfectly ok if you do not have a UNIX machine. A Windows machine will do it just fine. Just for referencing, the examples showed throughout this book were executed in a Linux machine (more specifically, in Ubuntu).\n\n2.4.1 Install python\nTo install python on Windows, you can download the executable files at the official Python website4. But, if you are on Linux, you can install python with the package manager, by opening a terminal and running the following command:\nsudo apt-get install python3\n\n\n2.4.2 Install pyspark\nInstalling the pyspark python package is pretty straightforward. Just open a terminal (if you need help to open the terminal check Section 2.3), and use the pip command to do it:\nTerminal$ pip install pyspark\nIf you try to run the above command (inside a terminal of any OS), and a message like pip: command not found appears, this means that you do not have the pip tool installed on your machine. Hence, if you face this kind of message, you need to install pip before you even install pyspark.\nThe pip tool is automatically installed with Python on Windows. So, if you face this message (pip: command not found), then, is very likely that you do not have Python correctly installed on your machine. Or maybe, Python is not installed at all in any shape or size in your system. So, you should comeback to previous section, and re install it.\nIn UNIX-like systems, installing pip is very easy, because you can use the built-in package manager to do this for you. In Debian like distros (e.g. Ubuntu), you use the apt tool, and, in Arch-Linux like distros (e.g. Arch Linux, Manjaro) you would use the pacman tool. Both possibilities are exposed below:\n# If you are in a Debian like distro of Linux\n# and need to install `pip`, use this command:\nTerminal$ apt install python3-pip\n# If you are in a Arch-Linux like distro of Linux\n# and need to install `pip`, use this command:\nTerminal$ pacman -S python-pip\nBut, if you are running in MacOs, is best to use the get-pip.py method to do install pip5. To use this method, try to run the command below on the terminal. After you installed pip, run the same pip command we showed earlier to install pyspark.\n# To install `pip` on a MacOs:\nTerminal$ curl https://bootstrap.pypa.io/get-pip.py | python3\nAfter you correctly installed pip, try to execute the first command again, to install the pyspark package on your machine.\n\n\n2.4.3 Install Spark"
  },
  {
    "objectID": "Chapters/02-python.html",
    "href": "Chapters/02-python.html",
    "title": "3  Key concepts of python",
    "section": "",
    "text": "If you have experience with python, and understands how objects and classes works, you might want to skip this entire chapter. But, if you are new to the language and do not have much experience with it, you might want to stick a little bit, and learn a few key concepts that will help you to understand how the pyspark package is organized, and how to work with it."
  },
  {
    "objectID": "Chapters/02-python.html#scripts",
    "href": "Chapters/02-python.html#scripts",
    "title": "3  Key concepts of python",
    "section": "3.2 Scripts",
    "text": "3.2 Scripts\nPython programs are written in plain text files that are saved with the .py extension. After you save these files, they are usually called “scripts”. So a script is just a text file that contains all the commands that make your python program.\nThere are many IDEs or programs that help you to write, manage, run and organize this kind of files (like Microsoft Visual Studio Code1, PyCharm2, Anaconda3 and RStudio4). But, if you do not have any of them installed, you can just create a new plain text file from the built-in Notepad program of your OS (operational system), and, save it with the .py extension."
  },
  {
    "objectID": "Chapters/02-python.html#how-to-run-a-python-program",
    "href": "Chapters/02-python.html#how-to-run-a-python-program",
    "title": "3  Key concepts of python",
    "section": "3.3 How to run a python program",
    "text": "3.3 How to run a python program\nAs you learn to write your Spark applications with pyspark, at some point, you will want to actually execute this pyspark program, to see its result. To do so, you need to execute it as a python program. There are many ways to run a python program, but I will show you the more “standard” way. That is to use the python command inside the terminal of your OS (you need to have python already installed).\nAs an example, lets create a simple “Hello world” program. First, open a new text file then save it somewhere in your machine (with the name hello.py). Remember to save the file with the .py extension. Then copy and paste the following command into this file:\n\nprint(\"Hello World!\")\n\nIt will be much easier to run this script, if you open the terminal inside the folder where you save the hello.py file. If you do not know how to do this, look at section Section 2.3. After you opened the terminal inside the folder, just run the python hello.py command. As a result, python will execute hello.py, and, the text Hello World! should be printed to the terminal:\nTerminal$ python hello.py\nHello World!\nBut, if for some reason you could not open the terminal inside the folder, just open a terminal (in any way you can), then, use the cd command (stands for “change directory”) with the path to the folder where you saved hello.py. This way, your terminal will be rooted in this folder.\nFor example, if I saved hello.py inside my Documents folder, the path to this folder in Windows would be something like this: \"C:\\Users\\pedro\\Documents\". On the other hand, this path on Linux would be something like \"/usr/pedro/Documents\". So the command to change to this directory would be:\n# On Windows:\nTerminal$ cd \"C:\\Users\\pedro\\Documents\"\n# On Linux:\nTerminal$ cd \"/usr/pedro/Documents\"\nAfter this cd command, you can run the python hello.py command in the terminal, and get the exact same result of the previous example.\nThere you have it! So every time you need to run your python program (or your pyspark program), just open a terminal and run the command python <complete path to your script>. If the terminal is rooted on the folder where you saved your script, you can just use the python <name of the script> command."
  },
  {
    "objectID": "Chapters/02-python.html#objects",
    "href": "Chapters/02-python.html#objects",
    "title": "3  Key concepts of python",
    "section": "3.4 Objects",
    "text": "3.4 Objects\nAlthough python is a general-purpose language, most of its features are focused on object-oriented programming. Meaning that, python is a programming language focused on creating, managing and modifying objects and classes of objects.\nSo, when you work with python, you are basically applying many operations and functions over a set of objects. In essence, an object in python, is a name that refers to a set of data. This data can be anything that you computer can store (or represent).\nHaving that in mind, an object is just a name, and this name is a reference, or a key to access some data. To define an object in python, you must use the assignment operator, which is the equal sign (=). In the example below, we are defining, or, creating an object called x, and it stores the value 10. Therefore, with the name x we can access this value of 10.\n\nx = 10\nprint(x)\n\n10\n\n\nWhen we store a value inside an object, we can easily reuse this value in multiple operations or expressions:\n\n# Multiply by 2\nprint(x * 2)\n\n20\n\n\n\n# Divide by 3\nprint(x / 3)\n\n3.3333333333333335\n\n\n\n# Print its class\nprint(type(x))\n\n<class 'int'>\n\n\nRemember, an object can store any type of value, or any type of data. For example, it can store a single string, like the object salutation below:\n\nsalutation = \"Hello! My name is Pedro\"\n\nOr, a list of multiple strings:\n\nnames = [\n  \"Anne\", \"Vanse\", \"Elliot\",\n  \"Carlyle\", \"Ed\", \"Memphis\"\n]\n\nprint(names)\n\n['Anne', 'Vanse', 'Elliot', 'Carlyle', 'Ed', 'Memphis']\n\n\nOr a dict containing the description of a product:\n\nproduct = {\n  'name': 'Coca Cola',\n  'volume': '2 litters',\n  'price': 2.52,\n  'group': 'non-alcoholic drinks',\n  'department': 'drinks'\n}\n\nprint(product)\n\n{'name': 'Coca Cola', 'volume': '2 litters', 'price': 2.52, 'group': 'non-alcoholic drinks', 'department': 'drinks'}\n\n\nAnd many other things…"
  },
  {
    "objectID": "Chapters/02-python.html#expressions",
    "href": "Chapters/02-python.html#expressions",
    "title": "3  Key concepts of python",
    "section": "3.5 Expressions",
    "text": "3.5 Expressions\nPython programs are organized in blocks of expressions (or statements). A python expression is a statement that describes an operation to be performed by the program. For example, the expression below describes the sum between 3 and 5.\n\n3 + 5\n\n8\n\n\nThe expression above is composed of numbers (like 3 and 5) and a operator, more specifically, the sum operator (+). But any python expression can include a multitude of different items. It can be composed of functions (like print(), map() and str()), constant strings (like \"Hello World!\"), logical operators (like !=, <, > and ==), arithmetic operators (like *, /, **, %, - and +), structures (like lists, arrays and dicts) and many other types of commands.\nBelow we have a more complex example, that contains the def keyword (which starts a function definition; in the example below, this new function being defined is double()), many built-in functions (list(), map() and print()), a arithmetic operator (*), numbers and a list (initiated by the pair of brackets - []).\n\ndef double(x):\n  return x * 2\n  \nprint(list(map(double, [4, 2, 6, 1])))\n\n[8, 4, 12, 2]\n\n\nPython expressions are evaluated in a sequential manner (from top to bottom of your python file). In other words, python runs the first expression in the top of your file, them, goes to the second expression, and runs it, them goes to the third expression, and runs it, and goes on and on in that way, until it hits the end of the file. So, in the example above, python executes the function definition (initiated at def double(x):), before it executes the print() statement, because the print statement is below the function definition.\nThis order of evaluation is commonly referred as “control flow” in many programming languages. Sometimes, this order can be a fundamental part of the python program. Meaning that, sometimes, if we change the order of the expressions in the program, we can produce unexpected results (like an error), or change the results produced by the program.\nAs an example, the program below prints the result 4, because the print statement is executed before the expression x = 40.\n\nx = 1\n\nprint(x * 4)\n\nx = 40\n\n4\n\n\nBut, if we execute the expression x = 40 before the print statement, we then change the result produced by the program.\n\nx = 1\nx = 40\n\nprint(x * 4)\n\n160\n\n\nIf we go a little further, and, put the print statement as the first expression of the program, we then get a name error. This error warns us that, the object named x is not defined (i.e. it does not exist).\n\nprint(x * 4)\n\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nNameError: name 'x' is not defined\n\nx = 1\nx = 40\n\nThis error occurs, because inside the print statement, we call the name x. But, this is the first expression of the program, and at this point of the program, we did not defined a object called x. We make this definition, after the print statement, with x = 1 and x = 40. In other words, at this point, python do not know any object called x."
  },
  {
    "objectID": "Chapters/02-python.html#packages",
    "href": "Chapters/02-python.html#packages",
    "title": "3  Key concepts of python",
    "section": "3.6 Packages",
    "text": "3.6 Packages\nA python package (or a python “library”) is basically a set of functions and classes that provides important functionality to solve a specific problem. And pyspark is one of these many python packages available.\nPython packages are usually published (that is, made available to the public) through the PyPI archive5. If a python package is published in PyPI, then, you can easily install it through the pip tool, that we just used in Section 2.4.\nTo use a python package, you always need to: 1) have this package installed on your machine; 2) import this package in your python script. If a package is not installed in your machine, you will face a ModuleNotFoundError as you try to use it, like in the example below. So, if your program produce such an error, is very likely that you are trying to use a package that is not currently installed on your machine. To install it, you may use the pip install <name of the package> command on the terminal of your OS.\n\nimport a_package\n\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nModuleNotFoundError: No module named 'a_package'\nBut, if this package is already installed in your machine, then, you can just import it to your script. To do this, you just include an import statement at the start of your python file. For example, if I want to use the DataFrame function from the pandas package:\n\nimport pandas\n\ndf = pandas.DataFrame([\n  (1, 3214), (2, 4510), \n  (1, 9082), (4, 7822)\n])\n\nprint(df)\n\n   0     1\n0  1  3214\n1  2  4510\n2  1  9082\n3  4  7822\n\n\nTherefore, with import pandas I can access any of the functions available in the pandas package, by using the dot operator after the name of the package (pandas.<name of the function>). However, it can become very annoying to write pandas. every time you want to access a function from pandas, specially if you use it constantly in your code.\nTo make life a little easier, python offers some alternative ways to define this import statement. First, you can give an alias to this package that is shorter/easier to write. As an example, nowadays, is virtually a industry standard to import the pandas package as pd. To do this, you use the as keyword in your import statement. This way, you can access the pandas functionality with pd.<name of the function>:\n\nimport pandas as pd\n\ndf = pd.DataFrame([\n  (1, 3214), (2, 4510), \n  (1, 9082), (4, 7822)\n])\n\nprint(df)\n\n   0     1\n0  1  3214\n1  2  4510\n2  1  9082\n3  4  7822\n\n\nIn contrast, if you want to make your life even easier and produce a more “clean” code, you can import (from the package) just the functions that you need to use. In this method, you can eliminate the dot operator, and refer directly to the function by its name. To use this method, you include the from keyword in your import statement, like this:\n\nfrom pandas import DataFrame\n\ndf = DataFrame([\n  (1, 3214), (2, 4510), \n  (1, 9082), (4, 7822)\n])\n\nprint(df)\n\n   0     1\n0  1  3214\n1  2  4510\n2  1  9082\n3  4  7822\n\n\nSome packages may be very big, and includes many different functions and classes. As the size of the package becomes bigger and bigger, developers tend to divide this package in may “modules”. In other words, the many functions and classes that are available in a python package, are usually organized in “modules”.\nAs an example, the famous flask package is a fairly big package, that contains many functionalities. Because of it, the package is organized in a number of modules, such as cli, globals, views and sessions. To access the functions available in each one of these modules, you use the dot operator."
  },
  {
    "objectID": "Chapters/02-python.html#methods-versus-functions",
    "href": "Chapters/02-python.html#methods-versus-functions",
    "title": "3  Key concepts of python",
    "section": "3.7 Methods versus Functions",
    "text": "3.7 Methods versus Functions\nBeginners tend mix these two types of functions in python, but they are not the same. So lets describe the differences between the two.\nStandard python functions, are functions that we apply over an object. A classical example, is the print() function. You can see in the example below, that we are applying print() over the result object.\n\nresult = 10 + 54\nprint(result)\n\n64\n\n\nOther examples of a standard python function would be map() and list(). See in the example below, that we apply the map() function over a set of objects:\n\nwords = ['apple', 'star', 'abc']\nlengths = map(len, words)\nlist(lengths)\n\n[5, 4, 3]\n\n\nIn contrast, a python method is a function registered inside a python class. In other words, this function belongs to the class itself, and cannot be used outside of it. To use a method, you need to have an instance of the class where it is registered.\nFor example, the startswith() method belongs to the str class (this class is used to represent strings in python). So to use this method, we need to have an instance ot this class saved in a object that we can access. Note in the example below, that we access the startswith() method through the name object. This means that, startswith() is a function. But, we cannot use it without an object of class str, like name.\n\nname = \"Pedro\"\nname.startswith(\"P\")\n\nTrue\n\n\nSo, if we have a class called people, and, this class has a method called location(), we can use this location() method by using the dot operator (.) with the name of an object of class people. If an object called x is an instance of people class, then, we can do x.location().\nBut if this object x is of a different class, like int, then we can no longer use the location() method, because this method does not belong to the int class. For example, if your object is from class A, and, you try to use a method of class B, you will get an AttributeError.\nIn the example exposed below, I have an object called number of class int, and, I try to use the method startswith() from str class with this object:\n\nnumber = 2\n# You can see below that, the `x` object have class `int`\ntype(number)\n# Trying to use a method from `str` class\nnumber.startswith(\"P\")\n\nAttributeError: 'int' object has no attribute 'startswith'"
  },
  {
    "objectID": "Chapters/02-python.html#identifying-classes-and-their-methods",
    "href": "Chapters/02-python.html#identifying-classes-and-their-methods",
    "title": "3  Key concepts of python",
    "section": "3.8 Identifying classes and their methods",
    "text": "3.8 Identifying classes and their methods\nOver the next chapters, you will realize that the pyspark API does not have many standard python functions. So most of its functionality resides in class methods. As a result, the capability of understanding the objects that you have in your python program, and, identifying its classes and methods will be crucial while you are developing and debugging your Spark applications.\nEvery existing object in python represents an instance of a class. In other words, every object in python is associated to a given class. You can always identify the class of an object, by applying the type() function over this object. In the example below, we can see that, the name object is an instance of the str class.\n\nname = \"Pedro\"\ntype(name)\n\nstr\n\n\nIf you do not know all the methods that a class have, you can always apply the dir() function over this class to get a list of all available methods. For example, lets suppose you wanted to see all methods from the str class. To do so, you would do this:\n\ndir(str)\n\n['__add__', '__class__', '__contains__', '__delattr__', '__dir__',\n '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__',\n '__getitem__', '__getnewargs__', '__gt__', '__hash__', '__init__',\n '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', \n '__mod__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__',\n '__repr__', '__rmod__', '__rmul__', '__setattr__', '__sizeof__', \n '__str__', '__subclasshook__', 'capitalize', 'casefold', 'center',\n 'count', 'encode', 'endswith', 'expandtabs', 'find', 'format',\n 'format_map', 'index', 'isalnum', 'isalpha', 'isascii', 'isdecimal',\n 'isdigit', 'isidentifier', 'islower', 'isnumeric', 'isprintable',\n 'isspace', 'istitle', 'isupper', 'join', 'ljust', 'lower', 'lstrip', \n 'maketrans', 'partition', 'replace', 'rfind', 'rindex', 'rjust', \n 'rpartition', 'rsplit', 'rstrip', 'split', 'splitlines', 'startswith',\n 'strip', 'swapcase', 'title', 'translate', 'upper', 'zfill']"
  },
  {
    "objectID": "Chapters/03-spark.html",
    "href": "Chapters/03-spark.html",
    "title": "4  Introducing Apache Spark",
    "section": "",
    "text": "In essence, pyspark is an API to Apache Spark (or simply Spark). In other words, with pyspark we can build Spark applications using the python language. So, by learning a little more about Spark, you will understand a lot more about pyspark."
  },
  {
    "objectID": "Chapters/03-spark.html#what-is-spark",
    "href": "Chapters/03-spark.html#what-is-spark",
    "title": "4  Introducing Apache Spark",
    "section": "4.2 What is Spark?",
    "text": "4.2 What is Spark?\nSpark is a multi-language engine for large-scale data processing that supports both single-node machines and clusters of machines (Apache Spark Official Documentation 2022). Nowadays, Spark became the de facto standard for structure and manage big data applications.\nIt has a number of features that its predecessors did not have, like the capacity for in-memory processing and stream processing (Karau et al. 2015). But, the most important feature of all, is that Spark is an unified platform for big data processing (Chambers and Zaharia 2018). This means that Spark comes with multiple built-in libraries and tools that deals with different aspects of the work with big data. It has a built-in SQL engine1 for performing large-scale data processing; a complete library for scalable machine learning (MLib2); a stream processing engine3 for streaming analytics; and much more;\nIn general, big companies have many different data necessities, and as a result, the engineers and analysts may have to combine and integrate many tools and techniques together, so they can build many different data pipelines to fulfill these necessities. But this approach can create a very serious dependency problem, which imposes a great barrier to support this workflow. This is one of the big reasons why Spark got so successful. It eliminates big part of this problem, by already including almost everything that you might need to use.\n\nSpark is designed to cover a wide range of workloads that previously required separate distributed systems … By supporting these workloads in the same engine, Spark makes it easy and inexpensive to combine different processing types, which is often necessary in production data analysis pipelines. In addition, it reduces the management burden of maintaining separate tools (Karau et al. 2015)."
  },
  {
    "objectID": "Chapters/03-spark.html#spark-application",
    "href": "Chapters/03-spark.html#spark-application",
    "title": "4  Introducing Apache Spark",
    "section": "4.3 Spark application",
    "text": "4.3 Spark application\nYour personal computer can do a lot of things, but, it cannot efficiently deal with huge amounts of data. For this situation, we need several machines working together, adding up their resources to deal with the volume or complexity of the data. Spark is the framework that coordinates the computations across this set of machines (Chambers and Zaharia 2018).\nBecause of this, a relevant part of Spark’s structure is deeply connected to distributed computing models. You probably do not have a cluster of machines at home. So, while following the examples in this book, you will be running Spark on a single machine (i.e. single node mode). But lets just forget about this detail for a moment.\nIn every Spark application, you always have a single machine behaving as the driver node, and multiple machines behaving as the worker nodes. The driver node is responsible for managing the Spark application, i.e. asking for resources, distributing tasks to the workers, collecting and compiling the results, …. The worker nodes are responsible for executing the tasks that are assigned to them, and they need to send the results of these tasks back to the driver node.\nEvery Spark application is distributed into two different and independent processes: 1) a driver process; 2) and a set of executor processes (Chambers and Zaharia 2018). The driver process, or, the driver program, is where your application starts, and it is executed by the driver node. This driver program is responsible for: 1) maintaining information about your Spark Application; 2) responding to a user’s program or input; 3) and analyzing, distributing, and scheduling work across the executors (Chambers and Zaharia 2018).\nEvery time a Spark application starts, the driver process has to communicate with the cluster manager, to acquire workers to perform the necessary tasks. In other words, the cluster manager decides if Spark can use some of the resources (i.e. some of the machines) of the cluster. If the cluster manager allow Spark to use the nodes it needs, the driver program will break the application into many small tasks, and will assign these tasks to the worker nodes.\nThe executor processes, are the processes that take place within each one of the worker nodes. Each executor process is composed of a set of tasks, and the worker node is responsible for performing and executing these tasks that were assigned to him, by the driver program. After executing these tasks, the worker node will send the results back to the driver node (or the driver program). If they need, the worker nodes can communicate with each other, while performing its tasks.\nThis structure is represented in Figure 4.1:\n\n\n\nFigure 4.1: Spark application structure on a cluster of computers\n\n\nWhen you run Spark on a cluster of computers, you write the code of your Spark application (i.e. your pyspark code) on your (single) local computer, and then, submit this code to the driver node. After that, the driver node takes care of the rest, by starting your application, creating your Spark Session, asking for new worker nodes, sending the tasks to be performed, collecting and compiling the results and giving back these results to you.\nHowever, when you run Spark on your (single) local computer, the process is very similar. But, instead of submitting your code to another computer (which is the driver node), you will submit to your own local computer. In other words, when Spark is running on single-node mode, your computer becomes the driver and the worker node at the same time."
  },
  {
    "objectID": "Chapters/03-spark.html#starting-your-spark-session",
    "href": "Chapters/03-spark.html#starting-your-spark-session",
    "title": "4  Introducing Apache Spark",
    "section": "4.4 Starting your Spark Session",
    "text": "4.4 Starting your Spark Session\nEvery Spark application starts with a Spark Session. Basically, the Spark Session is the entry point to your application. This means that, in every pyspark program that you write, you should always start by defining your Spark Session. We do this, by using the getOrCreate() method from pyspark.sql.SparkSession.builder.\nJust store the result of this method in any python object. Is very common to name this object as spark, like in the example below. This way, you can access all the information and methods of Spark from this spark object.\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n\n22/06/01 21:43:55 WARN Utils: Your hostname, pedro-IdeaPad-3-15ALC6 resolves to a loopback address: 127.0.1.1; using 192.168.0.33 instead (on interface wlp1s0)\n22/06/01 21:43:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n\n\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n\n\n22/06/01 21:43:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable"
  },
  {
    "objectID": "Chapters/03-spark.html#running-your-first-spark-application",
    "href": "Chapters/03-spark.html#running-your-first-spark-application",
    "title": "4  Introducing Apache Spark",
    "section": "4.5 Running your first Spark application",
    "text": "4.5 Running your first Spark application\nTo demonstrate what I pyspark program looks like, lets write and run a very simple example of Spark application. This Spark application will build a simple table of 1 column that contains 5 numbers, and then, it will return a list with the first two rows of this table as the result.\nFirst, create a new blank text file in your computer, and save it somewhere with the name spark-example.py. Do not forget to put the .py extension in the name. This program we are writing together is a python program, and should be treated as such. With the .py extension in the name file, you are stating this fact quite clearly to your computer.\nAfter you created and saved the python script (i.e. the text file with the .py extension), you can start writing your pyspark program. As we noted in the previous section, you should always start your pyspark program by defining your Spark Session, with this code:\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n\nAfter you defined your Spark Session, and saved it in an object called spark, you can now access all Spark’s functionality through this spark object. To create the table we use the range() method, and to return the first two rows of the resulting table, we use the take() method:\n\ntable = spark.range(5)\ntable.take(2)\n\nSo, the entire program is just these two parts (or sections) of code. Just copy and paste this code to your python script, then save it.\n\n# The entire program:\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n\ntable = spark.range(5)\ntable.take(2)\n\nNow that you have written your first Spark application with pyspark, you want to execute this application and see its results. To do so, you need send this script to the python interpreter, and to do this you need to: 1) copy the path to your python script; and, 2) open a terminal.\n\n\n\n\nApache Spark Official Documentation. 2022. Documentation for Apache Spark 3.2.1; Available at: https://spark.apache.org/docs/latest/.\n\n\nChambers, Bill, and Matei Zaharia. 2018. Spark: The Definitive Guide: Big Data Processing Made Simple. Sebastopol, CA: O’Reilly Media.\n\n\nKarau, Holden, Andy Konwinski, Patrick Wendell, and Matei Zaharia. 2015. Learning Spark. Sebastopol, CA: O’Reilly Media."
  },
  {
    "objectID": "Chapters/04-dataframes.html",
    "href": "Chapters/04-dataframes.html",
    "title": "5  Introducing Spark DataFrames",
    "section": "",
    "text": "In this chapter, you will understand how Spark represents and manages tables (or tabular data). Different programming languages and frameworks use different names to describe a table. But, in Apache Spark, tables are referred as Spark DataFrames.\nIn pyspark, these DataFrames are stored inside python objects of class pyspark.sql.dataframe.DataFrame, and all the methods present in this class, are commonly referred as the DataFrame API of Spark. This is the most important API of Spark. Much of your Spark applications will heavily use this API to compose your data transformations and data flows (Chambers and Zaharia 2018)."
  },
  {
    "objectID": "Chapters/04-dataframes.html#spark-dataframes-versus-spark-datasets",
    "href": "Chapters/04-dataframes.html#spark-dataframes-versus-spark-datasets",
    "title": "5  Introducing Spark DataFrames",
    "section": "5.2 Spark DataFrames versus Spark Datasets",
    "text": "5.2 Spark DataFrames versus Spark Datasets\nSpark have two notions of structured data: DataFrames and Datasets. In summary, a Spark Dataset, is a distributed collection of data (Apache Spark Official Documentation 2022). In contrast, a Spark DataFrame is a Spark Dataset organized into named columns (Apache Spark Official Documentation 2022).\nThis means that, Spark DataFrames are very similar to tables as we know in relational databases - RDBMS. So in a Spark DataFrame, each column has a name, and they all have the same number of rows. Furthermore, all the rows inside a column must store the same type of data, but each column can store a different type of data.\nIn the other hand, Spark Datasets are considered a collection of any type of data. So a Dataset might be a collection of unstructured data as well, like log files, JSON and XML trees, etc. Spark Datasets can be created and transformed trough the Dataset API of Spark. But this API is available only in Scala and Java API’s of Spark. For this reason, we do not act directly on Datasets with pyspark, only DataFrames. That’s ok, because for the most part of applications, we do want to use DataFrames, and not Datasets, to represent our data.\nBut, what makes a Spark DataFrame different from other dataframes? Like the pandas DataFrame? Or the R native data.frame structure? Is the distributed aspect of it. Spark DataFrames are based on Spark Datasets, and these Datasets are collections of data that are distributed across the cluster. As an example, lets suppose you have the following table stored as a Spark DataFrame:\n\n\n\nID\nName\nValue\n\n\n\n\n1\nAnne\n502\n\n\n2\nCarls\n432\n\n\n3\nStoll\n444\n\n\n4\nPercy\n963\n\n\n5\nMartha\n123\n\n\n6\nSigrid\n621\n\n\n\nIf you are running Spark in a 4 nodes cluster (one is the driver node, and the other three are worker nodes). Each worker node of the cluster will store a section of this data. So you, as the programmer, will see, manage and transform this table as if it was a single and unified table. But behind the hoods, Spark will split this data and store it as many fragments across the Spark cluster. Figure 5.1 presents this notion in a visual manner.\n\n\n\nFigure 5.1: A Spark DataFrame is distributed across the cluster"
  },
  {
    "objectID": "Chapters/04-dataframes.html#building-a-spark-dataframe",
    "href": "Chapters/04-dataframes.html#building-a-spark-dataframe",
    "title": "5  Introducing Spark DataFrames",
    "section": "5.3 Building a Spark DataFrame",
    "text": "5.3 Building a Spark DataFrame\nThere are some different methods to create a Spark DataFrame. For example, because a DataFrame is basically a Dataset of rows, we can build a DataFrame from a collection of Row’s, through the createDataFrame() method from your Spark Session:\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\nfrom datetime import date\nfrom pyspark.sql import Row\n\ndata = [\n  Row(id = 1, value = 28.3, date = date(2021,1,1)),\n  Row(id = 2, value = 15.8, date = date(2021,2,2)),\n  Row(id = 3, value = 20.1, date = date(2021,3,6)),\n  Row(id = 4, value = 12.6, date = date(2021,4,15)),\n]\n\ndf = spark.createDataFrame(data)\n\n22/06/01 21:44:03 WARN Utils: Your hostname, pedro-IdeaPad-3-15ALC6 resolves to a loopback address: 127.0.1.1; using 192.168.0.33 instead (on interface wlp1s0)\n22/06/01 21:44:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n\n\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n22/06/01 21:44:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n\n\nRemember that a Spark DataFrame in python is a object of class pyspark.sql.dataframe.DataFrame as you can see below. If you try to see what is inside of this kind of object, you will get a small description of the columns present in the DataFrame as a result:\n\ntype(df)\n\n<class 'pyspark.sql.dataframe.DataFrame'>\n\ndf\n\nDataFrame[id: bigint, value: double, date: date]\n\n\nSo, in the above example, we use the Row() constructor (from pyspark.sql module) to build 4 rows. The createDataFrame() method, stack these 4 rows together to form our new DataFrame df. The result is a DataFrame with 4 rows and 3 columns (id, value and date).\nBut you can use different methods to create the same DataFrame. As another example, with the code below, we are creating a DataFrame called students. To do this, we create two python lists (data and columns), then, deliver these lists to createDataFrame() method. Each element of data is a python tuple that represents a row in the students DataFrame.\n\ndata = [\n  (12114, 'Anne', 21, 1.56, 8, 9, 10, 9, 'Economics', 'SC'),\n  (13007, 'Adrian', 23, 1.82, 6, 6, 8, 7, 'Economics', 'SC'),\n  (10045, 'George', 29, 1.77, 10, 9, 10, 7, 'Law', 'SC'),\n  (12459, 'Adeline', 26, 1.61, 8, 6, 7, 7, 'Law', 'SC'),\n  (10190, 'Mayla', 22, 1.67, 7, 7, 7, 9, 'Design', 'AR'),\n  (11552, 'Daniel', 24, 1.75, 9, 9, 10, 9, 'Design', 'AR')\n]\n\ncolumns = [\n  'StudentID', 'Name', 'Age', 'Height', 'Score1',\n  'Score2', 'Score3', 'Score4', 'Course', 'Department'\n]\n\nstudents = spark.createDataFrame(data, columns)\nstudents\n\nDataFrame[StudentID: bigint, Name: string, Age: bigint, Height: double, Score1: bigint, Score2: bigint, Score3: bigint, Score4: bigint, Course: string, Department: string]"
  },
  {
    "objectID": "Chapters/04-dataframes.html#viewing-a-spark-dataframe",
    "href": "Chapters/04-dataframes.html#viewing-a-spark-dataframe",
    "title": "5  Introducing Spark DataFrames",
    "section": "5.4 Viewing a Spark DataFrame",
    "text": "5.4 Viewing a Spark DataFrame\nA key aspect of Spark is its laziness. In other words, for most operations, Spark will only check if your code is correct and if it makes sense. Spark will not actually run or execute the operations you are describing in your code, unless you explicit ask for it with a trigger operation.\nYou can notice this laziness in the above output. Because when we call for an object that stores a Spark DataFrame (like df and students), Spark will only calculate and print the schema of your Spark DataFrame, and not the DataFrame itself.\nSo how can we actually see our DataFrame? How can we visualize the rows and values that are stored inside of it? We can use the show() method for this. It will print the table as pure text, as you can see in the example below:\n\nstudents.show()\n\n[Stage 0:>                                                          (0 + 1) / 1]\n\n\n                                                                                \n\n\n+---------+-------+---+------+------+------+------+------+---------+----------+\n|StudentID|   Name|Age|Height|Score1|Score2|Score3|Score4|   Course|Department|\n+---------+-------+---+------+------+------+------+------+---------+----------+\n|    12114|   Anne| 21|  1.56|     8|     9|    10|     9|Economics|        SC|\n|    13007| Adrian| 23|  1.82|     6|     6|     8|     7|Economics|        SC|\n|    10045| George| 29|  1.77|    10|     9|    10|     7|      Law|        SC|\n|    12459|Adeline| 26|  1.61|     8|     6|     7|     7|      Law|        SC|\n|    10190|  Mayla| 22|  1.67|     7|     7|     7|     9|   Design|        AR|\n|    11552| Daniel| 24|  1.75|     9|     9|    10|     9|   Design|        AR|\n+---------+-------+---+------+------+------+------+------+---------+----------+\n\n\n\nBy default, this method shows only the top rows of your DataFrame, but you can specify how much rows exactly you want to see, by using show(n), where n is the number of rows. For example, I can visualize only the first 2 rows of df like this:\n\ndf.show(2)\n\n+---+-----+----------+\n| id|value|      date|\n+---+-----+----------+\n|  1| 28.3|2021-01-01|\n|  2| 15.8|2021-02-02|\n+---+-----+----------+\nonly showing top 2 rows\n\n\n\n\n\n\n\nApache Spark Official Documentation. 2022. Documentation for Apache Spark 3.2.1; Available at: https://spark.apache.org/docs/latest/.\n\n\nChambers, Bill, and Matei Zaharia. 2018. Spark: The Definitive Guide: Big Data Processing Made Simple. Sebastopol, CA: O’Reilly Media."
  },
  {
    "objectID": "Chapters/references.html",
    "href": "Chapters/references.html",
    "title": "References",
    "section": "",
    "text": "Apache Spark Official Documentation. 2022. Documentation for\nApache Spark 3.2.1; Available at: https://spark.apache.org/docs/latest/.\n\n\nChambers, Bill, and Matei Zaharia. 2018. Spark: The Definitive\nGuide: Big Data Processing Made Simple. Sebastopol, CA: O’Reilly\nMedia.\n\n\nKarau, Holden, Andy Konwinski, Patrick Wendell, and Matei Zaharia. 2015.\nLearning Spark. Sebastopol, CA: O’Reilly Media."
  }
]
[
  {
    "objectID": "Chapters/07-import.html",
    "href": "Chapters/07-import.html",
    "title": "6  Importing data to Spark",
    "section": "",
    "text": "Another way of creating Spark DataFrames, is to read (or import) data from a file and convert it to a DataFrame. Spark can read a variety of file formats, including CSV, Parquet, JSON, ORC and Binary files. Furthermore, Spark can connect to other databases and import tables from them, using JDBC connections. We can access the read engines for these different file formats, by using the read module from your Spark Session object."
  },
  {
    "objectID": "Chapters/07-import.html#reading-data-from-static-files",
    "href": "Chapters/07-import.html#reading-data-from-static-files",
    "title": "6  Importing data to Spark",
    "section": "6.2 Reading data from static files",
    "text": "6.2 Reading data from static files\nStatic files are probably the easiest way to transport data from one computer to another. Because you just need to copy and paste this file to this other machine, or download it from the internet. To read any file stored inside your computer, Spark always need to know the path to this file.\nEvery OS have its own file system. Every file in your computer is stored in a specific address of this file system, and the “path” to this file is the path (or “steps”) that your computer needs to follow to reach this specific address, where the file is stored.\nAs an example, I have the following CSV file saved in my computer:\nname,age,job\nJorge,30,Developer\nBob,32,Developer\nThis CSV was saved in a file called people.csv, inside a folder called Data. So, to read this file, Spark needs to know the path to this people.csv file. In other words, Spark needs to know where this file is stored in my computer, to be able to read it.\n\n\nIn my specific case, considering where this Data folder is in my file system, a relative path to it would be \"../Data/\". Having the path to the folder where people.csv is stored, I just need to add this file to the path, resulting in \"../Data/people.csv\". See in the example below, that I gave this path to the read.csv() method of my Spark Session. As a result, Spark will visit this address, and, read the file that is stored there:\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n\npath = \"../Data/people.csv\"\n\ndf = spark.read.csv(path)\ndf.show()\n\n+-----+---+---------+\n|  _c0|_c1|      _c2|\n+-----+---+---------+\n| name|age|      job|\n|Jorge| 30|Developer|\n|  Bob| 32|Developer|\n+-----+---+---------+\n\n\n\nIn the above example, I gave a relative path to the file I wanted to read. But you can provide an absolute path1 to the file, if you want to.\nThe people.csv is located at a very specific folder in my Linux computer, so, the absolute path to this file is pretty long as you can see below. But, if I were in my Windows machine, this absolute path would be something like \"C:\\Users\\pedro\\Documents\\Projects\\...\".\n\npath = \"/home/pedro/Documents/Projets/Books/Introd-pyspark/Data/people.csv\"\n\ndf = spark.read.csv(path)\ndf.show()\n\n\n\n+-----+---+---------+\n|  _c0|_c1|      _c2|\n+-----+---+---------+\n| name|age|      job|\n|Jorge| 30|Developer|\n|  Bob| 32|Developer|\n+-----+---+---------+\n\n\n\nIf you give an invalid path (that is, a path that does not exist in your computer), you will get a AnalysisException. In the example below, I try to read a file called \"weird-file.csv\" that (in theory) is located at my current working directory. But when Spark looks inside my current directory, it does not find any file called \"weird-file.csv\". As a result, Spark raises a AnalysisException that warns me about this mistake.\n\ndf = spark.read.csv(\"weird-file.csv\")\n\nTraceback (most recent call last):\n  ...\npyspark.sql.utils.AnalysisException: Path does not exist: file:/home/pedro/Documents/Projects/Books/Introd-pyspark/weird-file.csv\nEvery time you face this “Path does not exist” error, it means that Spark did not found the file that you described in the path you gave to spark.read. In this case, is very likely that you have a typo or a mistake in your path. Maybe your forgot to add the .csv extension to the name of your file. Or maybe you forgot to use the right angled slash (/) instead of the left angled slash (\\).\nSometimes, is useful to list all the files that are stored inside a folder. This way, you can make sure that you are looking at the right folder of your file system. For that, you can use the listdir() function from os module of python. As an example, I can list all the files that are stored inside of the Data folder in this way:\n\nfrom os import listdir\nlistdir(\"../Data/\")\n\n['sales.json',\n 'people.csv',\n 'transf_reform.csv',\n 'sales1.json',\n 'penguins.csv']"
  },
  {
    "objectID": "Chapters/07-import.html#defining-import-options",
    "href": "Chapters/07-import.html#defining-import-options",
    "title": "6  Importing data to Spark",
    "section": "6.4 Defining import options",
    "text": "6.4 Defining import options\nWhile reading and importing your files, Spark will use the default values for the import options defined by the read engine you are using, unless you explicit ask it to use different values. Each read engine has its own read/import options.\nFor example, the spark.read.orc() engine has a option called mergeSchema. With this option, you can ask Spark to merge the schemas collected from all the ORC part-files. In contrast, the spark.read.csv() engine does not have such option. Because this functionality of “merging schemas” does not make sense with CSV files.\nThis means that, some import options are specific (or characteristic) of some file formats. For example, the sep option (where you define the separator character) is used only in the spark.read.csv() engine, because you do not have a special character that behaves as the “separator” in the other file formats (ORC, JSON, Parquet…). So it does not make sense to have such option in the other read engines.\nIn the other hand, some import options can co-exist in multiple read engines. For example, the spark.read.json() and spark.read.csv() have both an encoding option. The encoding is a very important information, and Spark needs it to correct interpret your file. By default, Spark will always assume that your files use the UTF-8 encoding system. Although this may not be true for your specific case, and for these cases you use this encoding option to tell Spark which one to use.\nIn the next sections, I will break down some of the most used import options for each file format. If you want to see the complete list of import options, you can visit the Data Source Option section in the specific part of the file format you are using in the Spark SQL Guide2.\n\n6.4.1 Import options for CSV files\nThe most important import options for CSV files are sep, encoding, header.\n\n\n6.4.2 Pulling data from SQL Databases\nWe can use the spark.read.jdbc() method to connect and read data from Databases using JDBC connections. Also, you can read a SQL table from your Spark context by using the spark.table() method."
  },
  {
    "objectID": "Chapters/06-dataframes-sql.html",
    "href": "Chapters/06-dataframes-sql.html",
    "title": "5  Working with SQL in pyspark",
    "section": "",
    "text": "As we discussed in Chapter 2, Spark is a multi-language engine for large-scale data processing. This means that we can build our Spark application using many different languages (like Java, Scala, Python and R). And Spark have some tools that allow us to mix these different languages with pure SQL in our application. We will focus on these tools in this chapter."
  },
  {
    "objectID": "Chapters/06-dataframes-sql.html#creating-sql-tables-in-spark",
    "href": "Chapters/06-dataframes-sql.html#creating-sql-tables-in-spark",
    "title": "5  Working with SQL in pyspark",
    "section": "5.2 Creating SQL Tables in Spark",
    "text": "5.2 Creating SQL Tables in Spark\nIn real life jobs at the industry, is very likely that your data will be allocated inside a SQL database. Spark can connect to external SQL databases through JDBC/ODBC connections, or, read tables from Apache Hive.\nAs a first example, lets use pyspark to create a DataFrame, and, register this DataFrame as a temporary SQL table in our Spark context. This way, we avoid the work to connect to some existing SQL database, and, still get to learn how to use SQL queries in pyspark.\nIn the example below, I am reading a CSV file from my computer called penguins.csv (remember that this CSV can be downloaded from the book repository1), then, I create a SQL temporary view (called penguins_view) from this penguins DataFrame with the createOrReplaceTempView() method.\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n\npath = \"../Data/penguins.csv\"\npenguins = spark.read\\\n  .csv(path, header = True)\n  \npenguins.createOrReplaceTempView('penguins_view')\n\n22/06/21 11:09:24 WARN Utils: Your hostname, pedro-IdeaPad-3-15ALC6 resolves to a loopback address: 127.0.1.1; using 192.168.0.33 instead (on interface wlp1s0)\n22/06/21 11:09:24 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n\n\nUsing Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n\n\n22/06/21 11:09:25 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n\n\n22/06/21 11:09:26 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n\n\nAfter these commands, I have now a view called penguins_view registered in the Spark SQL context, which I can query it, using pure SQL. We can execute a SQL query by using the sql() method from our Spark Session, like in the example below:\n\nspark.sql('select * from penguins_view').show(5)\n\n+-------+---------+--------------+-------------+-----------------+-----------+------+----+\n|species|   island|bill_length_mm|bill_depth_mm|flipper_length_mm|body_mass_g|   sex|year|\n+-------+---------+--------------+-------------+-----------------+-----------+------+----+\n| Adelie|Torgersen|          39.1|         18.7|              181|       3750|  male|2007|\n| Adelie|Torgersen|          39.5|         17.4|              186|       3800|female|2007|\n| Adelie|Torgersen|          40.3|           18|              195|       3250|female|2007|\n| Adelie|Torgersen|          null|         null|             null|       null|  null|2007|\n| Adelie|Torgersen|          36.7|         19.3|              193|       3450|female|2007|\n+-------+---------+--------------+-------------+-----------------+-----------+------+----+\nonly showing top 5 rows\n\n\n\nThis sql() method always result in a new Spark DataFrame. That is why I used the show() method right after sql(), to see what this new Spark DataFrame looked like.\nTherefore, in the above example, we wrote a simple and pure SQL query, and, executed it through pyspark, by using this sql() method. This means that, this sql() method is the bridge between pyspark and your SQL database. You give a pure SQL query (inside a string) to this sql() method, and, Spark will execute it, considering the SQL databases that you are connected with.\nHaving this in mind, we could use this sql() method to create a physical SQL table (instead of a temporary view). We just need to pass the necessary SQL commands that will create this table to be executed by this method. In the example below, we create a new database called examples, and, a table called code_brazil_states.\n\nspark.sql('create database `examples`')\nspark.sql('use `examples`'\nspark.sql('''\n  create table `code_brazil_states` (\n    `code` int,\n    `state_name` string\n  )\n''')\nspark.sql('insert into `code_brazil_states` values (31, \"Minas Gerais\")')\nspark.sql('insert into `code_brazil_states` values (15, \"Pará\")')\nspark.sql('insert into `code_brazil_states` values (41, \"Paraná\")')\nspark.sql('insert into `code_brazil_states` values (25, \"Paraíba\")')"
  },
  {
    "objectID": "Chapters/references.html",
    "href": "Chapters/references.html",
    "title": "References",
    "section": "",
    "text": "Apache Spark Official Documentation. 2022. Documentation for\nApache Spark 3.2.1; Available at: https://spark.apache.org/docs/latest/.\n\n\nChambers, Bill, and Matei Zaharia. 2018. Spark: The Definitive\nGuide: Big Data Processing Made Simple. Sebastopol, CA: O’Reilly\nMedia.\n\n\nKarau, Holden, Andy Konwinski, Patrick Wendell, and Matei Zaharia. 2015.\nLearning Spark. Sebastopol, CA: O’Reilly Media."
  },
  {
    "objectID": "Chapters/07-import.html#reading-other-file-formats",
    "href": "Chapters/07-import.html#reading-other-file-formats",
    "title": "6  Importing data to Spark",
    "section": "6.3 Reading other file formats",
    "text": "6.3 Reading other file formats\nEven CSV’s being a very popular format, is very likely that you will need to read archives in many different formats. To read a different file format, you just need to use a different read engine.\nIn more details, you need to give the path of this file, to a different method of the read module of your Spark Session. More specifically, to the method that corresponds to the file format you are interested in.\nThe main read engines available in Spark for static files are listed below:\n\nspark.read.json(): to read JSON files;\nspark.read.csv(): to read CSV files;\nspark.read.parquet(): to read Apache Parquet files;\nspark.read.orc(): to read ORC (Apache Optimized Row Columnar format) files;\n\nSo, to read a JSON file called sales.json that is stored in my Data folder, I can do this:\n\njson_data = spark.read.json(\"../Data/sales.json\")\njson_data.show()\n\n+-----+----------+------------+-------+-------------------+-----+\n|price|product_id|product_name|sale_id|          timestamp|units|\n+-----+----------+------------+-------+-------------------+-----+\n| 3.12|       134| Milk 1L Mua| 328711|2022-02-01T22:10:02|    1|\n| 1.22|       110|  Coke 350ml| 328712|2022-02-03T11:42:09|    3|\n| 4.65|       117|    Pepsi 2L| 328713|2022-02-03T14:22:15|    1|\n| 1.22|       110|  Coke 350ml| 328714|2022-02-03T18:33:08|    1|\n| 0.85|       341|Trident Mint| 328715|2022-02-04T15:41:36|    1|\n+-----+----------+------------+-------+-------------------+-----+"
  }
]
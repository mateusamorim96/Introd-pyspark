[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to pyspark",
    "section": "",
    "text": "Hello! This is the initial page!"
  },
  {
    "objectID": "01-intro.html",
    "href": "01-intro.html",
    "title": "2  Preface",
    "section": "",
    "text": "In essence, pyspark is a python package that provides an API for the Spark SQL module. In other words, with pyspark you are able to use the python language to program SQL queries and run them on a Spark cluster in a scalable and elegant way. This book focus on teaching the fundamentals of pyspark, and how to use it for big data analysis.\nThis book, also contains a small introduction to key python concepts that are important to understand how pyspark is organized and how it works in practice, and, since we will be using Spark under the hood, is very important to understand a little bit of how Spark works, so, we provide a small introduction to Spark key concepts as well.\nBig part of the knowledge exposed here is extracted from a lot of practical experience of the author, working with pyspark to analyze big data at platforms such as Databricks1."
  },
  {
    "objectID": "01-intro.html#some-conventions-of-this-book",
    "href": "01-intro.html#some-conventions-of-this-book",
    "title": "2  Preface",
    "section": "2.1 Some conventions of this book",
    "text": "2.1 Some conventions of this book\nThis book is about pyspark, which is a python package. As a result, we will be exposing a lot of python code across the entire book. Examples of python code, are always shown inside a gray rectangle, like this example below.\nEvery visible result that this python code produce, will be shown outside of the gray rectangle, just below the command that produced that visible result. Besides that, every line of result will always be written in plain black. So in the example below, the value 729 is the only visible result of this python code, and, the statement print(y) is the command that triggered this result.\n\nx = 3\ny = 9 ** x\n\nprint(y)\n\n729\n\n\nFurthermore, all terminal commands that we expose in this book, will always be: pre-fixed by Terminal$; written in black; and, not outlined by a gray rectangle. In the example below, we can easily detect that this command pip install jupyter should be inserted in the terminal of the OS (whatever is the terminal that your OS uses), and not in the python interpreter, because this command is prefixed with Terminal$.\nTerminal$ pip install jupyter\nSome terminal commands may produce visible results as well. In that case, these results will be right below the respective command, and will not be pre-fixed with Terminal$. For example, we can see below that the command echo \"Hello!\" produces the result \"Hello!\".\nTerminal$ echo \"Hello!\"\n\"Hello!\""
  },
  {
    "objectID": "01-intro.html#sec-install-software",
    "href": "01-intro.html#sec-install-software",
    "title": "2  Preface",
    "section": "2.2 First step: install the necessary software",
    "text": "2.2 First step: install the necessary software\nBefore we start, is really important that you have all the software you need already installed on your machine and ready to go. Spark and, as a result, pyspark, are both available for Windows and UNIX-like systems (e.g. Linux and MacOs).\nInstall and manage spark in Windows is a little harder than in a UNIX-like machine. But it’s perfectly ok if do not have a UNIX machine. A Windows machine will do it just fine. Just for referencing, the examples showed throughout this book were executed in a Linux machine (more specifically, in Ubuntu).\nSo, to use pyspark you need to install a spark distribution, and, the python package called pyspark. Installing the pyspark python package is pretty straightforward. Just open a terminal, and use the pip command to do it:\nTerminal$ pip install pyspark\nIf you are in Windows, this would mean: open the cmd.exe terminal, and, run the above command. But, if a message like pip: command not found appears, this means that you do not have the pip tool installed on your Windows. Hence, if you face this kind of message, you need to install pip before you even install pyspark.\nIn contrast, if you are running on a Linux or MacOs machine, you need to open a bash terminal (most distros of Linux can open a terminal by pressing the Ctrl+Alt+T shortcut on the keyboard), and, run the exact same command above. Again, if a message like pip: command not found appears, it means that pip is not installed on your system.\nIn UNIX-like systems, installing pip is even easier, because you can use the built-in package manager to do this for you. In Debian like distros (e.g. Ubuntu), you use the apt tool, and, in Arch-Linux like distros (e.g. Arch Linux, Manjaro) you would use the pacman tool. Both possibilities are exposed below:\n# If you are in a Debian like distro of Linux\n# and need to install `pip`, use this command:\nTerminal$ apt install python3-pip\n# If you are in a Arch-Linux like distro of Linux\n# and need to install `pip`, use this command:\nTerminal$ pacman -S python-pip\nBut, if you are running in MacOs, is best to use the get-pip.py method to do install pip2. To use this method, try to run the command below on the terminal. After you installed pip, run the same pip command we showed earlier to install pyspark.\n# To install `pip` on a MacOs:\nTerminal$ curl https://bootstrap.pypa.io/get-pip.py | python3"
  },
  {
    "objectID": "02-spark.html",
    "href": "02-spark.html",
    "title": "3  Introducing Apache Spark",
    "section": "",
    "text": "In essence, pyspark is an API to Apache Spark (or simply Spark). In other words, with pyspark we can build Spark applications using the python language. So, by learning a little more about Spark, you will understand a lot more about pyspark."
  },
  {
    "objectID": "02-spark.html#what-is-spark",
    "href": "02-spark.html#what-is-spark",
    "title": "3  Introducing Apache Spark",
    "section": "3.1 What is Spark?",
    "text": "3.1 What is Spark?\nSpark is a multi-language engine for large-scale data processing that supports both single-node machines and clusters of machines (Apache Spark Official Documentation 2022). Nowadays, Spark became the de facto standard for structure and manage big data applications.\nIt has a number of features that its predecessors did not have, like the capacity for in-memory processing and stream processing (Karau et al. 2015). But, the most important feature of all, is that Spark is an unified platform for big data processing (Chambers and Zaharia 2018). This means that Spark comes with multiple built-in libraries and tools that deals with different aspects of the work with big data. It has a built-in SQL engine1 for performing large-scale data processing; a complete library for scalable machine learning (MLib2); a stream processing engine3 for streaming analytics; and much more;\nIn general, big companies have many different data necessities, and as a result, the engineers and analysts may have to combine and integrate many tools and techniques together, so they can build many different data pipelines to fulfill these necessities. But this approach can create a very serious dependency problem, which imposes a great barrier to support this workflow. This is one of the big reasons why Spark got so successful. It eliminates big part of this problem, by already including almost everything that you might need to use.\n\nSpark is designed to cover a wide range of workloads that previously required separate distributed systems … By supporting these workloads in the same engine, Spark makes it easy and inexpensive to combine different processing types, which is often necessary in production data analysis pipelines. In addition, it reduces the management burden of maintaining separate tools (Karau et al. 2015)."
  },
  {
    "objectID": "02-spark.html#the-spark-session",
    "href": "02-spark.html#the-spark-session",
    "title": "3  Introducing Apache Spark",
    "section": "3.2 The Spark Session",
    "text": "3.2 The Spark Session\n\n\n\n\nApache Spark Official Documentation. 2022. Documentation for Apache Spark 3.2.1; Available at: https://spark.apache.org/docs/latest/.\n\n\nChambers, Bill, and Matei Zaharia. 2018. Spark: The Definitive Guide: Big Data Processing Made Simple. Sebastopol, CA: O’Reilly Media.\n\n\nKarau, Holden, Andy Konwinski, Patrick Wendell, and Matei Zaharia. 2015. Learning Spark. Sebastopol, CA: O’Reilly Media."
  },
  {
    "objectID": "03-python.html",
    "href": "03-python.html",
    "title": "4  Key concepts of python",
    "section": "",
    "text": "If you have experience with python, and understands how objects and classes works, you might want to skip this entire chapter. But, if you are new to the language and do not have much experience with it, you might want to stick a little bit, and learn a few key concepts that will help you to understand how the pyspark package is organized, and how to work with it."
  },
  {
    "objectID": "03-python.html#scripts",
    "href": "03-python.html#scripts",
    "title": "4  Key concepts of python",
    "section": "4.1 Scripts",
    "text": "4.1 Scripts\nPython programs are written in plain text files that are saved with the .py extension. After you save these files, they are usually called “scripts”. So a script is just a text file that contains all the commands that make your python program.\nThere are many IDEs or programs that help you to write, manage and organize this kind of files (like Microsoft Visual Studio Code1, PyCharm2, Anaconda3 and RStudio4). But, if you do not have any of them installed, you can just create a new plain text file from the built-in Notepad program of your OS (operational system), and, save it with the .py extension."
  },
  {
    "objectID": "03-python.html#how-to-run-a-python-program",
    "href": "03-python.html#how-to-run-a-python-program",
    "title": "4  Key concepts of python",
    "section": "4.2 How to run a python program",
    "text": "4.2 How to run a python program\nThere are many ways to run a python program, but I will show you the more “standard” way. That is to use the python command inside the terminal of your OS (you need to have python already installed).\nAs an example, lets create a simple “Hello world” program. First, open a new text file then save it somewhere in your machine (with the name hello.py). Remember to save the file with the .py extension. Then copy and paste the following command into this file:\n\nprint(\"Hello World!\")\n\nIt will be much easier to run this script, if you open the terminal inside the folder where you save the hello.py file. On Linux, with the built-in File Explorer, you can click (with the right button of you mouse) inside the folder, and you will have an option to open the terminal. If you are on windows, you can open the folder on the File Explorer, then, overwrite the path to this folder with “cmd” and press Enter.\nBut, if for some reason you could not open the terminal inside the folder, just open a terminal (in any way you can), then, use the cd command (stands for “change directory”) with the path to the folder where you saved hello.py. This way, your terminal will be rooted on this folder.\nFor example, if I saved hello.py inside my Documents folder, the path to this folder in Windows would be something like this: \"C:\\Users\\pedro\\Documents\". On the other hand, this path on Linux would be something like \"/usr/pedro/Documents\". So the command to change to this directory would be:\n# On Windows:\nTerminal$ cd \"C:\\Users\\pedro\\Documents\"\n# On Linux:\nTerminal$ cd \"/usr/pedro/Documents\"\nAfter this cd command, you just run the python hello.py command in the terminal. As a result, the hello.py will be executed, and, the text Hello World! should be printed to the terminal.\nTerminal$ python hello.py\nHello World!\nThere you have it! So every time you need to run your python program, just open a terminal and run the command python <path to your script>. If the terminal is rooted on the folder where you saved your script, you can just use the python <name of the file> command."
  },
  {
    "objectID": "03-python.html#objects",
    "href": "03-python.html#objects",
    "title": "4  Key concepts of python",
    "section": "4.3 Objects",
    "text": "4.3 Objects\nAlthough python is a general-purpose language, most of its features are focused on object-oriented programming. Meaning that, python is a programming language focused on creating, managing and modifying objects and classes of objects.\nSo, when you work with python, you are basically applying many operations and functions over a set of objects. In essence, an object in python, is a name that refers to a set of data. This data can be anything that you computer can store (or represent).\nHaving that in mind, an object is just a name, and this name is a reference, or a key to access some data. To define an object in python, you must use the assignment operator, which is the equal sign (=). In the example below, we are defining, or, creating an object called x, and it stores the value 10. Therefore, with the name x we can access this value of 10.\n\nx = 10\nprint(x)\n\n10\n\n\nWhen we store a value inside an object, we can easily reuse this value in multiple operations or expressions:\n\n# Multiply by 2\nprint(x * 2)\n\n20\n\n\n\n# Divide by 3\nprint(x / 3)\n\n3.3333333333333335\n\n\n\n# Print its class\nprint(type(x))\n\n<class 'int'>\n\n\nRemember, an object can store any type of value, or any type of data. For example, it can store a single string, like the object salutation below:\n\nsalutation = \"Hello! My name is Pedro\"\n\nOr, a list of multiple strings:\n\nnames = [\n  \"Anne\", \"Vanse\", \"Elliot\",\n  \"Carlyle\", \"Ed\", \"Memphis\"\n]\n\nprint(names)\n\n['Anne', 'Vanse', 'Elliot', 'Carlyle', 'Ed', 'Memphis']\n\n\nOr a dict containing the description of a product:\n\nproduct = {\n  'name': 'Coca Cola',\n  'volume': '2 litters',\n  'price': 2.52,\n  'group': 'non-alcoholic drinks',\n  'department': 'drinks'\n}\n\nprint(product)\n\n{'name': 'Coca Cola', 'volume': '2 litters', 'price': 2.52, 'group': 'non-alcoholic drinks', 'department': 'drinks'}\n\n\nAnd many other things…"
  },
  {
    "objectID": "03-python.html#expressions",
    "href": "03-python.html#expressions",
    "title": "4  Key concepts of python",
    "section": "4.4 Expressions",
    "text": "4.4 Expressions\nPython programs are organized in blocks of expressions (or statements). A python expression is a statement that describes an operation to be performed by the program. For example, the expression below describes the sum between 3 and 5.\n\n3 + 5\n\n8\n\n\nThe expression above is composed of numbers (like 3 and 5) and a operator, more specifically, the sum symbol (+). But any python expression can include a multitude of different items. It can be composed of functions (like print(), map() and str()), constant strings (like \"Hello World!\"), logical operators (like !=, <, > and ==), arithmetic operators (like *, /, **, %, - and +), structures (like lists, arrays and dicts) and many other types of commands.\nBelow we have a more complex example, that contains the def keyword (which starts a function definition; in the example below, this new function being defined is double()), many built-in functions (list(), map() and print()), a arithmetic operator (*), numbers and a list (initiated by the pair of brackets - []).\n\ndef double(x):\n  return x * 2\n  \nprint(list(map(double, [4, 2, 6, 1])))\n\n[8, 4, 12, 2]\n\n\nPython expressions are evaluated in a sequential manner (from top to bottom of your python file). In other words, python runs the first expression in the top of your file, them, goes to the second expression, and runs it, them goes to the third expression, and runs it, and goes on and on in that way, until it hits the end of the file. So, in the example above, python executes the function definition (initiated at def double(x):), before it executes the print() statement, because the print statement is below the function definition.\nThis order of evaluation is commonly referred as “control flow” in many programming languages. Sometimes, this order can be a fundamental part of the python program. Meaning that, sometimes, if we change the order of the expressions in the program, we can produce unexpected results (like an error), or change the results produced by the program.\nAs an example, the program below prints the result 4, because the print statement is executed before the expression x = 40.\n\nx = 1\n\nprint(x * 4)\n\nx = 40\n\n4\n\n\nBut, if we execute the expression x = 40 before the print statement, we then change the result produced by the program.\n\nx = 1\nx = 40\n\nprint(x * 4)\n\n160\n\n\nIf we go a little further, and, put the print statement as the first expression of the program, we then get a name error. This error warns us that, the object named x is not defined (i.e. it does not exist).\n\nprint(x * 4)\n\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nNameError: name 'x' is not defined\n\nx = 1\nx = 40\n\nThis error occurs, because inside the print statement, we call the name x. But, this is the first expression of the program, and at this point of the program, we did not defined a object called x. We make this definition, after the print statement, with x = 1 and x = 40. In other words, at this point, python do not know any object called x."
  },
  {
    "objectID": "03-python.html#packages",
    "href": "03-python.html#packages",
    "title": "4  Key concepts of python",
    "section": "4.5 Packages",
    "text": "4.5 Packages\nA python package (or a python “library”) is basically a set of functions and classes that provides important functionality to solve a specific problem. And pyspark is one of these many python packages available.\nPython packages are usually published (that is, made available to the public) through the PyPI archive5. If a python package is published in PyPI, then, you can easily install it through the pip tool, that we just used in Section 2.2.\nTo use a python package, you always need to: 1) have this package installed on your machine; 2) import this package in your python script. If a package is not installed in your machine, you will face a ModuleNotFoundError as you try to use it, like in the example below. So, if your program produce such an error, is very likely that this package is not currently installed on your machine. To install it, you may use the pip install <name of the package> command on the terminal.\n\nimport a_package\n\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nModuleNotFoundError: No module named 'a_package'\nBut, if this package is already installed in your machine, then, you can just import it to your session. To do this, you just include an import statement at the start of your python file. For example, if I want to use the DataFrame function from the pandas package:\n\nimport pandas\n\ndf = pandas.DataFrame([\n  (1, 3214), (2, 4510), \n  (1, 9082), (4, 7822)\n])\n\nprint(df)\n\n   0     1\n0  1  3214\n1  2  4510\n2  1  9082\n3  4  7822\n\n\nTherefore, with import pandas I can access any of the functions available in the pandas package, by using the dot operator after the name of the package (pandas.<name of the function>). However, it can become very annoying to write pandas. every time you want to access a function from pandas, specially if you use it constantly in your code.\nTo make life a little easier, python offers some alternative ways to define this import statement. First, you can give an alias to this package that is shorter/easier to write. As an example, nowadays, is virtually a industry standard to import the pandas package as pd. To do this, you use the as keyword in your import statement. This way, you can access the pandas functionality with pd.<name of the function>:\n\nimport pandas as pd\n\ndf = pd.DataFrame([\n  (1, 3214), (2, 4510), \n  (1, 9082), (4, 7822)\n])\n\nprint(df)\n\n   0     1\n0  1  3214\n1  2  4510\n2  1  9082\n3  4  7822\n\n\nIn contrast, if you want to make your life even easier and produce a more “clean” code, you can import (from the package) just the functions that you need to use. In this method, you can eliminate the dot operator, and refer directly to the function by its name. To use this method, you include the from keyword in your import statement, like this:\n\nfrom pandas import DataFrame\n\ndf = DataFrame([\n  (1, 3214), (2, 4510), \n  (1, 9082), (4, 7822)\n])\n\nprint(df)\n\n   0     1\n0  1  3214\n1  2  4510\n2  1  9082\n3  4  7822\n\n\nSome packages may be very big, and includes many different functions and classes. As the size of the package becomes bigger and bigger, developers tend to divide this package in may “modules”. In other words, the many functions and classes that are available in a python package, are usually organized in “modules”.\nAs an example, the famous flask package is a fairly big package, that contains many functionalities. Because of it, the package is organized in a number of modules, such as cli, globals, views and sessions. To access the functions available in each one of these modules, you use the dot operator."
  },
  {
    "objectID": "03-python.html#methods-versus-functions",
    "href": "03-python.html#methods-versus-functions",
    "title": "4  Key concepts of python",
    "section": "4.6 Methods versus Functions",
    "text": "4.6 Methods versus Functions\nBeginners tend mix these two types of functions in python, but they are not the same. So lets describe the differences between the two.\nStandard python functions, are functions that we apply over an object. A classical example, is the print() function. You can see in the example below, that we are applying print() over the result object.\n\nresult = 10 + 54\nprint(result)\n\n64\n\n\nOther examples of a standard python function would be map() and list(). See in the example below, that we apply the map() function over a set of objects:\n\nwords = ['apple', 'star', 'abc']\nlengths = map(len, words)\nlist(lengths)\n\n[5, 4, 3]\n\n\nIn contrast, a python method is a function registered inside a python class. In other words, this function belongs to the class itself, and cannot be used outside of it. To use a method, you need to have an instance of the class where it is registered.\nFor example, the startswith() method belongs to the str class (this class is used to represent strings in python). So to use this method, we need to have an instance ot this class saved in a object that we can access. Note in the example below, that we access the startswith() method through the name object. This means that, startswith() is a function. But, we cannot use it without an object of class str, like name.\n\nname = \"Pedro\"\nname.startswith(\"P\")\n\nTrue\n\n\nSo, if we have a class called people, and, a method called location(), we can use the location() method by using the dot operator (.) with the name of an object of class people. If an object called x is an instance of people class, then, we can do x.location()."
  },
  {
    "objectID": "03-python.html#identifying-classes-of-objects",
    "href": "03-python.html#identifying-classes-of-objects",
    "title": "4  Key concepts of python",
    "section": "4.7 Identifying classes of objects",
    "text": "4.7 Identifying classes of objects\nOver the next chapters, you will realize that the pyspark API does not have many standard python functions. So most of its functionality resides in class methods. As a result, the capability of understanding the objects that you have in your python program, and, identifying its classes will be crucial while you are developing you Spark applications.\nEvery existing object in python represents an instance of a class. In other words, every object is associated to a given class. You can always identify the class of an object, by applying the type() function over this object."
  },
  {
    "objectID": "04-dataframes.html",
    "href": "04-dataframes.html",
    "title": "5  Spark DataFrames",
    "section": "",
    "text": "In this chapter, you will understand how Spark represents and manages tables (or tabular data). Different programming languages and frameworks use different names to describe a table. But, in Apache Spark, tables are referred as Spark DataFrames.\nIn pyspark, these DataFrames are stored inside python objects of class pyspark.sql.DataFrame, and all the methods present in this class, are commonly referred as the DataFrame API of Spark. This is the most important API of Spark. Much of your Spark applications will heavily use this API to compose your data transformations and data flows (Chambers and Zaharia 2018)."
  },
  {
    "objectID": "04-dataframes.html#spark-dataframes-versus-spark-datasets",
    "href": "04-dataframes.html#spark-dataframes-versus-spark-datasets",
    "title": "5  Spark DataFrames",
    "section": "5.1 Spark DataFrames versus Spark Datasets",
    "text": "5.1 Spark DataFrames versus Spark Datasets\nSpark have two notions of structured data: DataFrames and Datasets. In summary, a Spark Dataset, is a distributed collection of data (Apache Spark Official Documentation 2022). In contrast, a Spark DataFrame is a Spark Dataset organized into named columns (Apache Spark Official Documentation 2022).\nThis means that, Spark DataFrames are very similar to tables as we know in relational databases - RDBMS. So in a Spark DataFrame, each column has a name, and they all have the same number of rows. Furthermore, all the rows inside a column must store the same type of data, however, each column can store a different type of data.\nBut Spark Datasets are considered a collection of any type of data. So a Dataset might be a collection of unstructured data as well, like log files, JSON and XML trees, etc. Spark Datasets can be created and managed trough the Dataset API of Spark. But this API is available only in Scala and Java API’s. Because of this, we do not act directly on Datasets with pyspark, only DataFrames.\nThat’s ok, because for the most part of applications, we do want to use DataFrames, and not Datasets, to represent our data."
  },
  {
    "objectID": "04-dataframes.html#building-a-spark-dataframe",
    "href": "04-dataframes.html#building-a-spark-dataframe",
    "title": "5  Spark DataFrames",
    "section": "5.2 Building a Spark DataFrame",
    "text": "5.2 Building a Spark DataFrame\nThere are some different methods to create a Spark DataFrame. Because a DataFrame is basically a Dataset of type Row, we can build a DataFrame from a collection of Row’s, through the createDataFrame() method from your Spark Session:\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\nfrom datetime import date\nfrom pyspark.sql import Row\n\ndata = [\n  Row(id = 1, value = 28.3, date = date(2021,1,1)),\n  Row(id = 2, value = 15.8, date = date(2021,2,2)),\n  Row(id = 3, value = 20.1, date = date(2021,3,6)),\n  Row(id = 4, value = 12.6, date = date(2021,4,15)),\n]\n\ndf = spark.createDataFrame(data)\ndf\n\nDataFrame[id: bigint, value: double, date: date]\n\n\nIn the above example, we use the Row() constructor to build 4 rows to our new DataFrame df. The result is a DataFrame with\nFor example, with the code below, we are creating a DataFrame called students. To do this, we create two python lists (data and columns), then, deliver these lists to createDataFrame() method. Each element of data is a python tuple that represents a row in the students DataFrame.\n\ndata = [\n  (12114, 'Anne', 21, 1.56, 8, 9, 10, 9, 'Economics', 'SC'),\n  (13007, 'Adrian', 23, 1.82, 6, 6, 8, 7, 'Economics', 'SC'),\n  (10045, 'George', 29, 1.77, 10, 9, 10, 7, 'Law', 'SC'),\n  (12459, 'Adeline', 26, 1.61, 8, 6, 7, 7, 'Law', 'SC'),\n  (10190, 'Mayla', 22, 1.67, 7, 7, 7, 9, 'Design', 'AR'),\n  (11552, 'Daniel', 24, 1.75, 9, 9, 10, 9, 'Design', 'AR')\n]\n\ncolumns = [\n  'StudentID', 'Name', 'Age', 'Height', 'Score1',\n  'Score2', 'Score3', 'Score4', 'Course', 'Department'\n]\n\nstudents = spark.createDataFrame(data, columns)\nstudents\n\nDataFrame[StudentID: bigint, Name: string, Age: bigint, Height: double, Score1: bigint, Score2: bigint, Score3: bigint, Score4: bigint, Course: string, Department: string]\n\n\nAnother example, is to read (or import) a file and convert it to a DataFrame. Spark can read a variety of file formats, including CSV, Parquet, JSON, ORC and Binary files. Furthermore, Spark can connect to other databases and import tables from them, using JDBC connections.\n\npath = \"examples/src/main/resources/people.csv\"\n\ndf = spark.read.csv(path)\ndf.show()\n\n\n\n\n\nApache Spark Official Documentation. 2022. Documentation for Apache Spark 3.2.1; Available at: https://spark.apache.org/docs/latest/.\n\n\nChambers, Bill, and Matei Zaharia. 2018. Spark: The Definitive Guide: Big Data Processing Made Simple. Sebastopol, CA: O’Reilly Media."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Apache Spark Official Documentation. 2022. Documentation for\nApache Spark 3.2.1; Available at: https://spark.apache.org/docs/latest/.\n\n\nChambers, Bill, and Matei Zaharia. 2018. Spark: The Definitive\nGuide: Big Data Processing Made Simple. Sebastopol, CA: O’Reilly\nMedia.\n\n\nKarau, Holden, Andy Konwinski, Patrick Wendell, and Matei Zaharia. 2015.\nLearning Spark. Sebastopol, CA: O’Reilly Media."
  }
]
# Transforming your Spark DataFrame - Part 2

## Introduction

At @sec-transforming-dataframes-part1 I introduced six core types of transformations over Spark DataFrames. In this chapter, I will expand your knowledge by introducing five more types of transformations available to Spark DataFrames, which are:

- Removing duplicated values;
- Merging multiple DataFrames with UNION operations;
- Merging multiple DataFrames with JOIN operations;
- Rows to columns with Pivot operations;
- Collecting and explode operations;
- Replacing and removing null values;

```{python}
#| include: false
# To execute the next chunks with success, we need to start a Spark Session
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()
sc = spark.sparkContext
sc.setLogLevel("OFF")
```

## Removing duplicated values from your DataFrame

Removing duplicated values from DataFrames is a very commom operation in ETL pipelines. Basically, in Spark, you have two options to remove duplicated values, which are:

- `distinct()` which removes all duplicated values considering the combination of all columns in the DataFrame;
- `drop_duplicates()` or `dropDuplicates()` which removes all duplicated values considering a specific combination of columns in the DataFrame;

These methods above are all DataFrames methods. Is worth mentioning that the methods `drop_duplicates()` and `dropDuplicates()` are equivalent. They both mean the same thing, and have the same arguments and perform the same operation.

Furthermore, when you run `drop_duplicates()` or `dropDuplicates()` without arguments, they use by default the combination of all columns in the DataFrame to locate the duplicated values. As consequence, over this specific situation, the methods `drop_duplicates()` or `dropDuplicates()` become equivalent to the `distinct()` method. Because they use the combination of all columns in the DataFrame.

## Replacing and removing `null` values



## Applying UNION operations

When you have many individual DataFrames that have the same columns, and you want to unify them into a single big DataFrame that have all the rows from these different DataFrames, you want to perform an UNION operation.

An UNION operation works on a pair of DataFrames. It returns the row-wise union of these two DataFrames. In pyspark, we perform UNION operations by using the `union()` DataFrame method. So the expression `df1.union(df2)` creates a new DataFrame which contains all the rows from both the `df1` and `df2` DataFrames.


## Applying JOIN operations

A JOIN operation is another very commom operation that works on a pair of DataFrame. It is also used to bring data from scattered sources into a single unified DataFrame. While an UNION operation merges the two DataFrames by row, a JOIN operation uses the columns instead to merge the two DataFrames. In pyspark, we use the `join()` DataFrame method to perform JOIN operations.

### What is a JOIN ?

I imagine you are already familiar with JOIN operations. However, in order to build good and precise JOIN operations, is always important to know what a JOIN operation actually is.

One key characteristic of JOIN operations is it's key matching mechanism. A JOIN uses the columns you provide to **build a key**. This key is used to identify rows (or "observations") in both DataFrames. In other words, these keys identifies relationships between the two DataFrames. These relations are vital to the JOIN.

Let's use the `info` and `band_instruments` DataFrames as an example.

```{python}
info = [
    ('Mick', 'Rolling Stones', '1943-07-26', True),
    ('John', 'Beatles', '1940-09-10', True),
    ('Paul', 'Beatles', '1942-06-18', True),
    ('George', 'Beatles', '1943-02-25', True),
    ('Ringo', 'Beatles', '1940-07-07', True)
]

info = spark.createDataFrame(info)
```



![The relations between `info` and `band_instruments` DataFrames](./../Figures/keys_comparacao.png)


## Pivot operations

## Implode and explode operations

## More operations in Arrays and Map



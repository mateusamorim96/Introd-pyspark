# Transforming your Spark DataFrame - Part 2

## Introduction

At @sec-transforming-dataframes-part1 I introduced six core types of transformations over Spark DataFrames. In this chapter, I will expand your knowledge by introducing five more types of transformations available to Spark DataFrames, which are:

- Removing duplicated values;
- Merging multiple DataFrames with UNION operations;
- Merging multiple DataFrames with JOIN operations;
- Rows to columns with Pivot operations;
- Collecting and explode operations;
- Replacing and removing null values;

```{python}
#| include: false
# To execute the next chunks with success, we need to start a Spark Session
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()
sc = spark.sparkContext
sc.setLogLevel("OFF")
```

## Removing duplicated values from your DataFrame

Removing duplicated values from DataFrames is a very commom operation in ETL pipelines. Basically, in Spark, you have two options to remove duplicated values, which are:

- `distinct()` which removes all duplicated values considering the combination of all columns in the DataFrame;
- `drop_duplicates()` or `dropDuplicates()` which removes all duplicated values considering a specific combination of columns in the DataFrame;

These methods above are all DataFrames methods. Is worth mentioning that the methods `drop_duplicates()` and `dropDuplicates()` are equivalent. They both mean the same thing, and have the same arguments and perform the same operation.

Furthermore, when you run `drop_duplicates()` or `dropDuplicates()` without arguments, they use by default the combination of all columns in the DataFrame to locate the duplicated values. As consequence, over this specific situation, the methods `drop_duplicates()` or `dropDuplicates()` become equivalent to the `distinct()` method. Because they use the combination of all columns in the DataFrame.

## Replacing and removing `null` values



## Applying UNION operations

When you have many individual DataFrames that have the same columns, and you want to unify them into a single big DataFrame that have all the rows from these different DataFrames, you want to perform an UNION operation.

An UNION operation works on a pair of DataFrames. It returns the row-wise union of these two DataFrames. In pyspark, we perform UNION operations by using the `union()` DataFrame method. So the expression `df1.union(df2)` creates a new DataFrame which contains all the rows from both the `df1` and `df2` DataFrames.


## Applying JOIN operations

A JOIN operation is another very commom operation that is also used to bring data from scattered sources into a single unified DataFrame. In pyspark, we can build JOIN operations by using the `join()` DataFrame method. This method accepts tree arguments, which are:

- `other`: the DataFrame you want to JOIN with (i.e. the DataFrame on the right side of the JOIN);
- `on`: a column name, or a list of column names, that represents the key (or keys) of the JOIN;
- `how`: the kind of JOIN you want to perform (inner, full, left, right);

As a first example, let's use the `info` and `band_instruments` DataFrames. With the code below, you can quickly re-create these two DataFrames in your session:

```{python}
info = [
    ('Mick', 'Rolling Stones', '1943-07-26', True),
    ('John', 'Beatles', '1940-09-10', True),
    ('Paul', 'Beatles', '1942-06-18', True),
    ('George', 'Beatles', '1943-02-25', True),
    ('Ringo', 'Beatles', '1940-07-07', True)
]

info = spark.createDataFrame(
    info,
    ['name', 'band', 'born', 'children']
)

band_instruments = [
    ('John', 'guitar'),
    ('Paul', 'bass'),
    ('Keith', 'guitar')
]

band_instruments = spark.createDataFrame(
    band_instruments,
    ['name', 'plays']
)
```

If you look closely to these two DataFrames, you will probably notice that they both describe musicians from two famous rock bands from 60's and 70's. The `info` DataFrame have more personal or general informations about the musicians, while the `band_instruments` DataFrame have only data about the main musical instruments that they play.

```{python}
info.show()
```

```{python}
band_instruments.show()
```

It might be of your interest, to have a single DataFrame that contains both the personal information and the musical instrument of each musician. In this case, you can build a JOIN operation between these DataFrames to get this result. An example of this JOIN in pyspark would be:

```{python}
info.join(band_instruments, on = 'name', how = 'left')\
    .show(5)
```

In the example above, we are performing a *left join* between the two DataFrames, using the `name` column as the JOIN key. Now, we have a single DataFrame with all 5 columns from both DataFrames (`plays`, `children`, `name`, `band` and `born`).


### What is a JOIN ?

I imagine you are already familiar with JOIN operations. However, in order to build good and precise JOIN operations, is very important to know what a JOIN operation actually is. So let's revisit it.

A JOIN operation merges two different DataFrames together into a single unified DataFrame. It does this by using a column (or a set of columns) as keys to identify the observations of both DataFrames, and connects these observations together.

A JOIN (like UNION) is also an operation that works on a pair of DataFrames. It is very commom to refer to this pair as "the sides of the JOIN". That is, the DataFrame on the left side of the JOIN, and the DataFrame on the right side of the JOIN. Or also, the DataFrames "A" (left side) and "B" (right side).

The main idea (or objective) of the JOIN is to bring all data from the DataFrame on the right side, into the DataFrame on the left side. In other words, a JOIN between DataFrames A and B results into a DataFrame C which contains all columns and rows from both DataFrames A and B.

In an UNION operation, both DataFrames must have the same columns, because in an UNION operation you are concatenating both DataFrames together vertically, so the number of columns (or the "width" of the tables) need to match. However, in a JOIN operation, both DataFrames only need to have at least one column in commom. Apart from that, in a JOIN, both DataFrames can have very different structure and columns from each other.

One key characteristic of JOIN operations is it's key matching mechanism. A JOIN uses the columns you provide to **build a key**. This key is used to identify rows (or "observations") in both DataFrames. In other words, these keys identifies relationships between the two DataFrames. These relations are vital to the JOIN.

If we go back to `info` and `band_instruments` DataFrames, and analyse them for a bit more, we can see that they both have a `name` column which contains the name of the musician being described on the current row. This `name` column can be used as **the key** of the JOIN. Because this column is available on both DataFrames, and it can be used to identify a single observation (or a single musician) present in each DataFrame.

So the JOIN key is a column (or a combination of columns) that can identify what observations are (and are not) present on both DataFrames. At @fig-keys-comparison, we can see the observations from `info` and `band_instruments` in a visual manner. You see in the figure that both Paul and John are described in both DataFrames. At the same time, Ringo, Mick and George are present only on `info`, while `Keith` is only at `band_instruments`. 

![The relations between `info` and `band_instruments` DataFrames](./../Figures/keys_comparacao.png){#fig-keys-comparison}

In a certain way, you can see the JOIN key as a way to **identify relationships between the two DataFrames**. A JOIN operation use these relationships to merge your DataFrames in a precise way. A JOIN does not simply horizontally glue two DataFrames together. It uses the JOIN key to perform a matching process between the observations of the two DataFrames.

This matching process ensures that the data present DataFrame "B" is correctly transported to the DataFrame "A". In other words, it ensures that the oranges are paired with oranges, apples with apples, bananas with bananas, you got it.

Just to describe visually what this matching process is, we have the @fig-join-matching below. In this figure, we have two DataFrames on the left and center of the image, which represents the inputs of the JOIN. We also have a third DataFrame on the right side of the image, which is the output (or the result) of the JOIN.

In this specific example, the column that represents the JOIN key is the `ID` column. Not only this column is present on both DataFrames, but it also represents an unique identifier to each person described in both tables. And that is precisely the job of a JOIN key. It represents a way to identify observations (or "persons", or "objects", etc.) on both tables.

You can see at @fig-join-matching, that when the `ID` 100 is found on the 1st row of the left DataFrame, the JOIN initiates a lookup/matching process on the center DataFrame, looking for a row in the DataFrame that matches this `ID` 100. When it finds this `ID` 100 (on the 4th row of the center DataFrame), it captures and connects these two rows on both DataFrames, because these rows describes the same person (or observation), and because of that, they should be connected. This same matching process happens for all remaining `ID` values.


![The matching process of a JOIN operation](./../Figures/pareamento1.png){#fig-join-matching}

### The different types of JOIN

JOIN operations actually comes in different flavours (or types). The four main known types of JOINs are: *full*, *left*, *right* and *inner*. All of these different types of JOIN perform the same steps and matching processes that we described on the previous section. But they differ on the treatment they do to unmatched observations. In other words, these different types of JOINs differ on **what they do in cases when an observation is not found on both DataFrames of the JOIN** (e.g. when an observation is found only on table A).

In other words, all these four types will perform the same matching process between the two DataFrames, and will connect observations that are found in both DataFrames. However, which rows are included in the final output is what changes between each type (or "flavour") of JOIN.

In this situation, the words "left" and "right" are identifiers to the DataFrames involved on the JOIN operation. That is, the word *left* refers to the DataFrame on the left side of the JOIN, while the word *right* refers to the DataFrame on the right side of the JOIN.

A very useful way of understanding these different types of JOINs is to represent both DataFrames as numerical sets (as we learn in mathematics). The @fig-join-sets gives you a visual representation of each type of JOIN using this "set model" of representing JOINs. Remember, all of these different types of JOIN work the same way, they just do different actions when an observation is not found on both tables.

The most "complete" and "greedy" type of JOIN is the *full join*. Because this type returns all possible combinations of both DataFrames. In other words, this type of JOIN will result in a DataFrame that have all observations from both DataFrames. It does not matter if an observation is present only on table A, or only on table B, or maybe, on both tables. A *full join* will always try to connect as much observation as it can.

![A visual representation of JOIN sets](./../Figures/join-sets.png){#fig-join-sets}

That is why the *full join* is represented on @fig-join-sets as the union between the two tables (or the two sets). In contrast, an *inner join* is the intersection of the two tables (or two sets). That is, an *inner join* will result in a new DataFrame which contains solely the observations that could be found on both tables. If a specific observation is found only on one table of the JOIN, this observation will be automatically removed from the result of the *inner join*.

If we go back to the `info` and `band_instruments` DataFrames, and use them as an example, you can see that only Paul and John are included on the result of an *inner join*. While in a *full join*, all musicians are included on the resulting DataFrame.

```{python}
# An inner join between `info` and `band_instruments`:
info.join(band_instruments, on = 'name', how = 'inner')\
    .show()
```

```{python}
# A full join between `info` and `band_instruments`:
info.join(band_instruments, on = 'name', how = 'full')\
    .show()
```

On the other hand, the *left join* and *right join* are kind of self-explanatory. On a *left join*, all the observations from the left DataFrame are kept intact on the resulting DataFrame of the JOIN, regardless of whether these observations were found or not on the right DataFrame. In contrast, an *right join* is the opposite of that. So, all observations from the right DataFrame are kept intact on the resulting DataFrame of the JOIN.


In pyspark, you can define the type of JOIN you want to use by setting the `on` argument at `join()` method. This argument accepts a string with the type of JOIN you want to use as input.

- `on = "left"`: make a *left join*;
- `on = "right"`: make a *right join*;
- `on = "full"`: make a *full join*;
- `on = "inner"`: make an *inner join*;
- `on = "semi"`: make a *semi join*;
- `on = "anti"`: make an *anti join*;

You can see on the list above, that `pyspark` do have two more types of JOINs, which are the *semi join* and *anti join*. These are "filtering types" of JOINs. Because they perform the matching process, and only filter the rows from table A (i.e. the DataFrame on the left side of the JOIN) based on the matches found on table B (i.e. the DataFrame on the right side of the JOIN).

In other words, these both types are used as a filter mechanism, and not as a merge mechanism. When you use these two types, instead of merging two DataFrames together, you are interested in filtering the rows of DataFrame A based on the existence of these rows in DataFrame B.

This is different from what we learned on *left*, *right*, *full* and *inner* types, because they do not only change which rows are included in the final result, but they also add the columns from table B into table A. Because of this behavior, these four main types are usually called as "additive types" of JOIN, since they are always adding data from table B into table A, i.e. they are merging the two tables together.

In more details, an *anti join* perform the exact opposite matching process of an *inner join*. This means that an *anti join* will always result in a new DataFrame that contains solely the observations that exists only on one DataFrame of the JOIN. In other words, the observations that are found on both tables are automatically removed from the resulting DataFrame of the JOIN. If we look at the example below, we can see that both John and Paul were removed from the resulting DataFrame of the *anti join*, because these two musicians are present on both DataFrames:

```{python}
info.join(band_instruments, on = 'name', how = 'anti')\
    .show()
```

In contrast, a *semi join* is equivalent to an *inner join*, with the difference that it does not adds the column from table B into table A. So this type of JOIN filter the rows from DataFrame A that also exists in DataFrame B. If an observation is found on both tables, this observation will appear on the resulting DataFrame.

```{python}
info.join(band_instruments, on = 'name', how = 'semi')\
    .show()
```

Just to keep using our visual model of sets, on @fig-join-sets2 you can see the *semi* and *anti* types represented as numerical sets.

![The two filter types of JOIN](./../Figures/join-sets2.png){#fig-join-sets2}

### The relations between keys

Since a JOIN operation works on a pair of DataFrames, there are three different possible relations between the keys found in both DataFrames.

### How JOINs build duplicated values



## Pivot operations

## Implode and explode operations

## More operations in Arrays and Map



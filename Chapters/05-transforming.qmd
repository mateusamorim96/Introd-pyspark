---
title: "Transforming your Spark DataFrame"
---

## Introduction

Virtually every data analysis or data pipeline will include some ETL (*Extract, Transform, Load*) process, and the T is an essential part of it. Because, you almost never have an input data, or a initial DataFrame that perfectly fits your needs.

That means, that you almost always have to transform the initial data that you have, to a specific format that you can use in your analysis. In this chapter, you will learn how to apply some of these basic transformations to your Spark DataFrame.

## Defining transformations

Spark DataFrames are immutable, meaning that, they cannot be directly changed. But you can use an existing DataFrame to create a new one, based on a set of transformations. In other words, you define a new DataFrame as a transformed version of an older DataFrame.

Basically every `pyspark` program that you write will have such transformations. Spark support many types of transformations, however, in this chapter, we will focus on four basic transformations that you can apply to a DataFrame:

-   Filtering rows;
-   Sorting rows;
-   Adding or deleting columns;
-   Calculate aggregates;

Therefore, when you apply one of the above transformations to an existing DataFrame, you will get a new DataFrame as a result. You usually combine multiple transformations together to get your desired result. As a first example, lets get back to the `df` DataFrame:

```{python}
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()
from datetime import date
from pyspark.sql import Row

data = [
  Row(id = 1, value = 28.3, date = date(2021,1,1)),
  Row(id = 2, value = 15.8, date = date(2021,1,1)),
  Row(id = 3, value = 20.1, date = date(2021,1,2)),
  Row(id = 4, value = 12.6, date = date(2021,1,3))
]

df = spark.createDataFrame(data)
```

In the example below, to create a new DataFrame called `big_values`, we begin with the `df` DataFrame, then, we filter its rows where `value` is greater than 15, then, we select `date` and `value` columns, then, we sort the rows based on the `value` column. So, this set of sequential transformations (filter it, then, select it, then, order it, ...) defines what this new `big_values` DataFrame is.

```{python}
from pyspark.sql.functions import col
# You define a chain of transformations to
# create a new DataFrame
big_values = df\
  .filter(col('value') > 15)\
  .select('date', 'value')\
  .orderBy('value')
```

Thus, to apply a transformation to an existing DataFrame, we use DataFrame methods such as `select()`, `filter()`, `withColumn()`, `orderBy()` and `agg()`. Remember, these are methods from the python class that defines Apache Spark DataFrame's (i.e. `pyspark.sql.dataframe.DataFrame`). This means that you can apply these transformations only to Spark DataFrames, and no other kind of python object.

Each one of these methods create a *lazily evaluated transformation*. Once again, we see the **lazy** aspect of Spark doing its work here. All these transformation methods are lazily evaluated, meaning that, Spark will only check if they make sense with the initial DataFrame that you have. Spark will not actually perform these transformations on your initial DataFrame, not untill you trigger these transformations with an **action**.

## Triggering calculations with actions {#sec-dataframe-actions}

Therefore, Spark will avoid performing any heavy calculation until such calculation is really needed. But how or when Spark will face this decision? **When it encounters an action**. An action is the tool you have to trigger Spark to actually perform the transformations you have defined.

> An action instructs Spark to compute the result from a series of transformations. [@chambers2018].

There are four kinds of actions in Spark:

-   Showing an output in the console;
-   Writing data to some file or data source;
-   Collecting data from a Spark DataFrame to native objects in python (or Java, Scala, R, etc.);
-   Counting the number of rows in a Spark DataFrame;

You already know the first type of action, because we used it before with the `show()` method. This `show()` method is an action by itself, because you are asking Spark to show some output to you. So we can make Spark to actually calculate the transformations that defines the `big_values` DataFrame, by asking Spark to show this DataFrame to us.

```{python}
big_values.show()
```

Another very useful action is the `count()` method, that gives you the number of rows in a DataFrame. To be able to count the number of rows in a DataFrame, Spark needs to access this DataFrame in the first place. That is why this `count()` method behaves as an action. Spark will perform the transformations that defines `big_values` to access the actual rows of this DataFrame and count them.

```{python}
big_values.count()
```

Furthermore, sometimes, you want to collect the data of a Spark DataFrame to use it inside python. In other words, sometimes you need to do some work that Spark cannot do by itself. To do so, you collect part of the data that is being generated by Spark, and store it inside a normal python object to use it in a standard python program.

That is what the `collect()` method do. It transfers all the data of your Spark DataFrame into a standard python list that you can easily access with python. More specifically, you get a python list full of `Row()` values:

```{python}
data = big_values.collect()
print(data)
```

The `take()` method is very similar to `collect()`. But you usually apply `take()` when you need to collect just a small section of your DataFrame (and not the entire thing), like the first `n` rows.

```{python}
n = 1
first_row = big_values.take(n)
print(first_row)
```

The last action would be the `write()` method, but we will explain this method latter, in the @sec-import-export.

## Understanding narrow and wide transformations

There are two kinds of transformations in Spark: narrow and wide transformations. Remember, a Spark DataFrame is divided into many small parts (called partitions), and, these parts are spread across the cluster. The basic difference between narrow and wide transformations, is if the transformation forces Spark to read data from multiple partitions to generate a single part of the result of that transformation.

More technically, narrow transformations are simply transformations where 1 input data (or 1 partition of the input DataFrame) contributes to only 1 partition of the output.

![Presenting narrow transformations](../Figures/narrow-transformations.png){#fig-narrow-transformations fig-align="center"}

In other words, each partition of your input DataFrame will be used (*separately*) to generate one individual part of the result of your transformation. As another perspective, you can understand narrow transformations as those where Spark does not need to read the entire input DataFrame to generate a single and small piece of your result.

A classic example of narrow transformation is a filter. For example, suppose you have three students (Anne, Carls and Mike), and that each one has a bag full of blue, orange and red balls mixed. Now, suppose you asked them to collect all the red balls of these bags, and combined them in a single bag.

To do this task, Mike does not need to know what balls are inside of the bag of Carls or Anne. He just need to collect the red balls that are solely on his bag. At the end of the task, each student will have a part of the end result (that is, all the red balls that were in his own bag), and they just need to combine all these parts to get the total result.

The same thing applies to filters in Spark DataFrames. When you filter all the rows where the column `state` is equal to `"Alaska"`, Spark will filter all the rows in each partition separately, and then, will combine all the outputs to get the final result.

In contrast, wide transformations are the opposite of that. In wide transformations, Spark needs to use more than 1 partition of the input DataFrame to generate a small piece of the result.

![Presenting wide transformations](../Figures/wide-transformations.png){#fig-wide-transformations fig-align="center"}

When this kind of transformation happens, each worker node of the cluster needs to share his partition with the others. In other words, what happens is a partition shuffle. Each worker node sends his partition to the others, so they can have access to it, while performing their assigned tasks.

Partition shuffles are a very popular topic in Apache Spark, because they can be a serious source of inefficiency in your Spark application [@chambers2018]. In more details, when these shuffles happens, Spark needs to write data back to the hard disk of the computer, and this is not a very fast operation. It does not mean that wide transformations are bad or slow, just that the shuffles they are producing can be a problem.

A classic example of wide operation is a grouped aggregation. For example, lets suppose we had a DataFrame with the daily sales of multiple stores spread across the country, and, we wanted to calculate the total sales per city/region. To calculate the total sales of a specific city, like SÃ£o Paulo, Spark would need to find all the rows that corresponds to this city, before adding the values, and these rows can be spread across multiple partitions of the cluster.

## Filtering rows of your DataFrame

To filter specific rows of a DataFrame, `pyspark` offers two equivalent methods called `where()` and `filter()`. In other words, they both do the same thing, and work in the same way. You should give to these methods, a logical expression that translates what you want to filter.

To demonstrate some of the examples in this and some of the next sections, we will use a different DataFrame, called `transf`. With the code below, you can import the data from `trans_reform.csv` to create this DataFrame in your Spark Session. Remember that, this CSV file is freely available to download at the repository of this book[^transforming-1].

[^transforming-1]: https://github.com/pedropark99/Introd-pyspark/tree/main/Data

```{python}
#| cache: true
from pyspark.sql.types import StructType, StructField
from pyspark.sql.types import DoubleType, StringType, IntegerType, TimestampType
path = "../Data/transf_reform.csv"
schema = StructType([
  StructField('datetime', TimestampType(), False),
  StructField('user', StringType(), True),
  StructField('value', DoubleType(), True),
  StructField('transferid', IntegerType(), False),
  StructField('country', StringType(), True),
  StructField('description', StringType(), True)
])

transf = spark.read\
  .csv(path, schema = schema, sep = ";", header = True)
```

This `transf` DataFrame contains bank transfer records from a fictitious bank. Column `datetime` presents the date and time when the transfer occurred; `user` is the bank user that did the transfer; `value` presents the value that was transferred; `transferid` is an unique ID for the transfer; `country` show us the target country of the transfer; and `description` store some description of this transfer if needed.

```{python}
transf.show(5)
```

As a first example, lets suppose you wanted to inspect all the rows where `value` is less than 1000. To do so, you can use the following code:

```{python}
transf\
  .filter("value < 1000")\
  .show(5)
```

Writing simple SQL logical expression inside a string is the most easy and "clean" way to create a filter expression in `pyspark`. However, you could write the same exact expression in a more "pythonic" way, using the `col()` function from `pyspark.sql.functions`.

```{python}
from pyspark.sql.functions import col

transf\
  .filter(col("value") < 1000)\
  .show(5)
```

You still have a more verbose alternative, that does not require the `col()` function. With this method, you refer to the specific column using the dot operator (`.`), like in the example below:

```{python}
#| eval : false
# This will give you the exact
# same result of the examples above
transf\
  .filter(transf.value < 1000)
```

### Logical operators available

As we saw in the previous section, there are two ways to write logical expressions in `pyspark`: write SQL logical expressions inside a string; or, write python logical expressions using the `col()` function. 

If you choose to write the SQL logical expressions in a string, you need to use the logical operators of SQL in your expression (not the logical operators of python). In the other hand, if you choose to write in the "python" way, than, you need to use the logical operators of python.

The logical operators of SQL are described in the table below:

| Operator | Example of expression| Meaning of the expression                               |
|----------|----------------------|---------------------------------------------------------|
| \<       |`x < y`               |is `x` less than `y`?                                    |
| \>       |`x > y`               |is `x` greater than `y`?                                 |
| \<=      |`x <= y`              |is `x` less or equal than `y`?                           |
| \>=      |`x >= y`              |is `x` greater or equal than `y`?                        |
| ==       |`x == y`              |is `x` equal to `y`?                                     |
| !=       |`x != y`              |is `x` not equal to `y`?                                 |
| in       |`x in y`              |is `x` one of the values listed in `y`?                  |
| and      |`x and y`             |both logical expressions `x` and `y` are true?           |
| or       |`x or y`              |at least one of logical expressions `x` and `y` are true?|
| not      |`not x`               |is the logical expression `x` not true?                  |

: List of logical operators of SQL {#tbl-logical-operators-sql}


The logical operators of python are described in the table below:

| Operator | Example of expression| Meaning of the expression                               |
|----------|----------------------|---------------------------------------------------------|
| \<       |`x < y`               |is `x` less than `y`?                                    |
| \>       |`x > y`               |is `x` greater than `y`?                                 |
| \<=      |`x <= y`              |is `x` less or equal than `y`?                           |
| \>=      |`x >= y`              |is `x` greater or equal than `y`?                        |
| ==       |`x == y`              |is `x` equal to `y`?                                     |
| !=       |`x != y`              |is `x` not equal to `y`?                                 |
| &        |`x & y`               |both logical expressions `x` and `y` are true?           |
| \|       |`x | y`               |at least one of logical expressions `x` and `y` are true?|
| ~        |`~x`                  |is the logical expression `x` not true?                  |

: List of logical operators of python {#tbl-logical-operators-python}

### Connecting multiple logical expressions

Sometimes, you need to write more complex logical expressions to correctly describe the rows you are interested in. That is, when you combine multiple logical expressions together.

As an example, lets suppose you wanted all the rows in `transf` DataFrame, where `value` is smaller than 1000, and, the `country` is Brazil, and, `user` is Eduardo. These conditions are dependent, that is, they are connected to each other. That is why I used the `and` keyword in the example below, to connect these three conditions together.

```{python}
condition = '''
  value < 1000 and country == 'Brazil' and user == 'Eduardo'
'''

transf\
  .filter(condition)\
  .show(5)
```

I could translate this logical expression into the "pythonic" way (using the `col()` function). However, I would have to surround each individual expression by parentheses, and, use the `&` operator to substitute the `and` keyword.

```{python}
transf\
  .filter(
    (col('value') < 1000) & (col('country') == 'Brazil') & (col('user') == 'Eduardo')
  )\
  .show(5)
```

This a very important detail, because is very easy to forget it. When building your complex logical expressions, if you are using the `col()` function, always remember to surround each expression by a pair of parentheses. Otherwise, you will get a very confusing and useless error message, like this:

```{python}
#| eval: false
transf\
  .filter(
    col('value') < 1000 & col('country') == 'Brazil' & col('user') == 'Eduardo'
  )\
  .show(5)
```


```{verbatim}
Traceback (most recent call last):
  File "<stdin>", line 3, in <module>
  File "/usr/local/spark/python/pyspark/sql/column.py", line 111, in _
    njc = getattr(self._jc, name)(jc)
  File "/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py", line 1321, in __call__
  File "/usr/local/spark/python/pyspark/sql/utils.py", line 111, in deco
    return f(*a, **kw)
  File "/usr/local/spark/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py", line 330, in get_return_value
py4j.protocol.Py4JError: An error occurred while calling o89.and. Trace:
py4j.Py4JException: Method and([class java.lang.Integer]) does not exist
	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)
	at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)
	at py4j.Gateway.invoke(Gateway.java:274)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
	at java.lang.Thread.run(Thread.java:748)
```

In the above examples, we have logical expressions that are dependent on each other. But, lets suppose these conditions were independent. In this case, we would use the `or` keyword, instead of `and`. Now, Spark will look for every row of `transf` where `value` is smaller than 1000, or, `country` is Brazil, or, `user` is Eduardo.

```{python}
condition = '''
  value < 1000 or country == 'Brazil' or user == 'Eduardo'
'''

transf\
  .filter(condition)\
  .show(5)
```

To translate this expression into the pythonic way, we have to substitute the `or` keyword by the `|` operator, and surround each expression by parentheses again:

```{python}
transf\
  .filter(
    (col('value') < 1000) | (col('country') == 'Brazil') | (col('user') == 'Eduardo')
  )\
  .show(5)
```


You can increase the complexity of your logical expressions by mixing dependent expressions with independent expressions. For example, to filter all the rows where `country` is Germany, and, `user` is either Eduardo or Ana, you would have the following code:

```{python}
condition = '''
  (user == 'Eduardo' or user == 'Ana') and country == 'Germany'
'''

transf\
  .filter(condition)\
  .show(5)
```

If you investigate the above condition carefully, maybe, you will identify that this condition could be rewritten in a simpler format, by using the `in` keyword. This way, Spark will look for all the rows where `user` is equal to one of the listed values (Eduardo and Ana), and, that `country` is Germany.

```{python}
condition = '''
  user in ('Eduardo', 'Ana') and country == 'Germany'
'''
```


### Translating the `in` keyword to the pythonic way

Python does have a `in` keyword just like SQL, but, this keyword does not work as expected in `pyspark`. To write a logical expression, using the pythonic way, that filters the rows where a column is equal to one of the listed values, you can use the `isin()` method. 

This method belongs to the `Column` class, so, you should always use `isin()` after a column name or a `col()` function. In the example below, we are filtering the rows where `user` is Eduardo or JÃºlio Cesar:

```{python}
transf\
  .filter(col('user').isin('Eduardo', 'JÃºlio Cesar'))\
  .show(5)
```

### Negating logical conditions

<!-- Describe the ~ operator -->

In some cases, is easier to describe what rows you **do not want** in your filter. In this case, you want to negate (or invert) your logical expression. For this, SQL provides the `not` keyword, that you place before the logical expression you want to negate.

For example, we can filter all the rows of `transf` where `user` is not equal to Ana. Remember, the methods `filter()` and `where()` are equivalents or synonymous (they both mean the same thing).

```{python}
condition = '''
  not user == 'Ana'
'''

transf\
  .where(condition)\
  .show(5)
```

To translate this expression to the pythonic way, we use the `~` operator. Because we are negating the logical expression as a whole, is important to surround the entire expression with parentheses.

```{python}
transf\
  .where(~(col('user') == 'Ana'))\
  .show(5)
```

If you forget to do this, Spark will think you are negating just the column (e.g. `~col('user')`), and not the entire expression. That would not make sense, and, as a result, Spark would throw an error:

```{python}
#| eval: false
transf\
  .where(~col('user') == 'Ana')\
  .show(5)
```

```{verbatim}
AnalysisException: cannot resolve '(NOT user)' due to data type mismatch: argument 1 requires boolean type, however, 'user' is of string type.;
'Filter (NOT user#25 = Ana)
```

Because the `~` operator is a little discrete and can go unnoticed, I sometimes use a different approach to negate my logical expressions. I make the entire expression equal to `False`. This way, I get all the rows where that particular expression is `False`. This makes my intention more visible in the code, but, is harder to write it.

```{python}
# Filter all the rows where `user` is not equal to
# Ana or Eduardo.
transf\
  .where( (col('user').isin('Ana', 'Eduardo')) == False )\
  .show(5)
```


### Filtering `null` values

Sometimes, the `null` values play an important role in your filter. You either want to collect all these `null` values, so you can investigate why they are null in the first place, or, you want to completely eliminate them from your DataFrame.

Because this is a special kind of value in Spark, with a special meaning (the "absence" of a value), you need to use a special syntax to correctly filter these values in your DataFrame. In SQL, you can use the `is` keyword to filter these values:

```{python}
transf\
  .where('user is null')\
  .show()
```

However, if you want to remove these values from your DataFrame, than, you can just negate (or invert) the above expression with the `not` keyword, like this:

```{python}
transf\
  .where('not user is null')\
  .show(5)
```

The `is` and `not` keywords have a special relation. Because you can create the same negation/inversion of the expression by inserting the `not` keyword in the middle of the expression (you can do this too in expressions with the `in` keyword). In other words, you might see in someone else's code this expression written in this form:

```{python}
transf\
  .where('user is not null')\
  .show(5)
```

This is a valid SQL logical expression, but is a strange one. Because we cannot use the `not` keyword in this manner on other kinds of logical expressions. Normally, we put the `not` keyword **before** the logical expression we want to negate, not in the middle of it. Anyway, just have in mind that this form of logical expression exists, and, that is a perfectly valid one.

When we translate the above examples to the "pythonic" way, many people tend to use the `null` equivalent of python, that is, the `None` value, in the expression. But as you can see in the result below, this method does not work as expected:

```{python}
transf\
  .where(col('user') == None)\
  .show()
```

The correct way to do this in `pyspark`, is to use the `isNull()` method from the `Column` class.

```{python}
transf\
  .where(col('user').isNull())\
  .show()
```

If you want to eliminate the `null` values, just use the inverse method `isNotNull()`.

```{python}
transf\
  .where(col('user').isNotNull())\
  .show(5)
```



## Selecting specific columns of your DataFrame

Sometimes, you need manage or transform the columns you have. For example, you might need to change the order of these columns, or, to delete/rename some of them. To do this, you can use the `select()` and `drop()` methods of your DataFrame. 

The `select()` method works very similarly to the `SELECT` statement of SQL. You basically list all the columns you want to keep in your DataFrame, in the specific order you want.

```{python}
transf\
  .select('transferid', 'datetime', 'user', 'value')\
  .show(5)
```

### Renaming your columns

Realize in the example above, that the column names can be delivered directly as strings to `select()`. This makes life pretty easy, but, it does not give you extra options. You may want to rename some of the columns, and, to do this, you need to use the `alias()` method from `Column` class. Since this is a method from `Column` class, I always use it after a `col()` function, or, after a column name.

```{python}
transf\
  .select(
    'datetime',
    col('transferid').alias('transferID'),  
    col('user').alias('clientName')
  )\
  .show(5)
```

### Dropping unnecessary columns

In some cases, your DataFrame just have too many columns and you just want to eliminate a few of them. In a situation like this, you can list the columns you want to drop from your DataFrame, inside the `drop()` method, like this:

```{python}
transf\
  .drop('user', 'description')\
  .show(5)
```


### You can add new columns with `select()`

When I say that `select()` works in the same way as the `SELECT` statement of SQL, it means that you can use `select()` to select columns that do not currently exist in your DataFrame, and add them to the final result. 

For example, I can select a new column (called `by_1000`) containing the `value` divided by 1000:

```{python}
transf\
  .select(
    'value',
    (col('value') / 1000).alias('by_1000')
  )\
  .show(5)
```

So this `by_1000` column do not exist in `transf` DataFrame, it was calculated and added to the final result by `select()`. The formula `col('value') / 1000` is the equation that defines what this `by_1000` column is, or, how it should be calculated. 

Besides that, `select()` provides a useful shortcut to reference all the columns of your DataFrame. That is, the star symbol (`*`) from the `SELECT` statement in SQL. This shortcut is very useful when you want to maintain all columns, and, add a new column, at the same time.

In the example below, we are adding the same `by_1000` column, however, we are bringing all the columns of `transf` together.

```{python}
transf\
  .select(
    '*',
    (col('value') / 1000).alias('by_1000')
  )\
  .show(5)
```


### Casting columns to a different data type

Spark try to do its best when guessing which is correct data type for the columns of your DataFrame. But, obviously, Spark can get it wrong, and, you end up deciding by your own which data type to use for a specific column.

While working with your DataFrame, you can use the `cast()` method inside `select()` to change the data type of a specific column. This `cast()` method belongs to the `Column` class, so, you should always use it after a column name, or, a `col()` function:

```{python}
transf\
  .select(
    'value',
    col('value').cast('int').alias('value_as_integer'),
    transf.value.cast('string').alias('value_as_string')
  )\
  .show(5)
```

To use this `cast()` method, you give the name of the data type (as a string) to which you want to cast the column. The main available data types to `cast()` are:

- `'string'`: correspond to `StringType()`;
- `'int'`: correspond to `IntegerType()`;
- `'long'`: correspond to `LongType()`;
- `'double'`: correspond to `DoubleType()`;
- `'date'`: correspond to `DateType()`;
- `'timestamp'`: correspond to `TimestampType()`;
- `'bool'`: correspond to `BooleanType()`;
- `'array'`: correspond to `ArrayType()`;
- `'dict'`: correspond to `MapType()`;

## Calculating or adding new columns to your DataFrame

Although you can add new columns with `select()`, this method is not specialized to do that. As consequence, when you want to add many new columns, it can become pretty annoying to write `select('*', new_column)` over and over again. That is why `pyspark` provides a special method called `withColumn()`.

This method has two arguments. First, is the name of the new column. Second, is the formula or equation that represents this new column. As an example, I could reproduce the same `by_1000` column like this:

```{python}
transf\
  .withColumn('by_1000', col('value') / 1000)\
  .show(5)
```



## Sorting rows of your DataFrame







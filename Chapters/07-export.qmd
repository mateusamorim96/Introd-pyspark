# Exporting data out of Spark {#sec-export}


```{python}
#| include: false
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()
sc = spark.sparkContext
sc.setLogLevel("OFF")
```


After you transformed your DataFrame and generated the results you need, you might need to actually export these results out of Spark, so you can:

- send this exported data to an external API.
- send these results to your manager or to your client.
- send this data to an ingest process that feeds some database. 

These are very commom patterns in all kinds of areas, and Spark offers some commom tools to meet this need.

## The `write` object as the main entrypoint

Every Spark session you start has an built-in `read` object that you can use to read data and import it into Spark (I described this object at @sec-read-files), and the same applies to writing data out of Spark. That is, Spark also offers a `write` object that you can use to write/output data out of Spark.

But in contrast to the `read` object, which is avaiable trough the `SparkSession` object (`spark`), this `write` object is available trough the `write` method of any `DataFrame` object. In other words, every DataFrame you create in Spark has a built-in `write` object that you can use to write/export the data present in this DataFrame out of Spark.

```{python}
#| cache: true
#| include: false
from pyspark.sql.types import StructType, StructField
from pyspark.sql.types import DoubleType, StringType
from pyspark.sql.types import LongType, TimestampType, DateType
path = "../Data/transf.csv"
schema = StructType([
  StructField('dateTransfer', DateType(), False),
  StructField('datetimeTransfer', TimestampType(), False),
  StructField('clientNumber', LongType(), False),
  StructField('transferValue', DoubleType(), False),
  StructField('transferCurrency', StringType(), False),
  StructField('transferID', LongType(), False),
  StructField('transferLog', StringType(), False),
  StructField('destinationBankNumber', LongType(), False),
  StructField('destinationBankBranch', LongType(), False),
  StructField('destinationBankAccount', StringType(), False)
])

transf = spark.read\
  .csv(path, schema = schema, sep = ";", header = True)
```

As an example, let's use the `transf` DataFrame that I presented at @sec-transforming-dataframes-part1. The `write` method of the `transf` DataFrame object is the main entrypoint to all the facilities that Spark offers to write/export `transf`'s data to somewhere else.

```{python}
transf.write
```


This `write` entrypoint is very similar in structure to the `read` entrypoint. Essentially, this write entrypoint have a collection of *write engines*. Each write engine is speciallized in writing data into a specific file format. So you have an engine for CSV files, another engine for JSON files, another to Parquet files, etc.

The main methods of the write entrypoint are:

- `mode()`: set the mode of the write process. This affects how the data will be written to the files, and how the process will behaviour in case exceptions/erros are raised during runtime.
- `option()`: to set an option to be used in the write process. This could be an option that is specific to the write engine used, or, an option that is global to the write process (that does not depend of the chosen write engine).
- `csv()`: the write engine to export data to a CSV file.
- `json()`: the write engine to export data to a JSON file.
- `parquet()`: the write engine to export data to a Parquet file.
- `orc()`: the write engine to export data to a ORC file.
- `text()`: the write engine to export data to a text file.
- `jdbc()`: saves the data of the current DataFrame into a database using the JDBC API.


## A simple example of writing an CSV file




## Number of partitions determines the number of files written


## Transforming to a Pandas DataFrame as a way to export data



## The `collect()` method as a way to export data

The `collect()` DataFrame method exports the DataFrame's data from Spark into a Python native object, more specifically, into a normal Python list. To some extent, this is a viable way to export data from Spark. Because by making data from Spark available as a normal/standard Python object, many new possibilities become open for us, such as:

- sending this data to another location via HTTP requests using the `request` Python package.
- sending this data by email using the `email` built-in Python package.
- sending this data by SFTP protocol with the `paramiko` Python package.
- sending this data to a cloud storage, such as Amazon S3 (using the `boto3` Python package).

By having the DataFrame's data easily available to Python as a Python list, we can do virtually anything with this data. We can use this data in basically anything that Python is capable of doing.

Just as a simple example, I needed to send the `transf` data to an fictitious endpoint using a `POST` HTTP request, the source code would probably be something similar to this:

```{python}
#| eval: false
import requests

dataframe_rows = transf.collect()

url = 'https://example.com/api/v1/transf'
for row in dataframe_rows:
    row_as_dict = row.asDict()
    requests.post(url, data = row_as_dict)
```
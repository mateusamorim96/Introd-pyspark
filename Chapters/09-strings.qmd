# Tools for string manipulation {#sec-string-tools}

```{python}
#| include: false
# To execute the next chunks with success, we need to start a Spark Session
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()
sc = spark.sparkContext
sc.setLogLevel("OFF")
```

Many of the world's data is represented (or stored) as text (or string variables). As a consequence, is very important to know the tools available to process and transform this kind of data, in any platform you use. In this chapter, we will focus on these tools.

Most of the functionality available in `pyspark` to process text data comes from functions available at the `pyspark.sql.functions` module. This means that processing and transforming text data in Spark usually involves applying a function on a column of a Spark DataFrame (by using DataFrame methods such as `withColumn()` and `select()`).


## The `logs` DataFrame

Over the next examples in this chapter, we will use the `logs` DataFrame, which contains various log messages registered at a fictitious IP adress. The data that represents this DataFrame is freely available trough the `logs.json` file, which you can download from the official repository of this book[^logs-download].

[^logs-download]: <https://github.com/pedropark99/Introd-pyspark/tree/main/Data>

Each line of this JSON file contains a message that was recorded by the logger of a fictitious system. Each log message have three main parts, which are: 1) the type of message (warning - `WARN`, information - `INFO`, error - `ERROR`); 2) timestamp of the event; 3) the content of the message. In the example below, we have an example of message:

> \[INFO\]: 2022-09-05 03:35:01.43 Looking for workers at South America region;

To import `logs.json` file into a Spark DataFrame, I can use the following code:

```{python}
path = './../Data/logs.json'
logs = spark.read.json(path)
n_truncate = 50
logs.show(5, truncate = n_truncate)
```

By default, when we use the `show()` action to see the contents of our Spark DataFrame, Spark will always truncate (or cut) any value in the DataFrame that is more than 20 characters long. Since the logs messages in the `logs.json` file are usually much longer than 20 characters, I am using the `truncate` argument of `show()` in the example above, to avoid this behaviour.

By setting this argument to 50, I am asking Spark to truncate (or cut) values at the 50th character (instead of the 20th). By doing this, you (reader) can actually see a much more significant part of the logs messages in the result above.


## Changing the case of letters in a string

Probably the most basic string transformation that exists is to change the case of the letters (or characters) that compose the string. That is, to raise specific letters to upper-case, or reduce them to lower-case, and vice-versa.

As a first example, lets go back to the `logs` DataFrame, and try to change all messages in this DataFrame to lower case, upper case and title case, by using the `lower()`, `upper()`, and `initcap()` functions from the `pyspark.sql.functions` module.

```{python}
from pyspark.sql.functions import (
    lower,
    upper,
    initcap
)

m = logs.select('message')
# Change to lower case:
m.withColumn('message', lower('message'))\
    .show(5, truncate = n_truncate)
```

```{python}
# Change to upper case:
m.withColumn('message', upper('message'))\
    .show(5, truncate = n_truncate)
```

```{python}
# Change to title case
# (first letter of each word is upper case):
m.withColumn('message', initcap('message'))\
    .show(5, truncate = n_truncate)
```


## Calculating string length

In Spark, you can use the `length()` function to get the length (i.e. the number of characters) of a string. In the example below, we can see that the first log message is 74 characters long, while the second log message have 112 characters.

```{python}
from pyspark.sql.functions import length
logs\
    .withColumn('length', length('message'))\
    .show(5)
```


## Trimming or removing spaces from strings

The process of removing unnecessary spaces from strings is usually called "trimming". In Spark, we have three functions that do this process, which are:

- `trim()`: removes spaces from both sides of the string;
- `ltrim()`: removes spaces from the left side of the string;
- `rtrim()`: removes spaces from the right side of the string;


```{python}
from pyspark.sql.functions import (
    trim, rtrim, ltrim
)

logs\
    .select('ip')\
    .withColumn('ip_trim', trim('ip'))\
    .withColumn('ip_ltrim', ltrim('ip'))\
    .withColumn('ip_rtrim', rtrim('ip'))\
    .show(5)
```

For the most part, I tend to remove these unnecessary strings when I want to: 1) tidy the values; 2) avoid weird and confusing mistakes in filters on my DataFrame. The second case is worth describing in more details.

Let's suppose you wanted to filter all rows from the `logs` DataFrame where `ip` is equal to the `1.0.104.27` IP adress. However, you can see in the result above, that I get nothing. Not a single row of result.

```{python}
from pyspark.sql.functions import col
logs.filter(col('ip') == "1.0.104.27")\
    .show(5)
```

But if you see the result of the previous example (where we appliead the three versions of "trim functions"), you know that this IP adress `1.0.104.27` exists in the DataFrame. You know that the filter above should find values for this IP adress. So why it did not find any rows?

The answer is these annoying (and hidden) spaces on both sides of the values from the `ip` column. If we remove these unnecessary spaces from the values of the `ip` column, we suddenly find the rows that we were looking for.

```{python}
logs.filter(trim(col('ip')) == "1.0.104.27")\
    .show(5)
```


## Extracting substrings

There are five main functions that we can use in order to extract substrings of a string, which are:

- `substring()` and `substr()`: extract a single substring based on a start position and the length (number of characters) of the collected substring[^index-one];
- `substring_index()`: extract a single substring based on a single delimiter[^index-one];
- `split()`: extract one or multiple substrings based on commom delimiter;
- `regexp_extract()`: extract a single substring based on a regular expression;

[^index-one]: These functions uses a one based index, and not zero based index.

You can obviously extract a substring that matches a particular regex (regular expression) by using the `regexp_extract()` function. However, I will describe this function, and the regex functionality available in `pyspark` at @sec-regex, or, more specifically, at @sec-regexp-extract. For now, just understand that you can also use regex to extract substrings from your text data.


### A substring based on a start position and length

The `substring()` and `substr()` functions they both work the same way. However, they come from different places. The `substring()` function comes from the `spark.sql.functions` module, while the `substr()` function is actually a method from the `Column` class.

One interesting aspect of these functions, is that they both use a one-based index, instead of a zero-based index. This means that the first character in the full string is identified by the index 1, instead of the index 0.

In the example below, we are extracting the substring that starts at the second character (index 2) and ends at the sixth character (index 6) in the string.

```{python}
from pyspark.sql.functions import col, substring
# `df1` and `df2` are equal, because
# they both mean the same thing
df1 = (logs
    .withColumn('sub', col('message').substr(2, 5))
)

df2 = (logs
    .withColumn('sub', substring('message', 2, 5))
)

df2.show(5)
```


### A substring based on a delimiter

The `substring_index()` function works very differently. It collects the substring formed between the start of the string, and the nth occurrence of a particular character.

For example, if you ask `substring_index()` to search for the 3rd occurrence of the character `$` in your string, the function will return to you the substring formed by all characters that are between the start of the string until the 3rd occurrence of this character `$`. You can also ask `substring_index()` to read backwards. That is, to start the search on the end of the string, and move backwards in the string until it gets to the 3rd occurrence of this character `$`.

As an example, let's look at the 10th log message present in the `logs` DataFrame. I used the `collect()` DataFrame method to collect this message into a raw python string, so we can see the full message.

```{python}
from pyspark.sql.functions import monotonically_increasing_id

mes_10th = (
    logs
    .withColumn(
        'row_id',
        monotonically_increasing_id()
    )
    .where(col('row_id') == 9)
)

message = mes_10th.collect()[0]['message']
print(message)
```

We can see that this log message is listing a set of libraries that were installed somewhere. Suppose you want to collect the first and the last libraries in this list. How would you do it?

A good start to this objective, is to isolate the list of libraries from the rest of the message. In other words, there is a bunch of characters in the start of the log message, that we do not care about. So let's get rid of them.

If you look closely to the message, you can see that the character `:` appears twice whithin the message. One close to the start of the string, and another time right before the start of the list of the libraries. We can use this character as our first delimiter, to collect the third substring that it creates within the total string, which is the substring that contains the list of libraries.

This first stage is presented visually at @fig-substring-delimiter1.

![The first stage of subsetting](./../Figures/substring-delimiter1.png){#fig-substring-delimiter1}

Now that we identified the substrings produced by the "delimiter character", we just need to understand better which index we need to use in `substring_index()` to get this third substring that we want. The @fig-substring-delimiter2 presents in a visual manner how the count system of `substring_index()` works.

When you use a positive index, `substring_index()` will count the occurrences of the delimiter character from left to right. But, when you use a negative index, the opposite happens. That is, `substring_index()` counts the occurrences of the delimiter character from right to left.

![The count system of `substring_index()`](./../Figures/substring-delimiter2.png){#fig-substring-delimiter2}

The index 1 represents the first substring that is before the the 1st occurence of the delimiter (`[INFO]`). The index 2 represents everything that is before the 2nd occurence of the delimiter (`[INFO]: 2022-09-05 04:02.09.05 Libraries installed`). Etcetera.

In contrast, the index -1 represents everything that is after the 1st occurence of the delimiter, couting from right to left (`pandas, flask, numpy, spark_map, pyspark`). The index -2 represents everything that is after the 2nd occurence of the delimiter (`2022-09-05 04:02.09.05 Libraries installed: pandas, flask, numpy, spark_map, pyspark`). Again, couting from right to left .

Having all these informations in mind, we can conclude that the following code fit our first objective. Note that I applied the `trim()` function over the result of `substring_index()`, to ensure that the result substring does not contain any unnecessary spaces at both ends.

```{python}
from pyspark.sql.functions import substring_index
mes_10th = mes_10th\
    .withColumn(
        'list_of_libraries',
        trim(substring_index('message', ':', -1))
    )

mes_10th.select('list_of_libraries')\
    .show(truncate = n_truncate)
```


### Forming an array of substrings

Now is a good time to introduce the `split()` function, because we can use it to extract the first and the last library from the list libraries of stored at the `mes_10th` DataFrame. Basically, this function also uses a delimiter character to cut the total string into multiple pieces, and store these pieces in a array of substrings. With this strategy, we can now access each substring (or each piece of the total string) individually.

If we look again at the string that we stored at the `list_of_libraries` column, we have a list of libraries, separated by a comma. 

```{python}
mes_10th\
    .select('list_of_libraries')\
    .show(truncate = n_truncate)
```


The comma character (`,`) plays an important role in this string, by separating each value in the list. And we can use this comma character as the delimiter inside `split()`, to get an array of substrings. Each element of this array is one of the many libraries in the list. The @fig-string-split presents this process visually.

![Building an array of substrings with `split()`](./../Figures/string-split.png){#fig-string-split}


The code to make this process is very straightforward. In the example below, the column `array_of_libraries` becomes a column of data type `ArrayType(StringType)`, that is, an array of string values.

```{python}
from pyspark.sql.functions import split
mes_10th = mes_10th\
    .withColumn(
        'array_of_libraries',
        split('list_of_libraries', ',')
    )

mes_10th\
    .select('array_of_libraries')\
    .show(truncate = n_truncate)
```

By having this array of substring, we can very easily select a specific element in this array, by using the `getItem()` column method, or, by using the open brackets as you would normally use to select an element in a python list.

You just need to give the index of the element you want to select, like in the example below that we select the first and the fifth libraries in the array.

```{python}
mes_10th\
    .withColumn('lib_1', col('array_of_libraries')[0])\
    .withColumn('lib_5', col('array_of_libraries').getItem(4))\
    .select('lib_1', 'lib_5')\
    .show(truncate = n_truncate)
```



## Concatenating multiple strings together

Sometimes, we need to concatenate multiple strings together, to form a single and longer string. To do this process, Spark offers two main functions, which are: `concat()` and `concat_ws()`. Both of these functions receives a list of columns as input, and will perform the same task, which is to concatenate the values of each column in the list, sequentially.

However, the `concat_ws()` function have an extra argument called `sep`, where you can define a string to be used as the separator (or the "delimiter") between the values of each column in the list. In some way, this `sep` argument and the `concat_ws()` function works very similarly to the [`join()` string method of python](https://docs.python.org/3/library/stdtypes.html#str.join)[^string-join-py].

[^string-join-py]: <https://docs.python.org/3/library/stdtypes.html#str.join>




<!-- 

## Introducing regular expressions {#sec-regex}


### Extracting substrings with `regexp_extract()` {#sec-regexp-extract}

To match grouping expressions use `$1`, `$2`, `$3`, etc...
```python
regexp_extract(teste, '^([a-z]+)', '$1')
```

If you need to extract multiple groups together, be sure to add a whitespace between each one, like `"$1 $2 $3"`. Because if write like`"$1$2$3"` is not going to work.

 -->


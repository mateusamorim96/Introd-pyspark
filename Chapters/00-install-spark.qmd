---
title: How to install Spark and `pyspark`
---

In order to build and run any Spark application through `pyspark`, you have to install Apache Spark in your computer. Apache Spark is available to all three major operating systems in the market today (macOS, Windows and Linux). 

The process of installation is kind of similar in all three OS's. But some steps work differently in each OS. Currently, I (the author of this book) do not have access to a macOS machine, and because of that, I will not describe the installation process of Spark on this platform here. If you have access to a macOS machine, and are willing to describe this installation process, I would be happy to review a PR with such content.



## What are the steps?

In short, the next steps for installing Spark are:

1. Install Java;
2. Install Python;
3. Install `pyspark`;
2. Download and extract Apache Spark;
3. Set a few environment variables;

These steps are simple, but they work differently depending on what operating system you are currently in. As a result, the next sections describes these steps for each operating system. 


## On Windows


### Install Java

Apache Spark is written in Scala, which is a fairly modern programming language that have powerful interoperability with the Java programming language. Because of this characteristic, some of the functionalities of Spark require you to have Java installed in your machine. 

In other words, you must have Java installed and configured to use Spark. If you do not have Java, Spark will likely fail when you try to start it. However, since Java is a very popular tecnology across the world, is possible that you already have it installed in your machine. To check if Java is already installed, you can run the following command in your OS terminal:

```{terminal, eval = FALSE}
Terminal$ java -version
```

If the above command outputs something similar to the text exposed below, than, you already have Java installed in your machine, and you can proceed to the next step.

```
java version "1.8.0_331"
Java(TM) SE Runtime Environment (build 1.8.0_331-b09)
Java HotSpot(TM) Client VM (build 25.331-b09, mixed mode, sharing)
```

But, if something different comes up in your terminal, than, you do not have Java installed. To fix this, [download Java from the official website](https://www.java.com/download/)^[<https://www.java.com/download/>], and install it.


### Install Python

You can easily install Python on Windows, by downloading the installer available at the [official website of the language](https://www.python.org/downloads/)^[<https://www.python.org/downloads/>], and executing it.


### Install `pyspark`

Installing the `pyspark` python package is pretty straightforward. Just open a terminal (if you need help to open the terminal check @sec-open-terminal), and use the `pip` command to do it:

```{terminal, eval = FALSE}
Terminal$ pip install pyspark
```

If you try to run the above command (inside a terminal of any OS), and a message like `pip: command not found` appears, this means that you do not have the `pip` tool installed on your machine. Hence, if you face this kind of message, you need to install `pip` before you even install `pyspark`.

The `pip` tool is automatically installed with Python on Windows. So, if you face this message (`pip: command not found`), then, is very likely that you do not have Python correctly installed on your machine. Or maybe, Python is not installed at all in any shape or size in your system. So, you should comeback to previous section, and re install it.

In UNIX-like systems, installing `pip` is very easy, because you can use the built-in package manager to do this for you. In Debian like distros (e.g. Ubuntu), you use the `apt` tool, and, in Arch-Linux like distros (e.g. Arch Linux, Manjaro) you would use the `pacman` tool. Both possibilities are exposed below:

```{terminal, eval = FALSE}
# If you are in a Debian like distro of Linux
# and need to install `pip`, use this command:
Terminal$ sudo apt install python3-pip
# If you are in a Arch-Linux like distro of Linux
# and need to install `pip`, use this command:
Terminal$ pacman -S python-pip
```

But, if you are running in MacOs, is best to use the `get-pip.py` method to do install `pip`^[See the following discussion: <https://stackoverflow.com/questions/17271319/how-do-i-install-pip-on-macos-or-os-x>]. To use this method, try to run the command below on the terminal. After you installed `pip`, run the same `pip` command we showed earlier to install `pyspark`.

```{terminal, eval = FALSE}
# To install `pip` on a MacOs:
Terminal$ curl https://bootstrap.pypa.io/get-pip.py | python3
```





### Download and extract the files of Apache Spark

First, you need to download Apache Spark from the [official website for the project](https://spark.apache.org/downloads)^[<https://spark.apache.org/downloads>]. Currently, the Apache Spark does not offers installers or package programs to install the software for you. In other words,  is available only as a TAR file (`.tgz`).

We usually install external software on Windows by using installers (i.e. executable files - `.exe`) that perform all the necessary steps to install the software in your machine. However, currently, the Apache Spark project does not offers such installers. This means that you have to install it yourself.

When you download Apache Spark from the official website, you will get all files of the program inside a TAR file (`.tgz`).

After you downloaded Spark to your machine, you need to extract all files from the TAR file (`.tgz`) to a specific location of your computer. It can be anywhere, just choose a place. As an example, I will extract the files to a folder called `Spark` at my hard disk `C:/`. 

To extract these files, you can use very popular UI tools like [7zip](https://7-zip.org/)^[<https://7-zip.org/>] or [WinRAR](https://www.win-rar.com/)^[<https://www.win-rar.com/>]. However, if you use a modern version of Windows, there is a `tar` command line tool available at the terminal that you can use to do this process in a programatic fashion.

```{terminal, eval = FALSE}
Terminal$ tar -x -f spark-3.3.1-bin-hadoop3.tgz
Terminal$ mv spark-3.3.1-bin-hadoop3 C:/Spark/spark-3.3.1-bin-hadoop3
```


### Set environment variables

Apache Spark will always look for three different environment variables in your system: `SPARK_HOME`, `PYSPARK_HOME` and `JAVA_HOME`. This means that you must have these three environment variables defined and correctly configured in your machine.


You can use Python itself to find the path to the executable of the Python compiler. Just the run the following Python code:

```{python}
#| eval: false
import sys
sys.executable
```
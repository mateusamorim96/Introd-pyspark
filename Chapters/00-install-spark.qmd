---
title: How to install Spark and `pyspark`
---

In order to build and run any Spark application through `pyspark`, you have to install Apache Spark in your computer. Apache Spark is available to all three major operating systems in the market today (macOS, Windows and Linux). 

The process of installation is kind of similar in all three OS's. But some steps work differently in each OS. Currently, I (the author of this book) do not have access to a macOS machine, and because of that, I will not describe the installation process of Spark on this platform here. If you have access to a macOS machine, and are willing to describe this installation process, I would be happy to review a PR with such content.



## What are the steps?

In short, the next steps for installing Spark are:

1. Install Java;
2. Install Python;
3. Install `pyspark`;
2. Download and extract Apache Spark;
3. Set a few environment variables;

The next sections describes these steps for each operating system. 


## On Windows

### Install Java

Apache Spark is written in Scala, which is a fairly modern programming language that have powerful interoperability with the Java programming language. Because of this characteristic, some of the functionalities of Spark require you to have Java installed in your machine. 

In other words, you must have Java installed to use Spark. If you do not have Java, Spark will likely fail when you try to start it. However, since Java is a very popular tecnology across the world, is possible that you already have it installed in your machine. To check if Java is already installed, you can run the following command in your OS terminal:

```{terminal}
#| eval: false
Terminal$ java -version
```

If the above command outputs something similar to the text exposed below, than, you already have Java installed in your machine, and you can proceed to the next step.

```
java version "19.0.1" 2022-10-18
Java(TM) SE Runtime Environment (build 19.0.1+10-21)
Java HotSpot(TM) 64-Bit Server VM (build 19.0.1+10-21, mixed mode, sharing)
```

But, if something different comes up in your terminal, than, is likely that you do not have Java installed. To fix this, [download Java from the official website](https://www.java.com/download/)^[<https://www.java.com/download/>], and install it.


### Install Python

You can easily install Python on Windows, by downloading the installer available at the [official website of the language](https://www.python.org/downloads/)^[<https://www.python.org/downloads/>], and executing it.


### Install `pyspark`

Installing the `pyspark` python package is pretty straightforward. Just open a terminal (if you need help to open the terminal check @sec-open-terminal), and use the `pip` command to do it:

```{terminal}
#| eval: false
Terminal$ pip install pyspark
```

If you try to run the above command (inside a terminal of any OS), and a message like `pip: command not found` appears, this means that you do not have the `pip` tool installed on your machine. Hence, if you face this kind of message, you need to install `pip` before you even install `pyspark`.

The `pip` tool is automatically installed with Python on Windows. So, if you face this message (`pip: command not found`), then, is very likely that you do not have Python correctly installed on your machine. Or maybe, Python is not installed at all in any shape or size in your system. So, you should comeback to previous section, and re install it.



### Download and extract the files of Apache Spark

First, you need to download Apache Spark from the [official website for the project](https://spark.apache.org/downloads)^[<https://spark.apache.org/downloads>]. Currently, the Apache Spark does not offers installers or package programs to install the software for you. In other words,  is available only as a TAR file (`.tgz`).

We usually install external software on Windows by using installers (i.e. executable files - `.exe`) that perform all the necessary steps to install the software in your machine. However, currently, the Apache Spark project does not offers such installers. This means that you have to install it yourself.

When you download Apache Spark from the official website, you will get all files of the program inside a TAR file (`.tgz`).

After you downloaded Spark to your machine, you need to extract all files from the TAR file (`.tgz`) to a specific location of your computer. It can be anywhere, just choose a place. As an example, I will extract the files to a folder called `Spark` at my hard disk `C:/`. 

To extract these files, you can use very popular UI tools like [7zip](https://7-zip.org/)^[<https://7-zip.org/>] or [WinRAR](https://www.win-rar.com/)^[<https://www.win-rar.com/>]. However, if you use a modern version of Windows, there is a `tar` command line tool available at the terminal that you can use to do this process in a programatic fashion.

```{terminal, eval = FALSE}
Terminal$ tar -x -f spark-3.3.1-bin-hadoop3.tgz
Terminal$ mv spark-3.3.1-bin-hadoop3 C:/Spark/spark-3.3.1-bin-hadoop3
```


### Set environment variables

<!-- test if `HADOOP_HOME` variable and winutils.exe is really necessary -->
<!-- test if `PYSPARK_PYTHON` is really necessary -->

Apache Spark will always look for three different environment variables in your system: `SPARK_HOME` and `JAVA_HOME`. This means that you must have these two environment variables defined and correctly configured in your machine. To configure an environment variable on Windows, I recommend you to use the menu that you can find by searching for "environment variables" in the Windows search box. Figures @fig-windows-env1, @fig-windows-env2 and @fig-windows-env3 show the path to this menu on Windows:

![Path to Windows Environment Variables Menu - Part 1](../Figures/windows-environment-variables1.png){#fig-windows-env1}
![Path to Windows Environment Variables Menu - Part 2](../Figures/windows-environment-variables2.png){#fig-windows-env2}
![Path to Windows Environment Variables Menu - Part 3](../Figures/windows-environment-variables3.png){#fig-windows-env3}

During the process of scheduling and running your Spark application, Spark will execute a set of scripts located at the home directory of Spark itself. Spark does this by looking for a environment variable called `SPARK_HOME` in your system. If you do not have this variable configured, Spark will not be able to locate its home directory, and as a consequence, it throws an runtime error.

In other words, you need to set a variable with name `SPARK_HOME` in your system. Its value should be the path to the home (or "root") directory where you installed (or unzipped) the Spark files. In my case I unzipped the files into a folder called `C:/Spark/spark-3.3.1-bin-hadoop3`, and that is the folder that I going to use in `SPARK_HOME`:

![Setting the `SPARK_HOME` environment variable](../Figures/windows-environment-variables4.png)


<!-- We need Java SDK not Java Runtime -->
<!-- Download at https://www.oracle.com/java/technologies/downloads/ -->
Now, a lot Spark functionality is closely related to Java SDK, and to find it, Spark will look for a environment variable called `JAVA_HOME`. On my machine, I have Java SDK installed at the folder `C:\Program Files\Java\jdk-19`. This might be not your case, and you may find the "jdk" folder for Java at a different location of your computer. Just find where it is, and use this path while setting this `JAVA_HOME` variable, like I did:

![Setting the `JAVA_HOME` environment variable](../Figures/windows-environment-variables5.png)

With these two environment variables setted, you should be able to start and use Spark already. Just open a brand new terminal, and type the `spark` command. An interactive Spark Session will initiate after this command, and give you a command prompt where you can start coding the Spark application you want to execute.



Seconde, lets set the variable `PYSPARK_PYTHON`. This variable must contain the path to the home of the python binary that you want to use while launching `pyspark`. ou can use Python itself to find the path to the executable of the Python compiler. Just the run the following Python code:

```{python}
#| eval: false
import sys
sys.executable
```



## On Linux

### Install `pyspark`

In UNIX-like systems, installing `pip` is very easy, because you can use the built-in package manager to do this for you. In Debian like distros (e.g. Ubuntu), you use the `apt` tool, and, in Arch-Linux like distros (e.g. Arch Linux, Manjaro) you would use the `pacman` tool. Both possibilities are exposed below:

```{terminal, eval = FALSE}
# If you are in a Debian like distro of Linux
# and need to install `pip`, use this command:
Terminal$ sudo apt install python3-pip
# If you are in a Arch-Linux like distro of Linux
# and need to install `pip`, use this command:
Terminal$ pacman -S python-pip
```
# Working with SQL in `pyspark`

## Introduction

As we discussed in @sec-introd-spark, Spark is a **multi-language** engine for large-scale data processing. This means that we can build our Spark application using many different languages (like Java, Scala, Python and R). Furthermore, you can also use the Spark SQL module of Spark to translate all of your transformations into pure SQL queries.

In more details, Spark SQL is a Spark module for structured data processing [@sparkdoc]. As a result, you can use it to translate all transformations that you build with the DataFrame API, which is the main structured API of Spark. This means that virtually all transformations exposed throughout this book, can be translated into a SQL query in Spark.

However, this also means that the Spark SQL module does not handle the transformations produced by the unstructured APIs of Spark, i.e. the Dataset API. Since the Dataset API is not available in `pyspark`, it is not covered in this book.

Due to the multi-language nature of Spark, you can mix python code that uses the DataFrame API with pure SQL queries to build your transformations. We will focus on this exchange between Python and SQL in this chapter.

## Creating SQL Tables in Spark

In real life jobs at the industry, is very likely that your data will be allocated inside a SQL-like database. Spark can connect to a external SQL database through JDBC/ODBC connections, or, read tables from Apache Hive. This way, you can sent your SQL queries to this external database.

However, to expose more simplified examples throughout this chapter, we will use `pyspark` to create a simple temporary SQL table in our Spark context, and use this temporary SQL table in our examples of SQL queries. This way, we avoid the work to connect to some existing SQL database, and, still get to learn how to use SQL queries in `pyspark`.

First, lets create our Spark Session. You can see below that I used the `config()` method to set a specific option of the session called `spark.sql.catalogImplementation`, to the value `"hive"`. This option controls the implementation of the Spark SQL Catalog, which is a core part of the SQL functionality of Spark [^06-dataframes-sql-catalog].

[^06-dataframes-sql-catalog]: There are some very good materials explaining what is the Spark SQL Catalog, and which is the purpose of it. For a soft introduction, I recommend Sarfaraz Hussain post: <https://medium.com/@sarfarazhussain211/metastore-in-apache-spark-9286097180a4>. For a more technical introduction, see <https://jaceklaskowski.gitbooks.io/mastering-spark-sql/content/spark-sql-Catalog.html>.

Spark usually complain with a `AnalysisException` error when you try to create SQL tables with this option undefined (or not configured). Since we will be creating SQL tables during the examples of this chpater, is nice to set this option right at the start of your script if you decide to follow the examples[^06-dataframes-sql-1].

[^06-dataframes-sql-1]: You can learn more about this specific issue by looking at this StackOverflow post: <https://stackoverflow.com/questions/50914102/why-do-i-get-a-hive-support-is-required-to-create-hive-table-as-select-error>.

```{python}
#| eval: false
from pyspark.sql import SparkSession
spark = SparkSession\
  .builder\
  .config("spark.sql.catalogImplementation","hive")\
  .getOrCreate()
```

```{python}
#| include: false
from pyspark.sql import SparkSession
spark = SparkSession\
  .builder\
  .config("spark.sql.catalogImplementation","hive")\
  .getOrCreate()

sc = spark.sparkContext
sc.setLogLevel("OFF")
```

## `TABLEs` versus `VIEWs`

To run a complete SQL query over any Spark DataFrame, you must register this DataFrame in the Spark SQL Catalog of your Spark Session. You can register a Spark DataFrame into this catalog as a physical SQL `TABLE`, or, as a SQL `VIEW`. 

If you are familiar with SQL in other platforms, you probably already heard of these two types (`TABLE` or `VIEW`) of tables. But if not, we will explain each one in this section. Is worth pointing out that choosing between these two types **does not affect** your code, or your transformations in any way. It just affect the way that Spark SQL stores the table/DataFrame itself.

### `VIEWs` are stored as queries

When you register a DataFrame as a SQL `VIEW`, the query to produce this DataFrame is stored, not the DataFrame itself. As a consequence, when you call (or access) this SQL `VIEW` inside your SQL queries (for example, with a `SELECT * FROM` statement), Spark SQL will automatically build this SQL `VIEW` "on the fly" (or "on runtime"), by running the query necessary to build the initial DataFrame that you stored inside this `VIEW`.

In other words, when you create a SQL `VIEW`, Spark SQL do not store any physical data or rows of the table/DataFrame. It just stores the SQL query necessary to build your table/DataFrame, or, for temporary SQL `VIEWs`, a memory pointer to the Spark DataFrame that you stored in this `VIEW`. In some sense, you can interpret any SQL `VIEWs` as an abbreviation to a previously stored SQL query.

Bearing these characteristics in mind, for most "use case scenarios", SQL `VIEWs` are easier to manage inside your data pipelines. Because you do not have update them. Since they are calculated "on the fly", a SQL `VIEW` will always translate the most recent version of the data.

### TABLEs are stored as actual tables/DataFrames

In the other hand, SQL `TABLEs` are the "opposite" of SQL `VIEWs`. That is, SQL `TABLEs` are stored as physical tables inside the SQL database. In this way, all rows of your table are stored inside the data warehouse of Spark SQL. 

Because of these characteristics, SQL `TABLEs` are usually faster to read, load and transform. But, as a collateral effect, you have to physically update the data inside this `TABLE`.

## Temporary versus Persistent sources

Spark SQL have two ways notions of storing a SQL `TABLE`  as we usually know in SQL databases. You can convert any Spark DataFrame into a 


In the example below, I am reading a CSV file from my computer called `penguins.csv` (remember that this CSV can be downloaded from the book repository[^06-dataframes-sql-2]), then, I create a SQL temporary view (called `penguins_view`) from this `penguins` DataFrame with the `createOrReplaceTempView()` method.


[^06-dataframes-sql-2]: <https://github.com/pedropark99/Introd-pyspark/tree/main/Data>

```{python}
path = "../Data/penguins.csv"
penguins = spark.read\
  .csv(path, header = True)
  
penguins.createOrReplaceTempView('penguins_view')
```

After these commands, I have now a SQL view called `penguins_view` registered in my the Spark SQL context, which I can query it, using pure SQL. To execute a SQL query, you can use the `sql()` method from your Spark Session, like in the example below:

```{python}
spark.sql('select * from penguins_view').show(5)
```

This `sql()` method receives a SQL query inside a string, and always outputs a new Spark DataFrame as a result. That is why I used the `show()` method right after `sql()`, to see what this new Spark DataFrame looked like.

Therefore, in the above example, we wrote a simple and pure SQL query, and, executed it through `pyspark`, by using this `sql()` method. This means that, this `sql()` method is the bridge between `pyspark` and your SQL database. You give a pure SQL query (inside a string) to this `sql()` method, and, Spark will execute it, considering the SQL databases that you are connected with.

Having this in mind, we could use this `sql()` method to create a physical SQL table (instead of a temporary view). We just need to pass the necessary SQL commands that will create this table to be executed by this method. In the example below, we create a new database called `examples`, and, a table called `code_brazil_states`.

```{python}
#| include: false
spark.sql('drop database if exists `examples` cascade')
```


```{python}
spark.sql('create database `examples`')
spark.sql('use `examples`')
spark.sql('''
  create table `code_brazil_states` (
    `code` int,
    `state_name` string
  )
''')
spark.sql('insert into `code_brazil_states` values (31, "Minas Gerais")')
spark.sql('insert into `code_brazil_states` values (15, "Pará")')
spark.sql('insert into `code_brazil_states` values (41, "Paraná")')
spark.sql('insert into `code_brazil_states` values (25, "Paraíba")')
```

We can see now this new physical SQL table using a simple query like this:

```{python}
spark\
  .sql('select * from examples.code_brazil_states')\
  .show()
```


## Selecting Spark SQL tables

In Spark, tables registered in our SQL Spark Context are called Spark SQL tables. To access any of these tables, you have to query it through a SQL query, as you would normally do in any SQL database. However, it can be quite annoying to type a "select * from" kind of query every time you want to use a Spark SQL table.

That is why Spark offers a shortcut to us, which is the `table()` method of your Spark session. In other words, the code `spark.table("table_name")` is a shortcut to `spark.sql('select * from table_name')`. For example, we could rewrite the previous query as:

```{python}
spark\
  .table('examples.code_brazil_states')\
  .show()
```



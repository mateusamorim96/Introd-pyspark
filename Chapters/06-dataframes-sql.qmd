# Working with SQL in `pyspark`

## Introduction

As we discussed in @sec-introd-spark, Spark is a **multi-language** engine for large-scale data processing. This means that we can build our Spark application using many different languages (like Java, Scala, Python and R). Furthermore, you can also use the Spark SQL module of Spark to translate all of your transformations into pure SQL queries.

Spark SQL is a Spark module for structured data processing [@sparkdoc]. Because Spark SQL is a structured module of Spark, you can use it to translate all transformations that you build with the DataFrame API, which is the main structured API of Spark.

This means that all transformations exposed throughout this book, can be translated into a SQL query in Spark. However, this also means that the Spark SQL module does not handle the transformations produced by the unstructured APIs of Spark, i.e. the Dataset API. Since the Dataset API is not available in `pyspark`, it is not covered in this book.

Due to the multi-language nature of Spark, you can mix python code that uses the DataFrame API with pure SQL queries to build your transformations. We will focus on this exchange between Python and SQL in this chapter.

## Creating SQL Tables in Spark

In real life jobs at the industry, is very likely that your data will be allocated inside a SQL database. Spark can connect to a external SQL database through JDBC/ODBC connections, or, read tables from Apache Hive. This way, you sent you SQL queries to this external database.

However, to expose a more simplified example, we will use `pyspark` to create a simple temporary SQL table in our Spark context, and use this temporary SQL table in our examples of SQL queries. This way, we avoid the work to connect to some existing SQL database, and, still get to learn how to use SQL queries in `pyspark`.

In the example below, I am reading a CSV file from my computer called `penguins.csv` (remember that this CSV can be downloaded from the book repository[^06-dataframes-sql-1]), then, I create a SQL temporary view (called `penguins_view`) from this `penguins` DataFrame with the `createOrReplaceTempView()` method.

[^06-dataframes-sql-1]: https://github.com/pedropark99/Introd-pyspark/tree/main/Data

```{python}
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()

path = "../Data/penguins.csv"
penguins = spark.read\
  .csv(path, header = True)
  
penguins.createOrReplaceTempView('penguins_view')
```

After these commands, I have now a view called `penguins_view` registered in the Spark SQL context, which I can query it, using pure SQL. We can execute a SQL query by using the `sql()` method from our Spark Session, like in the example below:

```{python}
spark.sql('select * from penguins_view').show(5)
```

This `sql()` method always result in a new Spark DataFrame. That is why I used the `show()` method right after `sql()`, to see what this new Spark DataFrame looked like.

Therefore, in the above example, we wrote a simple and pure SQL query, and, executed it through `pyspark`, by using this `sql()` method. This means that, this `sql()` method is the bridge between `pyspark` and your SQL database. You give a pure SQL query (inside a string) to this `sql()` method, and, Spark will execute it, considering the SQL databases that you are connected with.

Having this in mind, we could use this `sql()` method to create a physical SQL table (instead of a temporary view). We just need to pass the necessary SQL commands that will create this table to be executed by this method. In the example below, we create a new database called `examples`, and, a table called `code_brazil_states`.

```{python}
#| eval: false
spark.sql('create database `examples`')
spark.sql('use `examples`'
spark.sql('''
  create table `code_brazil_states` (
    `code` int,
    `state_name` string
  )
''')
spark.sql('insert into `code_brazil_states` values (31, "Minas Gerais")')
spark.sql('insert into `code_brazil_states` values (15, "Pará")')
spark.sql('insert into `code_brazil_states` values (41, "Paraná")')
spark.sql('insert into `code_brazil_states` values (25, "Paraíba")')
```

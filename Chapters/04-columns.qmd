

## Introducing the `Column` class

```{python}
#| include: false
# To execute the next chunks with success, we need to start a Spark Session
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()
sc = spark.sparkContext
sc.setLogLevel("OFF")
from datetime import date
from pyspark.sql import Row

data = [
  Row(id = 1, value = 28.3, date = date(2021,1,1)),
  Row(id = 2, value = 15.8, date = date(2021,1,1)),
  Row(id = 3, value = 20.1, date = date(2021,1,2)),
  Row(id = 4, value = 12.6, date = date(2021,1,3))
]

df = spark.createDataFrame(data)
```

As we described at the introduction of @sec-dataframes-chapter, you will massively use the methods from the `DataFrame` class in your Spark applications to manage, modify and calculate your Spark DataFrames.

However, there is one more python class that provides some very useful methods that you will regularly use, which is the `Column` class, or more specifically, the `pyspark.sql.column.Column` class.

The `Column` class is used to represent a column in a Spark DataFrame. This means that, each column of your Spark DataFrame is a object of class `Column`. We can confirm this statement, by taking the `df` DataFrame that we showed at @sec-building-a-dataframe, and look at the class of any column of it. Like the `id` column:

```{python}
type(df.id)
```

## Building a column object

You can refer to or create a column, by using the `col()` and `column()` functions from `pyspark.sql.functions` module. These functions receive a string input with the name of the column you want to create/refer to.

Their result are always a object of class `Column`. For example, the code below creates a column called `ID`:

```{python}
from pyspark.sql.functions import col
id_column = col('ID')
print(id_column)
```


## Columns are strongly related to expressions {#sec-columns-related-expressions}

Many kinds of transformations that we want to apply over a Spark DataFrame, are usually described through expressions, and, these expressions in Spark are mainly composed by **column transformations**. That is why the `Column` class, and its methods, are so important in Spark.

Columns in Spark are so strongly related to expressions that the columns themselves are initially interpreted as expressions. If we look again at the column `id` from `df` DataFrame, Spark will bring a expression as a result, and not the values hold by this column.

```{python}
df.id
```

Having these ideas in mind, when I created the column `ID` on the previous section, I created a "column expression". This means that `col("ID")` is just an expression, and as consequence, Spark does not know which are the values of column `ID`, or, where it lives (which is the DataFrame that this column belongs?). For now, Spark is not interested in this information, it just knows that we have an expression referring to a column called `ID`.

These ideas relates a lot to the **lazy aspect** of Spark that we talked about in @sec-viewing-a-dataframe. Spark will not perform any heavy calculation, or show you the actual results/values from you code, until you trigger the calculations with an action (we will talk more about these "actions" on @sec-dataframe-actions). As a result, when you access a column, Spark will only deliver a expression that represents that column, and not the actual values of that column.

This is handy, because we can store our expressions in variables, and, use it latter, in multiple parts of our code. For example, I can keep building and merging a column with different kinds of operators, to build a more complex expression. In the example below, I create a expression that doubles the values of `ID` column:

```{python}
expr1 = id_column * 2
print(expr1)
```

Remember, with this expression, Spark knows that we want to get a column called `ID` somewhere, and double its values. But Spark will not perform that action right now.

Logical expressions follow the same logic. In the example below, I am looking for rows where the value in column `Name` is equal to `'Anne'`, and, the value in column `Grade` is above 6. Again, Spark just checks if this is a valid logical expression, he does not evaluate it until we ask for it with an action: 

```{python}
expr2 = (col('Name') == 'Anne') & (col('Grade') > 6)
print(expr2)
```

## Some key column methods 

Because many transformations that we want to apply over our DataFrames are expressed as column transformations, the methods from the `Column` class will be quite useful on many different contexts. You will see many of these methods across the next chapters, like `desc()`, `alias()` and `cast()`.

Remember, you can always use the `dir()` function to see the complete list of methods available in any python class. Is always useful to check the official documentation too^[<https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.Column.html>]. There you will have a more complete description of each method.

But since they are so important in Spark, lets just give you a brief overview of some of the most popular methods from the `Column` class (these methods will be described in more detail in later chapters):

- `desc()` and `asc()`: methods to order the values of the column in a descending or ascending order (respectively);
- `cast()` and `astype()`: methods to cast (or convert) the values of the column to a specific data type;
- `alias()`: method to rename a column;
- `substr()`: method that returns a new column with the sub string of each value;
- `isNull()` and `isNotNull()`: logical methods to test if each value in the column is a null value or not;
- `startswith()` and `endswith()`: logical methods to search for values that starts with or ends with a specific pattern;
- `like()` and `rlike()`: logical methods to search for a specific pattern or regular expression in the values of the column;
- `isin()`: logical method to test if each value in the column is some of the listed values;






## The `Column` class in `pyspark`

```{python}
#| include: false
# To execute the next chunks with success, we need to start a Spark Session
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()
from datetime import date
from pyspark.sql import Row

data = [
  Row(id = 1, value = 28.3, date = date(2021,1,1)),
  Row(id = 2, value = 15.8, date = date(2021,1,1)),
  Row(id = 3, value = 20.1, date = date(2021,1,2)),
  Row(id = 4, value = 12.6, date = date(2021,1,3))
]

df = spark.createDataFrame(data)
```

As we described at the introduction of @sec-dataframes-chapter, you will massively use the methods from the `DataFrame` class in your Spark applications to manage, modify and calculate your Spark DataFrames.

However, there is one more python class that provides some very useful methods that you will regularly use, which is the `Column` class, or more specifically, the `pyspark.sql.column.Column` class.

The `Column` class is used to represent a column in a Spark DataFrame. This means that, each column of your Spark DataFrame is a object of class `Column`. We can confirm this statement, by taking the `df` DataFrame that we showed at @sec-building-a-dataframe, and look at the class of any column of it:

```{python}
type(df.id)
```

## Building a column

You can refer to or create a column, by using the `col()` and `column()` functions from `pyspark.sql.functions` module. In other words, the result of `col()` or `column()` functions is always a object of class `Column`. The code below creates a column called `ID`.

```{python}
from pyspark.sql.functions import col
a_column = col('ID')
print(a_column)
```


## Columns are strongly related to expressions

Many kinds of transformations that we want to apply over a Spark DataFrame, are usually expressed through expressions, and, expressions in Spark are usually composed by **column transformations**. That is why the `Column` class, and its methods, are so important in Spark.

Columns are so strongly related to expressions that the columns themselves are initially interpreted as expressions. In other words, when I created the column `ID` on the previous section, I created a "column expression". This means that `col("ID")` is just an expression, and as consequence, Spark does not know which are the values of column `ID`, or, where it lives (which is the DataFrame that this column belongs?). For now, Spark just know that we have an `ID` column somewhere, and nothing more.

Having this concept in mind, I can build different kinds of expressions with this "column expression". For example, I can create a expression that doubles the values of `ID` column:

```{python}
expr1 = col('ID') * 2
print(expr1)
```

Remember, with this expression Spark knows that we want to get a column called `ID` and double its values. But Spark will not perform that action right now. It will only look for this `ID` column, and, perform this mathematical operation when we trigger the evaluatation of this expression through an action (we will talk more about these actions on @sec-dataframe-actions).

Logical expressions follow the same logic. In the example below, I am looking for rows where the value in column `Name` is equal to `'Anne'`, and, the value in column `Grade` is above 6. Again, Spark just checks if this is a valid logical expression, he does not evaluate it until we ask for it with an action: 

```{python}
expr2 = (col('Name') == 'Anne') & (col('Grade') > 6)
print(expr2)
```

You will see some of the methods from this `Column` class across the next chapters, like `desc()`, `alias()` and `cast()`. For now, just understand that this class of objects exists, and that they will be quite useful on many contexts.

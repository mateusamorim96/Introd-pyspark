# Introducing window functions {#sec-window-functions}

Spark offers a set of tools known as *window functions*. These tools are essential for an extensive range of tasks, and you should know them. But what are them?

```{python}
#| include: false
# To execute the next chunks with success, we need to start a Spark Session
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()
sc = spark.sparkContext
sc.setLogLevel("OFF")
```

Window functions in Spark are a set functions that performs calculations over windows of rows from your DataFrame. This is not a concept exclusive to Spark. In fact, window functions in Spark are essentially the same [as window functions in MySQL](https://dev.mysql.com/doc/refman/8.0/en/window-functions-usage.html)[^mysql-win-function].

[^mysql-win-function]: <https://dev.mysql.com/doc/refman/8.0/en/window-functions-usage.html>

When you use a window function, the rows of your DataFrame are divided into multiple windows. Each window contains a specific range of rows from the DataFrame. In this context, a window function is a function that receives a window (or a range of rows) as input, and calculates an aggregate or a specific index based on the set of rows that is contained in this input window.

You might find this description very similar to what `groupby()` and `agg()` methods do when combined together. And yes... To some extent, the idea of windows in a DataFrame is similar (but not identical) to the idea of "groups" created by *group by* functions, such as the `DataFrame.groupby()` method from `pyspark` (that we presented at @sec-group-by), or the [`DataFrame.groupby()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html)[^pandas-group-by] method from `pandas`, and also, to [`dplyr::group_by()`](https://dplyr.tidyverse.org/reference/group_by.html)[^dplyr-group-by] from the `tidyverse` framework. You will see further in this chapter how window functions differ from these operations.

[^pandas-group-by]: <https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html>
[^dplyr-group-by]: <https://dplyr.tidyverse.org/reference/group_by.html>


## How to define windows {#sec-window-def}

In order to use a window function you need to define the windows of your DataFrame first. You do this by creating a `Window` object in your session.

Every window object have two components, which are partitioning and ordering, and you specify each of these components by using the `partitionBy()` and `orderBy()` methods from the `Window` class. In order to create a `Window` object, you need to import the `Window` class from the `pyspark.sql.window` module:

```{python}
from pyspark.sql.window import Window
```

Over the next examples, I will be using the `transf` DataFrame that we presented at @sec-transforming-dataframes-part1. If you don't remember how to import/get this DataFrame into your session, come back to @sec-transf-dataframe.

```{python}
#| cache: true
#| include: false
from pyspark.sql.types import StructType, StructField
from pyspark.sql.types import DoubleType, StringType
from pyspark.sql.types import LongType, TimestampType, DateType
path = "../Data/transf.csv"
schema = StructType([
  StructField('dateTransfer', DateType(), False),
  StructField('datetimeTransfer', TimestampType(), False),
  StructField('clientNumber', LongType(), False),
  StructField('transferValue', DoubleType(), False),
  StructField('transferCurrency', StringType(), False),
  StructField('transferID', LongType(), False),
  StructField('transferLog', StringType(), False),
  StructField('destinationBankNumber', LongType(), False),
  StructField('destinationBankBranch', LongType(), False),
  StructField('destinationBankAccount', StringType(), False)
])

transf = spark.read\
  .csv(path, schema = schema, sep = ";", header = True)
```

```{python}
transf.show(5)
```

Now, lets create a window object using the `transf` DataFrame as our target. This DataFrame describes a set of transfers made in a fictitious bank. So a reasonable way of splitting this DataFrame is by day. That means that we can split this DataFrame into groups (or ranges) of rows by using the `dateTransfer` column. As a result, each partition in the `dateTransfer` column will create/identify a different window in this DataFrame.

```{python}
window_spec = Window.partitionBy('dateTransfer')
```

The above window object specifies that each unique value present in the `dateTransfer` column identifies a different window frame in the `transf` DataFrame. @fig-window-spec1 presents this idea visually. So each partition in the `dateTransfer` column creates a different window frame. And each window frame will become an input to a window function (when we use one).

![Visualizing the window frames - Part 1](./../Figures/window-spec1.png){#fig-window-spec1}

Until this point, defining windows are very much like defining groups in your DataFrame with *group by* functions (i.e. windows are very similar to groups). In the above example, we specified only the partition component of the windows. This means that **the ordering component is not mandatory** for using window functions. But lets define the ordering component anyway.

The partitioning component of the window object specifies which partitions of the DataFrame are translated into windows. In the other hand, the ordering component of the window object specifies how the rows within the window are ordered. Defining this component becomes very important when we are working with window functions that outputs (or that uses) indexes.

As an example, you might want to use in your calculations the first (or the *nth*) row in each window. In a situation like this, the order in which these rows are founded inside the window affects directly the output of your window function. That is why the ordering component matters.

For example, we can say that the rows within each window should be in descending order according to the `datetimeTransfer` column:

```{python}
from pyspark.sql.functions import col
window_spec = Window\
    .partitionBy('dateTransfer')\
    .orderBy(col('datetimeTransfer').desc())
```

With the above snippet, we are not only specifying how the window frames in the DataFrame are created (with the `partitionBy()`), but we are also specifying how the rows within the window are sorted (with the `orderBy()`). If we update our representation with the above window specification, we get something similar to @fig-window-spec2:

![Visualizing the window frames - Part 2](./../Figures/window-spec2.png){#fig-window-spec2}

Is worth mentioning that, both `partitionBy()` and `orderBy()` methods accepts multiple columns as input. In other words, you can use a combination of columns both to define how the windows in your DataFrame will be created, and how the rows within these windows will be sorted.

As an example, the window specification below is saying: 1) that a window frame is created for each unique combination of `dateTransfer` and `clientNumber`; 2) that the rows within each window are ordered accordingly to `transferCurrency` (ascending order) and `datetimeTransfer` (descending order).

```{python}
window_spec = Window\
    .partitionBy('dateTransfer', 'clientNumber')\
    .orderBy(
        col('transferCurrency').asc(),
        col('datetimeTransfer').desc()
    )
```


## Introducing the `over()` clause

In order to use a window function you **need to combine a over clause with a window object**. If you pair these two components together, then, the function you are using becomes into a window function.

Since we know now how to define window objects for our DataFrame, we can actually create and use this object to access window functionality, by pairing this window object with an `over()` clause.

In `pyspark` this `over()` clause is actually a method from the `Column` class. Since all aggregating functions available from the `pyspark.sql.functions` module outputs a new `Column` object, we tend to use the `over()` method right after the function call.

For example, if we wanted to calculate the mean of column `x` with the `mean()` function, and we had a window object called `window_spec`, we could use the `mean()` as a window function by writing:

```python
mean(col('x')).over(window_spec)
```

Having this in mind, if you see the `over()` method after a call of an aggregating function (such as `sum()`, `mean()`, etc.), then, you know that the aggregating function being called is used as a window function.

In Spark SQL there is an `OVER` clause that you can use to use a window specification.




## Window functions vs *group by* functions

Despite their similarities, window functions and *group by* functions are used for different purposes. One big difference between them, is that when you use `groupby()` + `agg()` you get one output row per each input group of rows, but in contrast, a window function outputs one row per input row. In other words, for a window of $n$ input rows a window function outputs $n$ rows that contains the same result (or the same aggregate result).

For example, lets suppose you want to calculate the total value transfered within each day. If you use a `groupby()` + `agg()` strategy, you get as result a new DataFrame containing one row for each unique date present in the `dateTransfer` column:

```{python}
from pyspark.sql.functions import sum
transf\
    .orderBy('dateTransfer')\
    .groupBy('dateTransfer')\
    .agg(sum(col('transferValue')).alias('dayTotalTransferValue'))\
    .show(5)
```

On the other site, if you use `sum()` as a window function instead, you get as result one row for each transfer. That is, you get one row of output for each input row in the `transf` DataFrame. The value that is present in the new column created (`dayTotalTransferValue`) is the total value transfered for the window (or the range of rows) that corresponds to the date in the `dateTransfer` column.

In other words, the value `39630.7` below corresponds to the sum of the `transferValue` column when `dateTransfer == "2022-01-01"`:

```{python}
window_spec = Window.partitionBy('dateTransfer')
transf\
    .select('dateTransfer', 'transferID', 'transferValue')\
    .withColumn(
        'dayTotalTransferValue',
        sum(col('transferValue')).over(window_spec)
    )\
    .show(5)
```

You probably already seen this pattern in other data frameworks. As a quick comparison, if you were using the `tidyverse` framework, you could calculate the exact same result above with the following snippet of R code:

```r
transf |>
    group_by(dateTransfer) |>
    mutate(
        dayTotalTransferValue = sum(transferValue)
    )
```

In contrast, you would need the following snippet of Python code to get the same result in the `pandas` framework:

```python
transf['dayTotalTransferValue'] = transf['transferValue']\
    .groupby(transf['dateTransfer'])\
    .transform('sum')
```


## Ranking window functions

The functions `row_number()`, `rank()` and `dense_rank()` from the `pyspark.sql.functions` module are ranking functions, in the sense that they seek to rank each row in the input window according to a ranking system.

The function `row_number()` returns a unique and sequential number to each row in a window, starting from 1. This function is very similar to [`ROW_NUMBER()` from MySQL](https://dev.mysql.com/doc/refman/8.0/en/window-function-descriptions.html#function_row-number)[^mysql-row-num].

[^mysql-row-num]: <https://dev.mysql.com/doc/refman/8.0/en/window-function-descriptions.html#function_row-number>




## Position-based window functions

### Getting the next and previous row with `lead()` and `lag()`



## Agreggating window functions

In essence, a window function is every function that is compatible with window frames, or, in other words, is a function that can receive window frames as input. Virtually every aggregating function that you can find inside the `pyspark.sql.functions` module (like `sum()`, `mean()`, `count()`, `max()` and `min()`) is a window function.



<!--
dense_rank
row_number
rank
percent_rank

nth
lead
lag
first
last

sum
max
min
mean
stddev

cume_dist
-->
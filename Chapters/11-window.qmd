# Introducing window functions {#sec-window-functions}

Spark offers a set of tools known as *window functions*. These tools are essential for an extensive range of tasks, and you should know them. But what are them?

```{python}
#| include: false
# To execute the next chunks with success, we need to start a Spark Session
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()
sc = spark.sparkContext
sc.setLogLevel("OFF")
```

Window functions in Spark are a set functions that performs calculations over windows of rows from your DataFrame. In other words, when you use these functions, the rows of your DataFrame are divided into multiple windows. Each window contains a specific range of rows from the DataFrame. In this context, a window function is a function that receives a window (or a range of rows) as input, and calculate an aggregate or a specific index based on the set of rows that is contained in this input window.

So instead of receiving all the rows of your DataFrame at once as input, a window function receives a range of rows as input and outputs an aggregate (or index) value for this specific range of rows.

To some extent, the idea of windows in a DataFrame is similar (but not identical) to the idea of "groups" created by *group by* functions, such as the `DataFrame.groupby()` method from `pyspark` (that we presented at @sec-group-by), or the [`DataFrame.groupby()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html)[^pandas-group-by] method from `pandas`, and also, to [`dplyr::group_by()`](https://dplyr.tidyverse.org/reference/group_by.html)[^dplyr-group-by] from the `tidyverse` framework.

[^pandas-group-by]: <https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html>
[^dplyr-group-by]: <https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html>


## Defining a window {#sec-window-def}

In order to use a window function you need to define the window frames of your DataFrame. You do this by creating a `Window` object in your session. Every window object have two components, which are partitioning and ordering, and you specify each of these components by using the `partitionBy()` and `orderBy()` methods from the `Window` class.

Before I give an example of a window object, we need an example of DataFrame to begin with. Let's use the `transf` DataFrame that we presented at @sec-transforming-dataframes-part1. If you don't remember how to import/get this DataFrame into your session, come back to @sec-transf-dataframe.

```{python}
#| cache: true
#| include: false
from pyspark.sql.types import StructType, StructField
from pyspark.sql.types import DoubleType, StringType
from pyspark.sql.types import LongType, TimestampType, DateType
path = "../Data/transf.csv"
schema = StructType([
  StructField('dateTransfer', DateType(), False),
  StructField('datetimeTransfer', TimestampType(), False),
  StructField('clientNumber', LongType(), False),
  StructField('transferValue', DoubleType(), False),
  StructField('transferCurrency', StringType(), False),
  StructField('transferID', LongType(), False),
  StructField('transferLog', StringType(), False),
  StructField('destinationBankNumber', LongType(), False),
  StructField('destinationBankBranch', LongType(), False),
  StructField('destinationBankAccount', StringType(), False)
])

transf = spark.read\
  .csv(path, schema = schema, sep = ";", header = True)
```

```{python}
transf.show(5)
```

Now, lets create a window object using the `transf` DataFrame as our target. In order to create a `Window` object, you need to import the `Window` class from the `pyspark.sql.window` module:
```{python}
from pyspark.sql.window import Window
```

As we described before, the `transf` DataFrame describes a set of transfers made in a fictitious bank. A reasonable way of splitting the rows of this DataFrame into window frames is using the `dateTransfer` column. In other words, each partition in the `dateTransfer` column will identify a different existing window frame in this DataFrame. We create this window specification in the snippet below:

```{python}
window_spec = Window.partitionBy('dateTransfer')
```

The above window object specifies that each unique date value present in the `dateTransfer` column identifies a different window frame in the `transf` DataFrame. @fig-window-spec1 presents this idea visually. So each partition in the `dateTransfer` column creates a different window frame. And each window frame will become an input to a window function (when we use one).

![Visualizing the window frames](./../Figures/window-spec1.png){#fig-window-spec1}

Until this point, defining window frames are very much like defining groups in your DataFrame with *group by* functions (i.e. window frames are very similar to groups). However, despite their similarities, these functions are used for different purposes. As we described at @sec-group-by, you use `groupby()` when you want to collect the aggregate values into a new DataFrame using the `agg()` method. In contrast, when you use a window function, you usually want to add new columns into your existing DataFrame using the `withColumn()` method.

In the above example, we specified only the partition component of the window frames. This means that **the ordering component is not mandatory** for using window functions. But lets define the ordering component anyway.

The partitioning component of the window object specifies which partitions of the DataFrame are translated into window frames. In the other hand, the ordering component of the window object specifies how the rows within the window frame are ordered. Defining this component becomes very important when we are working with window functions that outputs (or that uses) indexes. For example, we might want to use the first (or the *nth*) row in each window frame.


<!--
dense_rank
row_number
rank
percent_rank

nth
lead
lag
first
last

sum
max
min
mean
stddev

cume_dist
-->
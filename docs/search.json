[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to pyspark",
    "section": "",
    "text": "About this website\nWelcome! This is the initial page for the “Open Access” HTML version of the book “Introduction to pyspark”, written by Pedro Duarte Faria. This book provides an introduction to pyspark, which is a python API to Apache Spark.\nThis book is still under active construction. This means that not all chapters are ready yet, and some of its current contents might change in the close future. This book is licensed by the CC-BY 4.0 Creative Commons Attribution 4.0 International Public License."
  },
  {
    "objectID": "Chapters/01-intro.html#introduction",
    "href": "Chapters/01-intro.html#introduction",
    "title": "Preface",
    "section": "Introduction",
    "text": "Introduction\nIn essence, pyspark is a python package that provides an API for Apache Spark. In other words, with pyspark you are able to use the python language to write Spark applications and run them on a Spark cluster in a scalable and elegant way. This book focus on teaching the fundamentals of pyspark, and how to use it for big data analysis.\nThis book, also contains a small introduction to key python concepts that are important to understand how pyspark is organized and how it works in practice, and, since we will be using Spark under the hood, is very important to understand a little bit of how Spark works, so, we provide a small introduction to Spark as well.\nBig part of the knowledge exposed here is extracted from a lot of practical experience of the author, working with pyspark to analyze big data at platforms such as Databricks1. Another part of the knowledge is extracted from the official documentation of Apache Spark (Apache Spark Official Documentation 2022), as well as some established works such as Chambers and Zaharia (2018) and Damji et al. (2020)."
  },
  {
    "objectID": "Chapters/01-intro.html#about-the-author",
    "href": "Chapters/01-intro.html#about-the-author",
    "title": "Preface",
    "section": "About the author",
    "text": "About the author\nPedro Duarte Faria have a bachelor degree in Economics from Federal University of Ouro Preto - Brazil. Currently, he is a Data Analyst at Take Blip, and an Associate Developer for Apache Spark 3.0 certified by Databricks.\nThe author have more than 3 years of experience in the data analysis market. He developed data pipelines, reports and analysis for research institutions and some of the largest companies in the brazilian financial sector, such as the BMG Bank, Sodexo and Pan Bank, besides dealing with databases that go beyond the billion rows.\nFurthermore, Pedro have given several lectures and courses about the R language, inside graduate centers (such as PPEA-UFOP2), in addition to federal and state organizations (such as FJP-MG3). As researcher, he have experience in the field of Science, Technology and Innovation Economics.\nPersonal Website: https://pedro-faria.netlify.app/\nTwitter: @PedroPark9\nMastodon: @pedropark99@fosstodon.org"
  },
  {
    "objectID": "Chapters/01-intro.html#some-conventions-of-this-book",
    "href": "Chapters/01-intro.html#some-conventions-of-this-book",
    "title": "Preface",
    "section": "Some conventions of this book",
    "text": "Some conventions of this book\n\nPython code and terminal commands\nThis book is about pyspark, which is a python package. As a result, we will be exposing a lot of python code across the entire book. Examples of python code, are always shown inside a gray rectangle, like this example below.\nEvery visible result that this python code produce, will be shown outside of the gray rectangle, just below the command that produced that visible result. Besides that, every line of result will always be written in plain black. So in the example below, the value 729 is the only visible result of this python code, and, the statement print(y) is the command that triggered this visible result.\n\nx = 3\ny = 9 ** x\n\nprint(y)\n\n729\n\n\nFurthermore, all terminal commands that we expose in this book, will always be: pre-fixed by Terminal$; written in black; and, not outlined by a gray rectangle. In the example below, we can easily detect that this command pip install jupyter should be inserted in the terminal of the OS (whatever is the terminal that your OS uses), and not in the python interpreter, because this command is prefixed with Terminal$.\n#| eval: false\nTerminal$ pip install jupyter\nSome terminal commands may produce visible results as well. In that case, these results will be right below the respective command, and will not be pre-fixed with Terminal$. For example, we can see below that the command echo \"Hello!\" produces the result \"Hello!\".\nTerminal$ echo \"Hello!\"\n\n\nPython objects, functions and methods\nWhen I refer to some python object, function, method or package, I will use a monospaced font. In other words, if I have a python object called “name”, and, I am describing this object, I will use name in the paragraph, and not “name”. The same logic applies to functions, methods and package names.\n\n\nBe aware of differences between OS’s!\nSpark is available for all three main operational systems (or OS’s) used in the world (Windows, MacOs and Linux). I will use constantly the word OS as an abbreviation to “operational system”.\nThe snippets of python code shown throughout this book should just run correctly no matter which one of the three OS’s you are using. In other words, the python code snippets are made to be portable. So you can just copy and paste them to your computer, no matter which OS you are using.\nBut, at some points, I may need to show you some terminal commands that are OS specific, and are not easily portable. For example, Linux have a package manager, but Windows does not have one. This means that, if you are on Linux, you will need to use some terminal commands to install some necessary programs (like python). In contrast, if you are on Windows, you will generally download executable files (.exe) that make this installation for you.\nIn cases like this, I will always point out the specific OS of each one of the commands, or, I will describe the necessary steps to be made on each one the OS’s. Just be aware that these differences exists between the OS’s."
  },
  {
    "objectID": "Chapters/01-intro.html#install-the-necessary-software",
    "href": "Chapters/01-intro.html#install-the-necessary-software",
    "title": "Preface",
    "section": "Install the necessary software",
    "text": "Install the necessary software\nIf you want to follow the examples shown in this book, you must have Apache Spark and pyspark installed on your machine. If you do not know how to do this, you can consult the contents of Appendix B. This appendix give a tutorial with step-to-step on how to install these tools on Linux and Windows.\n\n\n\n\nApache Spark Official Documentation. 2022. Documentation for Apache Spark 3.2.1; Available at: https://spark.apache.org/docs/latest/.\n\n\nChambers, Bill, and Matei Zaharia. 2018. Spark: The Definitive Guide: Big Data Processing Made Simple. Sebastopol, CA: O’Reilly Media.\n\n\nDamji, Jules, Brooke Wenig, Tathagata Das, and Denny Lee. 2020. Learning Spark: Lightning-Fast Data Analytics. Sebastopol, CA: O’Reilly Media."
  },
  {
    "objectID": "Chapters/02-python.html#introduction",
    "href": "Chapters/02-python.html#introduction",
    "title": "1  Key concepts of python",
    "section": "1.1 Introduction",
    "text": "1.1 Introduction\nIf you have experience with python, and understands how objects and classes works, you might want to skip this entire chapter. But, if you are new to the language and do not have much experience with it, you might want to stick a little bit, and learn a few key concepts that will help you to understand how the pyspark package is organized, and how to work with it."
  },
  {
    "objectID": "Chapters/02-python.html#scripts",
    "href": "Chapters/02-python.html#scripts",
    "title": "1  Key concepts of python",
    "section": "1.2 Scripts",
    "text": "1.2 Scripts\nPython programs are written in plain text files that are saved with the .py extension. After you save these files, they are usually called “scripts”. So a script is just a text file that contains all the commands that make your python program.\nThere are many IDEs or programs that help you to write, manage, run and organize this kind of files (like Microsoft Visual Studio Code1, PyCharm2, Anaconda3 and RStudio4). Many of these programs are free to use, and, are easy to install.\nBut, if you do not have any of them installed, you can just create a new plain text file from the built-in Notepad program of your OS (operational system), and, save it with the .py extension."
  },
  {
    "objectID": "Chapters/02-python.html#how-to-run-a-python-program",
    "href": "Chapters/02-python.html#how-to-run-a-python-program",
    "title": "1  Key concepts of python",
    "section": "1.3 How to run a python program",
    "text": "1.3 How to run a python program\nAs you learn to write your Spark applications with pyspark, at some point, you will want to actually execute this pyspark program, to see its result. To do so, you need to execute it as a python program. There are many ways to run a python program, but I will show you the more “standard” way. That is to use the python command inside the terminal of your OS (you need to have python already installed).\nAs an example, lets create a simple “Hello world” program. First, open a new text file then save it somewhere in your machine (with the name hello.py). Remember to save the file with the .py extension. Then copy and paste the following command into this file:\n\nprint(\"Hello World!\")\n\nIt will be much easier to run this script, if you open the terminal inside the folder where you save the hello.py file. If you do not know how to do this, look at section Appendix A. After you opened the terminal inside the folder, just run the python3 hello.py command. As a result, python will execute hello.py, and, the text Hello World! should be printed to the terminal:\nTerminal$ python3 hello.py\nHello World!\nBut, if for some reason you could not open the terminal inside the folder, just open a terminal (in any way you can), then, use the cd command (stands for “change directory”) with the path to the folder where you saved hello.py. This way, your terminal will be rooted in this folder.\nFor example, if I saved hello.py inside my Documents folder, the path to this folder in Windows would be something like this: \"C:\\Users\\pedro\\Documents\". On the other hand, this path on Linux would be something like \"/usr/pedro/Documents\". So the command to change to this directory would be:\n{terminal, eval = FALSE} # On Windows: Terminal$ cd \"C:\\Users\\pedro\\Documents\" # On Linux: Terminal$ cd \"/usr/pedro/Documents\"\nAfter this cd command, you can run the python hello.py command in the terminal, and get the exact same result of the previous example.\nThere you have it! So every time you need to run your python program (or your pyspark program), just open a terminal and run the command python <complete path to your script>. If the terminal is rooted on the folder where you saved your script, you can just use the python <name of the script> command."
  },
  {
    "objectID": "Chapters/02-python.html#objects",
    "href": "Chapters/02-python.html#objects",
    "title": "1  Key concepts of python",
    "section": "1.4 Objects",
    "text": "1.4 Objects\nAlthough python is a general-purpose language, most of its features are focused on object-oriented programming. Meaning that, python is a programming language focused on creating, managing and modifying objects and classes of objects.\nSo, when you work with python, you are basically applying many operations and functions over a set of objects. In essence, an object in python, is a name that refers to a set of data. This data can be anything that you computer can store (or represent).\nHaving that in mind, an object is just a name, and this name is a reference, or a key to access some data. To define an object in python, you must use the assignment operator, which is the equal sign (=). In the example below, we are defining, or, creating an object called x, and it stores the value 10. Therefore, with the name x we can access this value of 10.\n\nx = 10\nprint(x)\n\n10\n\n\nWhen we store a value inside an object, we can easily reuse this value in multiple operations or expressions:\n\n# Multiply by 2\nprint(x * 2)\n\n20\n\n\n\n# Divide by 3\nprint(x / 3)\n\n3.3333333333333335\n\n\n\n# Print its class\nprint(type(x))\n\n<class 'int'>\n\n\nRemember, an object can store any type of value, or any type of data. For example, it can store a single string, like the object salutation below:\n\nsalutation = \"Hello! My name is Pedro\"\n\nOr, a list of multiple strings:\n\nnames = [\n  \"Anne\", \"Vanse\", \"Elliot\",\n  \"Carlyle\", \"Ed\", \"Memphis\"\n]\n\nprint(names)\n\n['Anne', 'Vanse', 'Elliot', 'Carlyle', 'Ed', 'Memphis']\n\n\nOr a dict containing the description of a product:\n\nproduct = {\n  'name': 'Coca Cola',\n  'volume': '2 litters',\n  'price': 2.52,\n  'group': 'non-alcoholic drinks',\n  'department': 'drinks'\n}\n\nprint(product)\n\n{'name': 'Coca Cola', 'volume': '2 litters', 'price': 2.52, 'group': 'non-alcoholic drinks', 'department': 'drinks'}\n\n\nAnd many other things…"
  },
  {
    "objectID": "Chapters/02-python.html#expressions",
    "href": "Chapters/02-python.html#expressions",
    "title": "1  Key concepts of python",
    "section": "1.5 Expressions",
    "text": "1.5 Expressions\nPython programs are organized in blocks of expressions (or statements). A python expression is a statement that describes an operation to be performed by the program. For example, the expression below describes the sum between 3 and 5.\n\n3 + 5\n\n8\n\n\nThe expression above is composed of numbers (like 3 and 5) and a operator, more specifically, the sum operator (+). But any python expression can include a multitude of different items. It can be composed of functions (like print(), map() and str()), constant strings (like \"Hello World!\"), logical operators (like !=, <, > and ==), arithmetic operators (like *, /, **, %, - and +), structures (like lists, arrays and dicts) and many other types of commands.\nBelow we have a more complex example, that contains the def keyword (which starts a function definition; in the example below, this new function being defined is double()), many built-in functions (list(), map() and print()), a arithmetic operator (*), numbers and a list (initiated by the pair of brackets - []).\n\ndef double(x):\n  return x * 2\n  \nprint(list(map(double, [4, 2, 6, 1])))\n\n[8, 4, 12, 2]\n\n\nPython expressions are evaluated in a sequential manner (from top to bottom of your python file). In other words, python runs the first expression in the top of your file, them, goes to the second expression, and runs it, them goes to the third expression, and runs it, and goes on and on in that way, until it hits the end of the file. So, in the example above, python executes the function definition (initiated at def double(x):), before it executes the print() statement, because the print statement is below the function definition.\nThis order of evaluation is commonly referred as “control flow” in many programming languages. Sometimes, this order can be a fundamental part of the python program. Meaning that, sometimes, if we change the order of the expressions in the program, we can produce unexpected results (like an error), or change the results produced by the program.\nAs an example, the program below prints the result 4, because the print statement is executed before the expression x = 40.\n\nx = 1\n\nprint(x * 4)\n\nx = 40\n\n4\n\n\nBut, if we execute the expression x = 40 before the print statement, we then change the result produced by the program.\n\nx = 1\nx = 40\n\nprint(x * 4)\n\n160\n\n\nIf we go a little further, and, put the print statement as the first expression of the program, we then get a name error. This error warns us that, the object named x is not defined (i.e. it does not exist).\n\nprint(x * 4)\n\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nNameError: name 'x' is not defined\n\nx = 1\nx = 40\n\nThis error occurs, because inside the print statement, we call the name x. But, this is the first expression of the program, and at this point of the program, we did not defined a object called x. We make this definition, after the print statement, with x = 1 and x = 40. In other words, at this point, python do not know any object called x."
  },
  {
    "objectID": "Chapters/02-python.html#packages-or-libraries",
    "href": "Chapters/02-python.html#packages-or-libraries",
    "title": "1  Key concepts of python",
    "section": "1.6 Packages (or libraries)",
    "text": "1.6 Packages (or libraries)\nA python package (or a python “library”) is basically a set of functions and classes that provides important functionality to solve a specific problem. And pyspark is one of these many python packages available.\nPython packages are usually published (that is, made available to the public) through the PyPI archive5. If a python package is published in PyPI, then, you can easily install it through the pip tool, that we just used in Appendix B.\nTo use a python package, you always need to: 1) have this package installed on your machine; 2) import this package in your python script. If a package is not installed in your machine, you will face a ModuleNotFoundError as you try to use it, like in the example below.\n\nimport pandas\n\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nModuleNotFoundError: No module named 'pandas'\nIf your program produce this error, is very likely that you are trying to use a package that is not currently installed on your machine. To install it, you may use the pip install <name of the package> command on the terminal of your OS.\n{terminal, eval = FALSE} pip install pandas\nBut, if this package is already installed in your machine, then, you can just import it to your script. To do this, you just include an import statement at the start of your python file. For example, if I want to use the DataFrame function from the pandas package:\n\n# Now that I installed the `pandas` package with `pip`\n# this `import` statement works without any errors:\nimport pandas\n\ndf = pandas.DataFrame([\n  (1, 3214), (2, 4510), \n  (1, 9082), (4, 7822)\n])\n\nprint(df)\n\n   0     1\n0  1  3214\n1  2  4510\n2  1  9082\n3  4  7822\n\n\nTherefore, with import pandas I can access any of the functions available in the pandas package, by using the dot operator after the name of the package (pandas.<name of the function>). However, it can become very annoying to write pandas. every time you want to access a function from pandas, specially if you use it constantly in your code.\nTo make life a little easier, python offers some alternative ways to define this import statement. First, you can give an alias to this package that is shorter/easier to write. As an example, nowadays, is virtually a industry standard to import the pandas package as pd. To do this, you use the as keyword in your import statement. This way, you can access the pandas functionality with pd.<name of the function>:\n\nimport pandas as pd\n\ndf = pd.DataFrame([\n  (1, 3214), (2, 4510), \n  (1, 9082), (4, 7822)\n])\n\nprint(df)\n\n   0     1\n0  1  3214\n1  2  4510\n2  1  9082\n3  4  7822\n\n\nIn contrast, if you want to make your life even easier and produce a more “clean” code, you can import (from the package) just the functions that you need to use. In this method, you can eliminate the dot operator, and refer directly to the function by its name. To use this method, you include the from keyword in your import statement, like this:\n\nfrom pandas import DataFrame\n\ndf = DataFrame([\n  (1, 3214), (2, 4510), \n  (1, 9082), (4, 7822)\n])\n\nprint(df)\n\n   0     1\n0  1  3214\n1  2  4510\n2  1  9082\n3  4  7822\n\n\nJust to be clear, you can import multiple functions from the package, by listing them. Or, if you prefer, you can import all components of the package (or module/sub-module) by using the star shortcut (*):\n\n# Import `search()`, `match()` and `compile()` functions:\nfrom re import search, match, compile\n# Import all functions from the `os` package\nfrom os import *\n\nSome packages may be very big, and includes many different functions and classes. As the size of the package becomes bigger and bigger, developers tend to divide this package in many “modules”. In other words, the functions and classes of this python package are usually organized in “modules”.\nAs an example, the pyspark package is a fairly large package, that contains many classes and functions. Because of it, the package is organized in a number of modules, such as sql (to access Spark SQL), pandas (to access the Pandas API of Spark), ml (to access Spark MLib).\nTo access the functions available in each one of these modules, you use the dot operator between the name of the package and the name of the module. For example, to import all components from the sql and pandas modules of pyspark, you would do this:\n\nfrom pyspark.sql import *\nfrom pyspark.pandas import *\n\nGoing further, we can have sub-modules (or modules inside a module) too. As an example, the sql module of pyspark have the functions and window sub-modules. To access these sub-modules, you use the dot operator again:\n\n# Importing `functions` and `window` sub-modules:\nimport pyspark.sql.functions as F\nimport pyspark.sql.window as W"
  },
  {
    "objectID": "Chapters/02-python.html#methods-versus-functions",
    "href": "Chapters/02-python.html#methods-versus-functions",
    "title": "1  Key concepts of python",
    "section": "1.7 Methods versus Functions",
    "text": "1.7 Methods versus Functions\nBeginners tend mix these two types of functions in python, but they are not the same. So lets describe the differences between the two.\nStandard python functions, are functions that we apply over an object. A classical example, is the print() function. You can see in the example below, that we are applying print() over the result object.\n\nresult = 10 + 54\nprint(result)\n\n64\n\n\nOther examples of a standard python function would be map() and list(). See in the example below, that we apply the map() function over a set of objects:\n\nwords = ['apple', 'star', 'abc']\nlengths = map(len, words)\nlist(lengths)\n\n[5, 4, 3]\n\n\nIn contrast, a python method is a function registered inside a python class. In other words, this function belongs to the class itself, and cannot be used outside of it. This means that, in order to use a method, you need to have an instance of the class where it is registered.\nFor example, the startswith() method belongs to the str class (this class is used to represent strings in python). So to use this method, we need to have an instance of this class saved in a object that we can access. Note in the example below, that we access the startswith() method through the name object. This means that, startswith() is a function. But, we cannot use it without an object of class str, like name.\n\nname = \"Pedro\"\nname.startswith(\"P\")\n\nTrue\n\n\nNote in the example above, that we access any class method in the same way that we would access a sub-module/module of a package. That is, by using the dot operator (.).\nSo, if we have a class called people, and, this class has a method called location(), we can use this location() method by using the dot operator (.) with the name of an object of class people. If an object called x is an instance of people class, then, we can do x.location().\nBut if this object x is of a different class, like int, then we can no longer use the location() method, because this method does not belong to the int class. For example, if your object is from class A, and, you try to use a method of class B, you will get an AttributeError.\nIn the example exposed below, I have an object called number of class int, and, I try to use the method startswith() from str class with this object:\n\nnumber = 2\n# You can see below that, the `x` object have class `int`\ntype(number)\n# Trying to use a method from `str` class\nnumber.startswith(\"P\")\n\nAttributeError: 'int' object has no attribute 'startswith'"
  },
  {
    "objectID": "Chapters/02-python.html#identifying-classes-and-their-methods",
    "href": "Chapters/02-python.html#identifying-classes-and-their-methods",
    "title": "1  Key concepts of python",
    "section": "1.8 Identifying classes and their methods",
    "text": "1.8 Identifying classes and their methods\nOver the next chapters, you will realize that pyspark programs tend to use more methods than standard functions. So most of the functionality of pyspark resides in class methods. As a result, the capability of understanding the objects that you have in your python program, and, identifying its classes and methods will be crucial while you are developing and debugging your Spark applications.\nEvery existing object in python represents an instance of a class. In other words, every object in python is associated to a given class. You can always identify the class of an object, by applying the type() function over this object. In the example below, we can see that, the name object is an instance of the str class.\n\nname = \"Pedro\"\ntype(name)\n\nstr\n\n\nIf you do not know all the methods that a class have, you can always apply the dir() function over this class to get a list of all available methods. For example, lets suppose you wanted to see all methods from the str class. To do so, you would do this:\n\ndir(str)\n\n['__add__', '__class__', '__contains__', '__delattr__', '__dir__',\n '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__',\n '__getitem__', '__getnewargs__', '__gt__', '__hash__', '__init__',\n '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', \n '__mod__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__',\n '__repr__', '__rmod__', '__rmul__', '__setattr__', '__sizeof__', \n '__str__', '__subclasshook__', 'capitalize', 'casefold', 'center',\n 'count', 'encode', 'endswith', 'expandtabs', 'find', 'format',\n 'format_map', 'index', 'isalnum', 'isalpha', 'isascii', 'isdecimal',\n 'isdigit', 'isidentifier', 'islower', 'isnumeric', 'isprintable',\n 'isspace', 'istitle', 'isupper', 'join', 'ljust', 'lower', 'lstrip', \n 'maketrans', 'partition', 'replace', 'rfind', 'rindex', 'rjust', \n 'rpartition', 'rsplit', 'rstrip', 'split', 'splitlines', 'startswith',\n 'strip', 'swapcase', 'title', 'translate', 'upper', 'zfill']"
  },
  {
    "objectID": "Chapters/03-spark.html#introduction",
    "href": "Chapters/03-spark.html#introduction",
    "title": "2  Introducing Apache Spark",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\nIn essence, pyspark is an API to Apache Spark (or simply Spark). In other words, with pyspark we can build Spark applications using the python language. So, by learning a little more about Spark, you will understand a lot more about pyspark."
  },
  {
    "objectID": "Chapters/03-spark.html#what-is-spark",
    "href": "Chapters/03-spark.html#what-is-spark",
    "title": "2  Introducing Apache Spark",
    "section": "2.2 What is Spark?",
    "text": "2.2 What is Spark?\nSpark is a multi-language engine for large-scale data processing that supports both single-node machines and clusters of machines (Apache Spark Official Documentation 2022). Nowadays, Spark became the de facto standard for structure and manage big data applications.\nIt has a number of features that its predecessors did not have, like the capacity for in-memory processing and stream processing (Karau et al. 2015). But, the most important feature of all, is that Spark is an unified platform for big data processing (Chambers and Zaharia 2018).\nThis means that Spark comes with multiple built-in libraries and tools that deals with different aspects of the work with big data. It has a built-in SQL engine1 for performing large-scale data processing; a complete library for scalable machine learning (MLib2); a stream processing engine3 for streaming analytics; and much more;\nIn general, big companies have many different data necessities, and as a result, the engineers and analysts may have to combine and integrate many tools and techniques together, so they can build many different data pipelines to fulfill these necessities. But this approach can create a very serious dependency problem, which imposes a great barrier to support this workflow. This is one of the big reasons why Spark got so successful. It eliminates big part of this problem, by already including almost everything that you might need to use.\n\nSpark is designed to cover a wide range of workloads that previously required separate distributed systems … By supporting these workloads in the same engine, Spark makes it easy and inexpensive to combine different processing types, which is often necessary in production data analysis pipelines. In addition, it reduces the management burden of maintaining separate tools (Karau et al. 2015)."
  },
  {
    "objectID": "Chapters/03-spark.html#spark-application",
    "href": "Chapters/03-spark.html#spark-application",
    "title": "2  Introducing Apache Spark",
    "section": "2.3 Spark application",
    "text": "2.3 Spark application\nYour personal computer can do a lot of things, but, it cannot efficiently deal with huge amounts of data. For this situation, we need several machines working together, adding up their resources to deal with the volume or complexity of the data. Spark is the framework that coordinates the computations across this set of machines (Chambers and Zaharia 2018). Because of this, a relevant part of Spark’s structure is deeply connected to distributed computing models.\nYou probably do not have a cluster of machines at home. So, while following the examples in this book, you will be running Spark on a single machine (i.e. single node mode). But lets just forget about this detail for a moment.\nIn every Spark application, you always have a single machine behaving as the driver node, and multiple machines behaving as the worker nodes. The driver node is responsible for managing the Spark application, i.e. asking for resources, distributing tasks to the workers, collecting and compiling the results, …. The worker nodes are responsible for executing the tasks that are assigned to them, and they need to send the results of these tasks back to the driver node.\nEvery Spark application is distributed into two different and independent processes: 1) a driver process; 2) and a set of executor processes (Chambers and Zaharia 2018). The driver process, or, the driver program, is where your application starts, and it is executed by the driver node. This driver program is responsible for: 1) maintaining information about your Spark Application; 2) responding to a user’s program or input; 3) and analyzing, distributing, and scheduling work across the executors (Chambers and Zaharia 2018).\nEvery time a Spark application starts, the driver process has to communicate with the cluster manager, to acquire workers to perform the necessary tasks. In other words, the cluster manager decides if Spark can use some of the resources (i.e. some of the machines) of the cluster. If the cluster manager allow Spark to use the nodes it needs, the driver program will break the application into many small tasks, and will assign these tasks to the worker nodes.\nThe executor processes, are the processes that take place within each one of the worker nodes. Each executor process is composed of a set of tasks, and the worker node is responsible for performing and executing these tasks that were assigned to him, by the driver program. After executing these tasks, the worker node will send the results back to the driver node (or the driver program). If they need, the worker nodes can communicate with each other, while performing its tasks.\nThis structure is represented in Figure 2.1:\n\n\n\nFigure 2.1: Spark application structure on a cluster of computers\n\n\nWhen you run Spark on a cluster of computers, you write the code of your Spark application (i.e. your pyspark code) on your (single) local computer, and then, submit this code to the driver node. After that, the driver node takes care of the rest, by starting your application, creating your Spark Session, asking for new worker nodes, sending the tasks to be performed, collecting and compiling the results and giving back these results to you.\nHowever, when you run Spark on your (single) local computer, the process is very similar. But, instead of submitting your code to another computer (which is the driver node), you will submit to your own local computer. In other words, when Spark is running on single-node mode, your computer becomes the driver and the worker node at the same time."
  },
  {
    "objectID": "Chapters/03-spark.html#spark-application-versus-pyspark-application",
    "href": "Chapters/03-spark.html#spark-application-versus-pyspark-application",
    "title": "2  Introducing Apache Spark",
    "section": "2.4 Spark application versus pyspark application",
    "text": "2.4 Spark application versus pyspark application\nThe pyspark package is just a tool to write Spark applications using the python programming language. This means, that every pyspark application is a Spark application written in python.\nWith this conception in mind, you can understand that a pyspark application is a description of a Spark application. When we compile (or execute) our python program, this description is translated into a raw Spark application that will be executed by Spark.\nTo write a pyspark application, you write a python script that uses the pyspark library. When you execute this python script with the python interpreter, the application will be automatically converted to Spark code, and will be sent to Spark to be executed across the cluster;"
  },
  {
    "objectID": "Chapters/03-spark.html#core-parts-of-a-pyspark-program",
    "href": "Chapters/03-spark.html#core-parts-of-a-pyspark-program",
    "title": "2  Introducing Apache Spark",
    "section": "2.5 Core parts of a pyspark program",
    "text": "2.5 Core parts of a pyspark program\nIn this section, I want to point out the core parts that composes every pyspark program. This means that every pyspark program that you write will have these “core parts”, which are:\n\nimporting the Spark libraries (or packages);\nstarting your Spark Session;\ndefining a set of transformations and actions over Spark DataFrames;\n\n\n2.5.1 Importing the Spark libraries (or packages)\nSpark comes with a lot of functionality installed. But, in order to use it in your pyspark program, you have to import most of these functionalities to your session. This means that you have to import specific packages (or “modules”) of pyspark to your python session.\nFor example, most of the functions used to define our transformations and aggregations in Spark DataFrames, comes from the pyspark.sql.functions module.\nThat is why we usually start our python scripts by importing functions from this module, like this:\n\nfrom pyspark.sql.functions import sum, col\nsum_expr = sum(col('Value'))\n\nOr, importing the entire module with the import keyword, like this:\n\nimport pyspark.sql.functions as F\nsum_expr = F.sum(F.col('Value'))\n\n\n\n2.5.2 Starting your Spark Session\nEvery Spark application starts with a Spark Session. Basically, the Spark Session is the entry point to your application. This means that, in every pyspark program that you write, you should always start by defining your Spark Session. We do this, by using the getOrCreate() method from pyspark.sql.SparkSession.builder module.\nJust store the result of this method in any python object. Is very common to name this object as spark, like in the example below. This way, you can access all the information and methods of Spark from this spark object.\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n\n\n\n2.5.3 Defining a set of transformations and actions\nEvery pyspark program is composed by a set of transformations and actions over a set of Spark DataFrames.\nWe will explain Spark DataFrames in more deth on the Chapter 3. For now just understand that they are the basic data sctructure that feed all pyspark programs.\nIn other words, on every pyspark program we are transforming multiple Spark DataFrames to get the result we want. As an example, in the script below we begin with the Spark DataFrame stored in the object students, and, apply multiple transformations over it to build the ar_department DataFrame. Lastly, we apply the .show() action over the ar_department DataFrame:\n\nfrom pyspark.sql.functions import col\n# Apply some transformations over\n# the `students` DataFrame:\nar_department = students\\\n  .filter(col('Age') > 22)\\\n  .withColumn('IsArDepartment', col('Department') == 'AR')\\\n  .orderBy(col('Age').desc())\n  \n  \n# Apply the `.show()` action\n# over the `ar_department` DataFrame:\nar_department.show()"
  },
  {
    "objectID": "Chapters/03-spark.html#building-your-first-spark-application",
    "href": "Chapters/03-spark.html#building-your-first-spark-application",
    "title": "2  Introducing Apache Spark",
    "section": "2.6 Building your first Spark application",
    "text": "2.6 Building your first Spark application\nTo demonstrate what a pyspark program looks like, lets write and run our first example of a Spark application. This Spark application will build a simple table of 1 column that contains 5 numbers, and then, it will return a simple python list containing this five numbers as a result.\n\n2.6.1 Writing the code\nFirst, create a new blank text file in your computer, and save it somewhere with the name spark-example.py. Do not forget to put the .py extension in the name. This program we are writing together is a python program, and should be treated as such. With the .py extension in the name file, you are stating this fact quite clearly to your computer.\nAfter you created and saved the python script (i.e. the text file with the .py extension), you can start writing your pyspark program. As we noted in the previous section, you should always start your pyspark program by defining your Spark Session, with this code:\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n\nAfter you defined your Spark Session, and saved it in an object called spark, you can now access all Spark’s functionality through this spark object.\nTo create our first Spark table we use the range() method from the spark object. The range() method works similarly as the standard python function called range(). It basically creates a sequence of numbers, from 0 to \\(n - 1\\). However, this range() method from spark stores this sequence of numbers as rows in a Spark table (or a Spark DataFrame):\n\ntable = spark.range(5)\n\nAfter this step, we want to collect all the rows of the resulting table into a python list. And to do that, we use the collect() method from the Spark table:\n\nresult = table.collect()\nprint(result)\n\nSo, the entire program is composed of these three parts (or sections) of code. If you need it, the entire program is reproduced below. You can copy and paste all of this code to your python script, and then, save it:\n\n# The entire program:\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n\ntable = spark.range(5)\nresult = table.collect()\nprint(result)\n\n\n\n2.6.2 Executing the code\nNow that you have written your first Spark application with pyspark, you want to execute this application and see its results. Yet, to run a pyspark program, remember that you need to have the necessary software installed on your machine. I talk about how to install these software at Appendix B.\nAnyway, to execute this pyspark that you wrote, you need send this script to the python interpreter, and to do this you need to: 1) open a terminal inside the folder where you python script is stored; and, 2) use the python command from the terminal with the name of your python script.\nIf you do not know how to open a terminal from inside a folder of your computer, you can consult Appendix A of this book, where I teach you how to do it.\nIn my current situation, I running Spark on a Ubuntu distribution, and, I saved the spark-example.py script inside a folder called SparkExample. This folder is located at the path ~/Documentos/Projetos/Livros/Introd-pyspark/SparkExample of my computer. This means that, I need to open a terminal that is rooted inside this SparkExample folder.\nYou probably have saved your spark-example.py file in a different folder of your computer. This means that you need to open the terminal from a different folder.\nAfter I opened a terminal rooted inside the SparkExample folder. I just use the python3 command to access the python interpreter, and, give the name of the python script that I want to execute. In this case, the spark-example.py file. As a result, our first pyspark program will be executed:\n\nTerminal$ python3 spark-example.py\n\n\n\n23/01/20 22:24:19 WARN Utils: Your hostname, pedro-dev resolves to a loopback address: 127.0.1.1; using 192.168.0.90 instead (on interface wlp1s0) 23/01/20 22:24:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address 23/01/20 22:24:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 23/01/20 22:24:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041. [Row(id=0), Row(id=1), Row(id=2), Row(id=3), Row(id=4)]\n\n\nYou can see in the above result, that this Spark application produces a sequence of numbers, from 0 to 4, and, returns this sequence as a set of Row objects, inside a python list.\nCongratulations! You have just run your first Spark application using pyspark!"
  },
  {
    "objectID": "Chapters/03-spark.html#overview-of-the-main-classesmodules-in-pyspark",
    "href": "Chapters/03-spark.html#overview-of-the-main-classesmodules-in-pyspark",
    "title": "2  Introducing Apache Spark",
    "section": "2.7 Overview of the main classes/modules in pyspark",
    "text": "2.7 Overview of the main classes/modules in pyspark\nBefore we continue, I want to give you a very brief overview of the main classes/modules that exists in pyspark. They are:\n\npyspark.sql.SparkSession: the SparkSession class that defines your Spark Session, or, the entry point to your Spark application;\npyspark.sql.dataframe: module that defines the DataFrame class;\npyspark.sql.column: module that defines the Column class;\npyspark.sql.types: module that contains all data types of Spark;\npyspark.sq.functions: module that contains all of the main Spark functions that we use in transformations;\n\n\n\n\n\nApache Spark Official Documentation. 2022. Documentation for Apache Spark 3.2.1; Available at: https://spark.apache.org/docs/latest/.\n\n\nChambers, Bill, and Matei Zaharia. 2018. Spark: The Definitive Guide: Big Data Processing Made Simple. Sebastopol, CA: O’Reilly Media.\n\n\nKarau, Holden, Andy Konwinski, Patrick Wendell, and Matei Zaharia. 2015. Learning Spark: Lightning-Fast Data Analytics. Sebastopol, CA: O’Reilly Media."
  },
  {
    "objectID": "Chapters/04-dataframes.html#introduction",
    "href": "Chapters/04-dataframes.html#introduction",
    "title": "3  Introducing Spark DataFrames",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nIn this chapter, you will understand how Spark represents and manages tables (or tabular data). Different programming languages and frameworks use different names to describe a table. But, in Apache Spark, they are referred as Spark DataFrames.\nIn pyspark, these DataFrames are stored inside python objects of class pyspark.sql.dataframe.DataFrame, and all the methods present in this class, are commonly referred as the DataFrame API of Spark. This is the most important API of Spark, because much of your Spark applications will heavily use this API to compose your data transformations and data flows (Chambers and Zaharia 2018)."
  },
  {
    "objectID": "Chapters/04-dataframes.html#spark-dataframes-versus-spark-datasets",
    "href": "Chapters/04-dataframes.html#spark-dataframes-versus-spark-datasets",
    "title": "3  Introducing Spark DataFrames",
    "section": "3.2 Spark DataFrames versus Spark Datasets",
    "text": "3.2 Spark DataFrames versus Spark Datasets\nSpark have two notions of structured data: DataFrames and Datasets. In summary, a Spark Dataset, is a distributed collection of data (Apache Spark Official Documentation 2022). In contrast, a Spark DataFrame is a Spark Dataset organized into named columns (Apache Spark Official Documentation 2022).\nThis means that, Spark DataFrames are very similar to tables as we know in relational databases - RDBMS, or, in spreadsheets (like Excel). So in a Spark DataFrame, each column has a name, and they all have the same number of rows. Furthermore, all the rows inside a column must store the same type of data, but each column can store a different type of data.\nIn the other hand, Spark Datasets are considered a collection of any type of data. So a Dataset might be a collection of unstructured data as well, like log files, JSON and XML trees, etc. Spark Datasets can be created and transformed trough the Dataset API of Spark. But this API is available only in Scala and Java API’s of Spark. For this reason, we do not act directly on Datasets with pyspark, only DataFrames. That’s ok, because for the most part of applications, we do want to use DataFrames, and not Datasets, to represent our data.\nHowever, what makes a Spark DataFrame different from other dataframes? Like the pandas DataFrame? Or the R native data.frame structure? Is the distributed aspect of it. Spark DataFrames are based on Spark Datasets, and these Datasets are collections of data that are distributed across the cluster. As an example, lets suppose you have the following table stored as a Spark DataFrame:\n\n\n\nID\nName\nValue\n\n\n\n\n1\nAnne\n502\n\n\n2\nCarls\n432\n\n\n3\nStoll\n444\n\n\n4\nPercy\n963\n\n\n5\nMartha\n123\n\n\n6\nSigrid\n621\n\n\n\nIf you are running Spark in a 4 nodes cluster (one is the driver node, and the other three are worker nodes). Each worker node of the cluster will store a section of this data. So you, as the programmer, will see, manage and transform this table as if it was a single and unified table. But behind the hoods, Spark will split this data and store it as many fragments across the Spark cluster. Figure 3.1 presents this notion in a visual manner.\n\n\n\nFigure 3.1: A Spark DataFrame is distributed across the cluster"
  },
  {
    "objectID": "Chapters/04-dataframes.html#partitions-of-a-spark-dataframe",
    "href": "Chapters/04-dataframes.html#partitions-of-a-spark-dataframe",
    "title": "3  Introducing Spark DataFrames",
    "section": "3.3 Partitions of a Spark DataFrame",
    "text": "3.3 Partitions of a Spark DataFrame\nA Spark DataFrame is always broken into many small pieces, and, these pieces are always spread across the cluster of machines. Each one of these small pieces of the total data are considered a DataFrame partition.\nFor the most part, you do not manipulate these partitions manually or individually (Karau et al. 2015), because Spark automatically do this job for you.\nAs we exposed in Figure 3.1, each node of the cluster will hold a piece of the total DataFrame. If we translate this distribution into a “partition” distribution, this means that each node of the cluster can hold one or multiple partitions of the Spark DataFrame.\nIf we sum all partitions present in a node of the cluster, we get a chunk of the total DataFrame. The figure below demonstrates this notion:\n\n\n\nFigure 3.2: Partitions of a DataFrame\n\n\nIf the Spark DataFrame is not big, each node of the cluster will probably store just a single partition of this DataFrame. In contrast, depending on the complexity and size of the DataFrame, Spark will split this DataFrame into more partitions that there are nodes in the cluster. In this case, each node of the cluster will hold more than 1 partition of the total DataFrame."
  },
  {
    "objectID": "Chapters/04-dataframes.html#sec-dataframe-class",
    "href": "Chapters/04-dataframes.html#sec-dataframe-class",
    "title": "3  Introducing Spark DataFrames",
    "section": "3.4 The DataFrame class in pyspark",
    "text": "3.4 The DataFrame class in pyspark\nIn pyspark, every Spark DataFrame is stored inside a python object of class pyspark.sql.dataframe.DataFrame. Or more succintly, a object of class DataFrame.\nLike any python class, the DataFrame class comes with multiple methods that are available for every object of this class. This means that you can use any of these methods in any Spark DataFrame that you create through pyspark.\nAs an example, in the code below I expose all the available methods from this DataFrame class. First, I create a Spark DataFrame with spark.range(5), and, store it in the object df5. After that, I use the dir() function to show all the methods that I can use through this df5 object:\n\ndf5 = spark.range(5)\navailable_methods = dir(df5)\nprint(available_methods)\n\n\n\n['__class__', '__delattr__', '__dict__', '__dir__',\n'__doc__', '__eq__', '__format__', '__ge__',\n'__getattr__', '__getattribute__', '__getitem__', '__gt__',\n'__hash__', '__init__', '__init_subclass__', '__le__',\n'__lt__', '__module__', '__ne__', '__new__',\n'__reduce__', '__reduce_ex__', '__repr__', '__setattr__',\n'__sizeof__', '__str__', '__subclasshook__', '__weakref__',\n'_collect_as_arrow', '_jcols', '_jdf', '_jmap',\n'_joinAsOf', '_jseq', '_lazy_rdd', '_repr_html_',\n'_sc', '_schema', '_session', '_sort_cols',\n'_sql_ctx', '_support_repr_html', '_to_corrected_pandas_type', 'agg',\n'alias', 'approxQuantile', 'cache', 'checkpoint',\n'coalesce', 'colRegex', 'collect', 'columns',\n'corr', 'count', 'cov', 'createGlobalTempView',\n'createOrReplaceGlobalTempView', 'createOrReplaceTempView', 'createTempView', 'crossJoin',\n'crosstab', 'cube', 'describe', 'distinct',\n'drop', 'dropDuplicates', 'drop_duplicates', 'dropna',\n'dtypes', 'exceptAll', 'explain', 'fillna',\n'filter', 'first', 'foreach', 'foreachPartition',\n'freqItems', 'groupBy', 'groupby', 'head',\n'hint', 'inputFiles', 'intersect', 'intersectAll',\n'isEmpty', 'isLocal', 'isStreaming', 'is_cached',\n'join', 'limit', 'localCheckpoint', 'mapInArrow',\n'mapInPandas', 'na', 'observe', 'orderBy',\n'pandas_api', 'persist', 'printSchema', 'randomSplit',\n'rdd', 'registerTempTable', 'repartition', 'repartitionByRange',\n'replace', 'rollup', 'sameSemantics', 'sample',\n'sampleBy', 'schema', 'select', 'selectExpr',\n'semanticHash', 'show', 'sort', 'sortWithinPartitions',\n'sparkSession', 'sql_ctx', 'stat', 'storageLevel',\n'subtract', 'summary', 'tail', 'take',\n'toDF', 'toJSON', 'toLocalIterator', 'toPandas',\n'to_koalas', 'to_pandas_on_spark', 'transform', 'union',\n'unionAll', 'unionByName', 'unpersist', 'where',\n'withColumn', 'withColumnRenamed', 'withColumns', 'withMetadata',\n'withWatermark', 'write', 'writeStream', 'writeTo'],\n\n\n\nAll the methods present in this DataFrame class, are commonly referred as the DataFrame API of Spark. This is the most important API of Spark. Much of your Spark applications will heavily use this API to compose your data transformations and data flows (Chambers and Zaharia 2018)."
  },
  {
    "objectID": "Chapters/04-dataframes.html#sec-building-a-dataframe",
    "href": "Chapters/04-dataframes.html#sec-building-a-dataframe",
    "title": "3  Introducing Spark DataFrames",
    "section": "3.5 Building a Spark DataFrame",
    "text": "3.5 Building a Spark DataFrame\nThere are some different methods to create a Spark DataFrame. For example, because a DataFrame is basically a Dataset of rows, we can build a DataFrame from a collection of Row’s, through the createDataFrame() method from your Spark Session:\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\nfrom datetime import date\nfrom pyspark.sql import Row\n\ndata = [\n  Row(id = 1, value = 28.3, date = date(2021,1,1)),\n  Row(id = 2, value = 15.8, date = date(2021,1,1)),\n  Row(id = 3, value = 20.1, date = date(2021,1,2)),\n  Row(id = 4, value = 12.6, date = date(2021,1,3))\n]\n\ndf = spark.createDataFrame(data)\n\nRemember that a Spark DataFrame in python is a object of class pyspark.sql.dataframe.DataFrame as you can see below:\n\ntype(df)\n\npyspark.sql.dataframe.DataFrame\n\n\nIf you try to see what is inside of this kind of object, you will get a small description of the columns present in the DataFrame as a result:\n\ndf\n\nDataFrame[id: bigint, value: double, date: date]\n\n\nSo, in the above example, we use the Row() constructor (from pyspark.sql module) to build 4 rows. The createDataFrame() method, stack these 4 rows together to form our new DataFrame df. The result is a Spark DataFrame with 4 rows and 3 columns (id, value and date).\nBut you can use different methods to create the same Spark DataFrame. As another example, with the code below, we are creating a DataFrame called students. To do this, we create two python lists (data and columns), then, deliver these lists to createDataFrame() method. Each element of data is a python tuple that represents a row in the students DataFrame. And each element of columns represent the name of a column present in this new DataFrame.\n\ndata = [\n  (12114, 'Anne', 21, 1.56, 8, 9, 10, 9, 'Economics', 'SC'),\n  (13007, 'Adrian', 23, 1.82, 6, 6, 8, 7, 'Economics', 'SC'),\n  (10045, 'George', 29, 1.77, 10, 9, 10, 7, 'Law', 'SC'),\n  (12459, 'Adeline', 26, 1.61, 8, 6, 7, 7, 'Law', 'SC'),\n  (10190, 'Mayla', 22, 1.67, 7, 7, 7, 9, 'Design', 'AR'),\n  (11552, 'Daniel', 24, 1.75, 9, 9, 10, 9, 'Design', 'AR')\n]\n\ncolumns = [\n  'StudentID', 'Name', 'Age', 'Height', 'Score1',\n  'Score2', 'Score3', 'Score4', 'Course', 'Department'\n]\n\nstudents = spark.createDataFrame(data, columns)\nstudents\n\nDataFrame[StudentID: bigint, Name: string, Age: bigint, Height: double, Score1: bigint, Score2: bigint, Score3: bigint, Score4: bigint, Course: string, Department: string]\n\n\nYou can also use a method that returns a DataFrame object by default. Examples are the table() and range() methods from your Spark Session, like we used in the Section 3.4, to create the df5 object.\nOther examples are the methods used to read data and import it to pyspark. These methods are available in the spark.read module, like spark.read.csv() and spark.read.json(). These methods will be described in more depth in Chapter 6."
  },
  {
    "objectID": "Chapters/04-dataframes.html#sec-viewing-a-dataframe",
    "href": "Chapters/04-dataframes.html#sec-viewing-a-dataframe",
    "title": "3  Introducing Spark DataFrames",
    "section": "3.6 Viewing a Spark DataFrame",
    "text": "3.6 Viewing a Spark DataFrame\nA key aspect of Spark is its laziness. In other words, for most operations, Spark will only check if your code is correct and if it makes sense. Spark will not actually run or execute the operations you are describing in your code, unless you explicit ask for it with a trigger operation, which is called an “action” (this kind of operation is described in Section 5.3).\nYou can notice this laziness in the output below:\n\nstudents\n\nDataFrame[StudentID: bigint, Name: string, Age: bigint, Height: double, Score1: bigint, Score2: bigint, Score3: bigint, Score4: bigint, Course: string, Department: string]\n\n\nBecause when we call for an object that stores a Spark DataFrame (like df and students), Spark will only calculate and print a summary of the structure of your Spark DataFrame, and not the DataFrame itself.\nSo how can we actually see our DataFrame? How can we visualize the rows and values that are stored inside of it? For this, we use the show() method. With this method, Spark will print the table as pure text, as you can see in the example below:\n\nstudents.show()\n\n[Stage 0:>                                                          (0 + 1) / 1]                                                                                \n\n\n+---------+-------+---+------+------+------+------+------+---------+----------+\n|StudentID|   Name|Age|Height|Score1|Score2|Score3|Score4|   Course|Department|\n+---------+-------+---+------+------+------+------+------+---------+----------+\n|    12114|   Anne| 21|  1.56|     8|     9|    10|     9|Economics|        SC|\n|    13007| Adrian| 23|  1.82|     6|     6|     8|     7|Economics|        SC|\n|    10045| George| 29|  1.77|    10|     9|    10|     7|      Law|        SC|\n|    12459|Adeline| 26|  1.61|     8|     6|     7|     7|      Law|        SC|\n|    10190|  Mayla| 22|  1.67|     7|     7|     7|     9|   Design|        AR|\n|    11552| Daniel| 24|  1.75|     9|     9|    10|     9|   Design|        AR|\n+---------+-------+---+------+------+------+------+------+---------+----------+\n\n\n\nBy default, this method shows only the top rows of your DataFrame, but you can specify how much rows exactly you want to see, by using show(n), where n is the number of rows. For example, I can visualize only the first 2 rows of df like this:\n\ndf.show(2)\n\n+---+-----+----------+\n| id|value|      date|\n+---+-----+----------+\n|  1| 28.3|2021-01-01|\n|  2| 15.8|2021-01-01|\n+---+-----+----------+\nonly showing top 2 rows"
  },
  {
    "objectID": "Chapters/04-dataframes.html#getting-the-name-of-the-columns",
    "href": "Chapters/04-dataframes.html#getting-the-name-of-the-columns",
    "title": "3  Introducing Spark DataFrames",
    "section": "3.7 Getting the name of the columns",
    "text": "3.7 Getting the name of the columns\nIf you need to, you can easily collect a python list with the column names present in your DataFrame, in the same way you would do in a pandas DataFrame. That is, by using the columns method of your DataFrame, like this:\n\nstudents.columns\n\n['StudentID',\n 'Name',\n 'Age',\n 'Height',\n 'Score1',\n 'Score2',\n 'Score3',\n 'Score4',\n 'Course',\n 'Department']"
  },
  {
    "objectID": "Chapters/04-dataframes.html#spark-data-types",
    "href": "Chapters/04-dataframes.html#spark-data-types",
    "title": "3  Introducing Spark DataFrames",
    "section": "3.8 Spark Data Types",
    "text": "3.8 Spark Data Types\nEach column of your Spark DataFrame is associated with a specific data type. Spark supports a large number of different data types. You can see the full list at the official documentation page1. For now, we will focus on the most used data types, which are listed below:\n\nIntegerType: Represents 4-byte signed integer numbers. The range of numbers that it can represent is from -2147483648 to 2147483647.\nLongType: Represents 8-byte signed integer numbers. The range of numbers that it can represent is from -9223372036854775808 to 9223372036854775807.\nFloatType: Represents 4-byte single-precision floating point numbers.\nDoubleType: Represents 8-byte double-precision floating point numbers.\nStringType: Represents character string values.\nBooleanType: Represents boolean values (true or false).\nTimestampType: Represents datetime values, i.e. values that contains fields year, month, day, hour, minute, and second, with the session local time-zone. The timestamp value represents an absolute point in time.\nDateType: Represents date values, i.e. values that contains fields year, month and day, without a time-zone.\n\nBesides these more “standard” data types, Spark supports two other complex types, which are ArrayType and MapType:\n\nArrayType(elementType, containsNull): Represents a sequence of elements with the type of elementType. containsNull is used to indicate if elements in a ArrayType value can have null values.\nMapType(keyType, valueType, valueContainsNull): Represents a set of key-value pairs. The data type of keys is described by keyType and the data type of values is described by valueType. For a MapType value, keys are not allowed to have null values. valueContainsNull is used to indicate if values of a MapType value can have null values.\n\nEach one of these Spark data types have a corresponding python class in pyspark, which are stored in the pyspark.sql.types module. As a result, to access, lets say, type StryngType, we can do this:\n\nfrom pyspark.sql.types import StringType\ns = StringType()\nprint(s)\n\nStringType()"
  },
  {
    "objectID": "Chapters/04-dataframes.html#the-dataframe-schema",
    "href": "Chapters/04-dataframes.html#the-dataframe-schema",
    "title": "3  Introducing Spark DataFrames",
    "section": "3.9 The DataFrame Schema",
    "text": "3.9 The DataFrame Schema\nThe schema of a Spark DataFrame is the combination of column names and the data types associated with each of these columns. Schemas can be set explicitly by you (that is, you can tell Spark how the schema of your DataFrame should look like), or, they can be automatically defined by Spark while reading or creating your data.\nYou can get a succinct description of a DataFrame schema, by looking inside the object where this DataFrame is stored. For example, lets look again to the df DataFrame.\nIn the result below, we can see that df has three columns (id, value and date). By the description id: bigint, we know that id is a column of type bigint, which translates to the LongType() of Spark. Furthermore, by the descriptions value: double and date: date, we know too that the columns value and date are of type DoubleType() and DateType(), respectively.\n\ndf\n\nDataFrame[id: bigint, value: double, date: date]\n\n\nYou can also visualize a more complete report of the DataFrame schema by using the printSchema() method, like this:\n\ndf.printSchema()\n\nroot\n |-- id: long (nullable = true)\n |-- value: double (nullable = true)\n |-- date: date (nullable = true)\n\n\n\n\n3.9.1 Accessing the DataFrame schema\nSo, by calling the object of your DataFrame (i.e. an object of class DataFrame) you can see a small description of the schema of this DataFrame. But, how can you access this schema programmatically? You do this, by using the schema method of your DataFrame, like in the example below:\n\ndf.schema\n\nStructType([StructField('id', LongType(), True), StructField('value', DoubleType(), True), StructField('date', DateType(), True)])\n\n\nThe result of the schema method, is a StructType() object, that contains some information about each column of your DataFrame. More specifically, a StructType() object is filled with multiple StructField() objects. Each StructField() object stores the name and the type of a column, and a boolean value (True or False) that indicates if this column can contain any null value inside of it.\nYou can use a for loop to iterate through this StructType() and get the information of each column separately.\n\nschema = df.schema\nfor column in schema:\n  print(column)\n\nStructField('id', LongType(), True)\nStructField('value', DoubleType(), True)\nStructField('date', DateType(), True)\n\n\nYou can access just the data type of each column by using the dataType method of each StructField() object.\n\nfor column in schema:\n  datatype = column.dataType\n  print(datatype)\n\nLongType()\nDoubleType()\nDateType()\n\n\nAnd you can do the same for column names and the boolean value (that indicates if the column can contain “null” values), by using the name and nullable methods, respectively.\n\n# Accessing the name of each column\nfor column in schema:\n  print(column.name)\n\nid\nvalue\ndate\n\n\n\n# Accessing the boolean value that indicates\n# if the column can contain null values\nfor column in schema:\n  print(column.nullable)\n\nTrue\nTrue\nTrue\n\n\n\n\n3.9.2 Building a DataFrame schema\nWhen Spark creates a new DataFrame, it will automatically guess which schema is appropriate for that DataFrame. In other words, Spark will try to guess which are the appropriate data types for each column. But, this is just a guess, and, sometimes, Spark go way off.\nBecause of that, in some cases, you have to tell Spark how exactly you want this DataFrame schema to be like. To do that, you need to build the DataFrame schema by yourself, with StructType() and StructField() constructors, alongside with the Spark data types (i.e. StringType(), DoubleType(), IntegerType(), …). Remember, all of these python classes come from the pyspark.sql.types module.\nIn the example below, the schema object represents the schema of the registers DataFrame. This DataFrame have three columns (ID, Date, Name) of types IntegerType, DateType and StringType, respectively. See below that I deliver this schema object that I built to spark.createDataFrame().\n\nfrom pyspark.sql.types import StructType, StructField\nfrom pyspark.sql.types import DateType, StringType, IntegerType\nfrom datetime import date\n\ndata = [\n  (1, date(2022, 1, 1), 'Anne'),\n  (2, date(2022, 1, 3), 'Layla'),\n  (3, date(2022, 1, 15), 'Wick'),\n  (4, date(2022, 1, 11), 'Paul')\n]\n\nschema = StructType([\n  StructField('ID', IntegerType(), True),\n  StructField('Date', DateType(), True),\n  StructField('Name', StringType(), True)\n])\n\nregisters = spark.createDataFrame(data, schema = schema)\n\nHaving this example in mind, to build a DataFrame schema from scratch, you have to build the equivalent StructType() object that represents the schema you want.\n\n\n3.9.3 Checking your DataFrame schema\nIn some cases, you need to include in your pyspark program, some checks that certifies that your Spark DataFrame have the expected schema. In other words, you want to take actions if your DataFrame have a different schema that might cause a problem in your program.\nTo check if a specific column of your DataFrame is associated with the data type \\(x\\), you have to use the DataFrame schema to check if the respective column is an “instance” of the python class that represents that data type \\(x\\). Lets use the df DataFrame as an example.\nSuppose you wanted to check if the id column is of type IntegerType. To do this check, we use the python built-in function isinstance() with the python class that represents the Spark IntegerType data type. But, you can see in the result below, that the id column is not of type IntegerType.\n\nfrom pyspark.sql.types import IntegerType\nschema = df.schema\nid_column = schema[0]\nisinstance(id_column.dataType, IntegerType)\n\nFalse\n\n\nThis unexpected result happens, because the id column is actually from the “big integer” type, or, the LongType (which are 8-byte signed integer). You can see below, that now the test results in true:\n\nfrom pyspark.sql.types import LongType\nisinstance(id_column.dataType, LongType)\n\nTrue\n\n\n\n\n\n\nApache Spark Official Documentation. 2022. Documentation for Apache Spark 3.2.1; Available at: https://spark.apache.org/docs/latest/.\n\n\nChambers, Bill, and Matei Zaharia. 2018. Spark: The Definitive Guide: Big Data Processing Made Simple. Sebastopol, CA: O’Reilly Media.\n\n\nKarau, Holden, Andy Konwinski, Patrick Wendell, and Matei Zaharia. 2015. Learning Spark: Lightning-Fast Data Analytics. Sebastopol, CA: O’Reilly Media."
  },
  {
    "objectID": "Chapters/04-columns.html#building-a-column-object",
    "href": "Chapters/04-columns.html#building-a-column-object",
    "title": "4  Introducing the Column class",
    "section": "4.1 Building a column object",
    "text": "4.1 Building a column object\nYou can refer to or create a column, by using the col() and column() functions from pyspark.sql.functions module. These functions receive a string input with the name of the column you want to create/refer to.\nTheir result are always a object of class Column. For example, the code below creates a column called ID:\n\nfrom pyspark.sql.functions import col\nid_column = col('ID')\nprint(id_column)\n\nColumn<'ID'>"
  },
  {
    "objectID": "Chapters/04-columns.html#sec-columns-related-expressions",
    "href": "Chapters/04-columns.html#sec-columns-related-expressions",
    "title": "4  Introducing the Column class",
    "section": "4.2 Columns are strongly related to expressions",
    "text": "4.2 Columns are strongly related to expressions\nMany kinds of transformations that we want to apply over a Spark DataFrame, are usually described through expressions, and, these expressions in Spark are mainly composed by column transformations. That is why the Column class, and its methods, are so important in Spark.\nColumns in Spark are so strongly related to expressions that the columns themselves are initially interpreted as expressions. If we look again at the column id from df DataFrame, Spark will bring a expression as a result, and not the values hold by this column.\n\ndf.id\n\nColumn<'id'>\n\n\nHaving these ideas in mind, when I created the column ID on the previous section, I created a “column expression”. This means that col(\"ID\") is just an expression, and as consequence, Spark does not know which are the values of column ID, or, where it lives (which is the DataFrame that this column belongs?). For now, Spark is not interested in this information, it just knows that we have an expression referring to a column called ID.\nThese ideas relates a lot to the lazy aspect of Spark that we talked about in Section 3.6. Spark will not perform any heavy calculation, or show you the actual results/values from you code, until you trigger the calculations with an action (we will talk more about these “actions” on Section 5.3). As a result, when you access a column, Spark will only deliver a expression that represents that column, and not the actual values of that column.\nThis is handy, because we can store our expressions in variables, and, use it latter, in multiple parts of our code. For example, I can keep building and merging a column with different kinds of operators, to build a more complex expression. In the example below, I create a expression that doubles the values of ID column:\n\nexpr1 = id_column * 2\nprint(expr1)\n\nColumn<'(ID * 2)'>\n\n\nRemember, with this expression, Spark knows that we want to get a column called ID somewhere, and double its values. But Spark will not perform that action right now.\nLogical expressions follow the same logic. In the example below, I am looking for rows where the value in column Name is equal to 'Anne', and, the value in column Grade is above 6. Again, Spark just checks if this is a valid logical expression, he does not evaluate it until we ask for it with an action:\n\nexpr2 = (col('Name') == 'Anne') & (col('Grade') > 6)\nprint(expr2)\n\nColumn<'((Name = Anne) AND (Grade > 6))'>"
  },
  {
    "objectID": "Chapters/04-columns.html#some-key-column-methods",
    "href": "Chapters/04-columns.html#some-key-column-methods",
    "title": "4  Introducing the Column class",
    "section": "4.3 Some key column methods",
    "text": "4.3 Some key column methods\nBecause many transformations that we want to apply over our DataFrames are expressed as column transformations, the methods from the Column class will be quite useful on many different contexts. You will see many of these methods across the next chapters, like desc(), alias() and cast().\nRemember, you can always use the dir() function to see the complete list of methods available in any python class. Is always useful to check the official documentation too1. There you will have a more complete description of each method.\nBut since they are so important in Spark, lets just give you a brief overview of some of the most popular methods from the Column class (these methods will be described in more detail in later chapters):\n\ndesc() and asc(): methods to order the values of the column in a descending or ascending order (respectively);\ncast() and astype(): methods to cast (or convert) the values of the column to a specific data type;\nalias(): method to rename a column;\nsubstr(): method that returns a new column with the sub string of each value;\nisNull() and isNotNull(): logical methods to test if each value in the column is a null value or not;\nstartswith() and endswith(): logical methods to search for values that starts with or ends with a specific pattern;\nlike() and rlike(): logical methods to search for a specific pattern or regular expression in the values of the column;\nisin(): logical method to test if each value in the column is some of the listed values;"
  },
  {
    "objectID": "Chapters/05-transforming.html#introduction",
    "href": "Chapters/05-transforming.html#introduction",
    "title": "5  Transforming your Spark DataFrame",
    "section": "5.1 Introduction",
    "text": "5.1 Introduction\nVirtually every data analysis or data pipeline will include some ETL (Extract, Transform, Load) process, and the T is an essential part of it. Because, you almost never have an input data, or a initial DataFrame that perfectly fits your needs.\nThis means that you always have to transform the initial data that you have, to a specific format that you can use in your analysis. In this chapter, you will learn how to apply some of these basic transformations to your Spark DataFrame."
  },
  {
    "objectID": "Chapters/05-transforming.html#sec-df-defining-transformations",
    "href": "Chapters/05-transforming.html#sec-df-defining-transformations",
    "title": "5  Transforming your Spark DataFrame",
    "section": "5.2 Defining transformations",
    "text": "5.2 Defining transformations\nSpark DataFrames are immutable, meaning that, they cannot be directly changed. But you can use an existing DataFrame to create a new one, based on a set of transformations. In other words, you define a new DataFrame as a transformed version of an older DataFrame.\nBasically every pyspark program that you write will have such transformations. Spark support many types of transformations, however, in this chapter, we will focus on four basic transformations that you can apply to a DataFrame:\n\nFiltering rows;\nSorting rows;\nAdding or deleting columns;\nCalculating aggregates;\n\nTherefore, when you apply one of the above transformations to an existing DataFrame, you will get a new DataFrame as a result. You usually combine multiple transformations together to get your desired result. As a first example, lets get back to the df DataFrame:\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\nfrom datetime import date\nfrom pyspark.sql import Row\n\ndata = [\n  Row(id = 1, value = 28.3, date = date(2021,1,1)),\n  Row(id = 2, value = 15.8, date = date(2021,1,1)),\n  Row(id = 3, value = 20.1, date = date(2021,1,2)),\n  Row(id = 4, value = 12.6, date = date(2021,1,3))\n]\n\ndf = spark.createDataFrame(data)\n\nIn the example below, to create a new DataFrame called big_values, we begin with the df DataFrame, then, we filter its rows where value is greater than 15, then, we select date and value columns, then, we sort the rows based on the value column. So, this set of sequential transformations (filter it, then, select it, then, order it, …) defines what this new big_values DataFrame is.\n\nfrom pyspark.sql.functions import col\n# You define a chain of transformations to\n# create a new DataFrame\nbig_values = df\\\n  .filter(col('value') > 15)\\\n  .select('date', 'value')\\\n  .orderBy('value')\n\nThus, to apply a transformation to an existing DataFrame, we use DataFrame methods such as select(), filter(), orderBy() and many others. Remember, these are methods from the python class that defines Spark DataFrame’s (i.e. the pyspark.sql.dataframe.DataFrame class).\nThis means that you can apply these transformations only to Spark DataFrames, and no other kind of python object. For example, if you try to use the orderBy() method in a standard python string (i.e. an object of class str), you will get an AttributeError error. Because this class of object in python, does not have a orderBy() method:\n\ns = \"A python string\"\ns.orderBy('value')\n\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\nAttributeError: 'str' object has no attribute 'orderBy'\nEach one of these DataFrame methods create a lazily evaluated transformation. Once again, we see the lazy aspect of Spark doing its work here. All these transformation methods are lazily evaluated, meaning that, Spark will only check if they make sense with the initial DataFrame that you have. Spark will not actually perform these transformations on your initial DataFrame, not untill you trigger these transformations with an action."
  },
  {
    "objectID": "Chapters/05-transforming.html#sec-dataframe-actions",
    "href": "Chapters/05-transforming.html#sec-dataframe-actions",
    "title": "5  Transforming your Spark DataFrame",
    "section": "5.3 Triggering calculations with actions",
    "text": "5.3 Triggering calculations with actions\nTherefore, Spark will avoid performing any heavy calculation until such calculation is really needed. But how or when Spark will face this decision? When it encounters an action. An action is the tool you have to trigger Spark to actually perform the transformations you have defined.\n\nAn action instructs Spark to compute the result from a series of transformations. (Chambers and Zaharia 2018).\n\nThere are four kinds of actions in Spark:\n\nShowing an output in the console;\nWriting data to some file or data source;\nCollecting data from a Spark DataFrame to native objects in python (or Java, Scala, R, etc.);\nCounting the number of rows in a Spark DataFrame;\n\nYou already know the first type of action, because we used it before with the show() method. This show() method is an action by itself, because you are asking Spark to show some output to you. So we can make Spark to actually calculate the transformations that defines the big_values DataFrame, by asking Spark to show this DataFrame to us.\n\nbig_values.show()\n\n[Stage 0:>                                                        (0 + 12) / 12]\n\n\n+----------+-----+\n|      date|value|\n+----------+-----+\n|2021-01-01| 15.8|\n|2021-01-02| 20.1|\n|2021-01-01| 28.3|\n+----------+-----+\n\n\n\n                                                                                \n\n\nAnother very useful action is the count() method, that gives you the number of rows in a DataFrame. To be able to count the number of rows in a DataFrame, Spark needs to access this DataFrame in the first place. That is why this count() method behaves as an action. Spark will perform the transformations that defines big_values to access the actual rows of this DataFrame and count them.\n\nbig_values.count()\n\n3\n\n\nFurthermore, sometimes, you want to collect the data of a Spark DataFrame to use it inside python. In other words, sometimes you need to do some work that Spark cannot do by itself. To do so, you collect part of the data that is being generated by Spark, and store it inside a normal python object to use it in a standard python program.\nThat is what the collect() method do. It transfers all the data of your Spark DataFrame into a standard python list that you can easily access with python. More specifically, you get a python list full of Row() values:\n\ndata = big_values.collect()\nprint(data)\n\n[Row(date=datetime.date(2021, 1, 1), value=15.8), Row(date=datetime.date(2021, 1, 2), value=20.1), Row(date=datetime.date(2021, 1, 1), value=28.3)]\n\n\nThe take() method is very similar to collect(). But you usually apply take() when you need to collect just a small section of your DataFrame (and not the entire thing), like the first n rows.\n\nn = 1\nfirst_row = big_values.take(n)\nprint(first_row)\n\n[Row(date=datetime.date(2021, 1, 1), value=15.8)]\n\n\nThe last action would be the write() method, but we will explain this method latter, in the Chapter 6."
  },
  {
    "objectID": "Chapters/05-transforming.html#understanding-narrow-and-wide-transformations",
    "href": "Chapters/05-transforming.html#understanding-narrow-and-wide-transformations",
    "title": "5  Transforming your Spark DataFrame",
    "section": "5.4 Understanding narrow and wide transformations",
    "text": "5.4 Understanding narrow and wide transformations\nThere are two kinds of transformations in Spark: narrow and wide transformations. Remember, a Spark DataFrame is divided into many small parts (called partitions), and, these parts are spread across the cluster. The basic difference between narrow and wide transformations, is if the transformation forces Spark to read data from multiple partitions to generate a single part of the result of that transformation, or not.\nMore technically, narrow transformations are simply transformations where 1 input data (or 1 partition of the input DataFrame) contributes to only 1 partition of the output.\n\n\n\nFigure 5.1: Presenting narrow transformations\n\n\nIn other words, each partition of your input DataFrame will be used (separately) to generate one individual part of the result of your transformation. As another perspective, you can understand narrow transformations as those where Spark does not need to read the entire input DataFrame to generate a single and small piece of your result.\nA classic example of narrow transformation is a filter. For example, suppose you have three students (Anne, Carls and Mike), and that each one has a bag full of blue, orange and red balls mixed. Now, suppose you asked them to collect all the red balls of these bags, and combined them in a single bag.\nTo do this task, Mike does not need to know what balls are inside of the bag of Carls or Anne. He just need to collect the red balls that are solely on his bag. At the end of the task, each student will have a part of the end result (that is, all the red balls that were in his own bag), and they just need to combine all these parts to get the total result.\nThe same thing applies to filters in Spark DataFrames. When you filter all the rows where the column state is equal to \"Alaska\", Spark will filter all the rows in each partition separately, and then, will combine all the outputs to get the final result.\nIn contrast, wide transformations are the opposite of that. In wide transformations, Spark needs to use more than 1 partition of the input DataFrame to generate a small piece of the result.\n\n\n\nFigure 5.2: Presenting wide transformations\n\n\nWhen this kind of transformation happens, each worker node of the cluster needs to share his partition with the others. In other words, what happens is a partition shuffle. Each worker node sends his partition to the others, so they can have access to it, while performing their assigned tasks.\nPartition shuffles are a very popular topic in Apache Spark, because they can be a serious source of inefficiency in your Spark application (Chambers and Zaharia 2018). In more details, when these shuffles happens, Spark needs to write data back to the hard disk of the computer, and this is not a very fast operation. It does not mean that wide transformations are bad or slow, just that the shuffles they are producing can be a problem.\nA classic example of wide operation is a grouped aggregation. For example, lets suppose we had a DataFrame with the daily sales of multiple stores spread across the country, and, we wanted to calculate the total sales per city/region. To calculate the total sales of a specific city, like São Paulo, Spark would need to find all the rows that corresponds to this city, before adding the values, and these rows can be spread across multiple partitions of the cluster."
  },
  {
    "objectID": "Chapters/05-transforming.html#filtering-rows-of-your-dataframe",
    "href": "Chapters/05-transforming.html#filtering-rows-of-your-dataframe",
    "title": "5  Transforming your Spark DataFrame",
    "section": "5.5 Filtering rows of your DataFrame",
    "text": "5.5 Filtering rows of your DataFrame\nTo filter specific rows of a DataFrame, pyspark offers two equivalent DataFrame methods called where() and filter(). In other words, they both do the same thing, and work in the same way. These methods receives a logical expression that translates what you want to filter as input.\nTo demonstrate some of the next examples in this chapter, we will use a different DataFrame called transf. With the code below, you can import the data from transf.csv to recreate this DataFrame in your Spark Session. Remember that, this CSV file is freely available to download at the repository of this book1.\n\nfrom pyspark.sql.types import StructType, StructField\nfrom pyspark.sql.types import DoubleType, StringType\nfrom pyspark.sql.types import LongType, TimestampType, DateType\npath = \"../Data/transf.csv\"\nschema = StructType([\n  StructField('dateTransfer', DateType(), False),\n  StructField('datetimeTransfer', TimestampType(), False),\n  StructField('clientNumber', LongType(), False),\n  StructField('transferValue', DoubleType(), False),\n  StructField('transferCurrency', StringType(), False),\n  StructField('transferID', LongType(), False),\n  StructField('transferLog', StringType(), False),\n  StructField('destinationBankNumber', LongType(), False),\n  StructField('destinationBankBranch', LongType(), False),\n  StructField('destinationBankAccount', StringType(), False)\n])\n\ntransf = spark.read\\\n  .csv(path, schema = schema, sep = \";\", header = True)\n\nThis transf DataFrame contains bank transfer records from a fictitious bank. Before I show you the actual data of this DataFrame, is useful to give you a quick description of each column that it contains:\n\ndateTransfer: the date when the transfer occurred;\ndatetimeTransfer: the date and time when the transfer occurred;\nclientNumber the unique number that identifies a client of the bank;\ntransferValue: the nominal value that was transferred;\ntransferCurrency: the currency of the nominal value transferred;\ntransferID: an unique ID for the transfer;\ntransferLog: store any error message that may have appeared during the execution of the transfer;\ndestinationBankNumber: the transfer destination bank number;\ndestinationBankBranch: the transfer destination branch number;\ndestinationBankAccount: the transfer destination account number;\n\nNow, to see the actual data of this DataFrame, we can use the show() action as usual.\n\ntransf.show(5)\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|  2022-12-31|2022-12-31 14:00:24|        5516|      7794.31|          zing ƒ|  20223563|       null|                   33|                 4078|               72424-2|\n|  2022-12-31|2022-12-31 10:32:07|        4965|       7919.0|          zing ƒ|  20223562|       null|                  421|                 1979|               36441-5|\n|  2022-12-31|2022-12-31 07:37:02|        4608|       5603.0|        dollar $|  20223561|       null|                  666|                 4425|               41323-1|\n|  2022-12-31|2022-12-31 07:35:05|        1121|      4365.22|        dollar $|  20223560|       null|                  666|                 2400|               74120-4|\n|  2022-12-31|2022-12-31 02:53:44|        1121|       4620.0|        dollar $|  20223559|       null|                  421|                 1100|               39830-0|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\nonly showing top 5 rows\n\n\n\nAs a first example, lets suppose you wanted to inspect all the rows where transferValue is less than 1000. To do so, you can use the following code:\n\ntransf\\\n  .filter(\"transferValue < 1000\")\\\n  .show(5)\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|  2022-12-18|2022-12-18 08:45:30|        1297|       142.66|        dollar $|  20223467|       null|                  421|                 5420|               43088-1|\n|  2022-12-13|2022-12-13 20:44:23|        5516|       992.15|        dollar $|  20223442|       null|                   33|                 5420|               41609-8|\n|  2022-11-24|2022-11-24 20:01:39|        1945|       174.64|        dollar $|  20223319|       null|                  421|                 2400|               34025-8|\n|  2022-11-07|2022-11-07 16:35:57|        4862|       570.69|        dollar $|  20223212|       null|                  290|                 5420|               51165-3|\n|  2022-11-04|2022-11-04 20:00:34|        1297|        854.0|        dollar $|  20223194|       null|                  421|                 4078|               43478-6|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\nonly showing top 5 rows\n\n\n\nWriting simple SQL logical expression inside a string is the most easy and “clean” way to create a filter expression in pyspark. However, you could write the same exact expression in a more “pythonic” way, using the col() function from pyspark.sql.functions.\n\nfrom pyspark.sql.functions import col\n\ntransf\\\n  .filter(col(\"transferValue\") < 1000)\\\n  .show(5)\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|  2022-12-18|2022-12-18 08:45:30|        1297|       142.66|        dollar $|  20223467|       null|                  421|                 5420|               43088-1|\n|  2022-12-13|2022-12-13 20:44:23|        5516|       992.15|        dollar $|  20223442|       null|                   33|                 5420|               41609-8|\n|  2022-11-24|2022-11-24 20:01:39|        1945|       174.64|        dollar $|  20223319|       null|                  421|                 2400|               34025-8|\n|  2022-11-07|2022-11-07 16:35:57|        4862|       570.69|        dollar $|  20223212|       null|                  290|                 5420|               51165-3|\n|  2022-11-04|2022-11-04 20:00:34|        1297|        854.0|        dollar $|  20223194|       null|                  421|                 4078|               43478-6|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\nonly showing top 5 rows\n\n\n\nYou still have a more verbose alternative, that does not require the col() function. With this method, you refer to the specific column using the dot operator (.), like in the example below:\n\n# This will give you the exact\n# same result of the examples above\ntransf\\\n  .filter(transf.transferValue < 1000)\n\n\n5.5.1 Logical operators available\nAs we saw in the previous section, there are two ways to write logical expressions in pyspark: write SQL logical expressions inside a string; or, write python logical expressions using the col() function.\nIf you choose to write the SQL logical expressions in a string, you need to use the logical operators of SQL in your expression (not the logical operators of python). In the other hand, if you choose to write in the “python” way, then, you need to use the logical operators of python.\nThe logical operators of SQL are described in the table below:\n\n\nTable 5.1: List of logical operators of SQL\n\n\n\n\n\n\n\nOperator\nExample of expression\nMeaning of the expression\n\n\n\n\n<\nx < y\nis x less than y?\n\n\n>\nx > y\nis x greater than y?\n\n\n<=\nx <= y\nis x less than or equal to y?\n\n\n>=\nx >= y\nis x greater than or equal to y?\n\n\n==\nx == y\nis x equal to y?\n\n\n!=\nx != y\nis x not equal to y?\n\n\nin\nx in y\nis x one of the values listed in y?\n\n\nand\nx and y\nboth logical expressions x and y are true?\n\n\nor\nx or y\nat least one of logical expressions x and y are true?\n\n\nnot\nnot x\nis the logical expression x not true?\n\n\n\n\nAnd, the logical operators of python are described in the table below:\n\n\nTable 5.2: List of logical operators of python\n\n\n\n\n\n\n\nOperator\nExample of expression\nMeaning of the expression\n\n\n\n\n<\nx < y\nis x less than y?\n\n\n>\nx > y\nis x greater than y?\n\n\n<=\nx <= y\nis x less than or equal to y?\n\n\n>=\nx >= y\nis x greater than or equal to y?\n\n\n==\nx == y\nis x equal to y?\n\n\n!=\nx != y\nis x not equal to y?\n\n\n&\nx & y\nboth logical expressions x and y are true?\n\n\n|\nx | y\nat least one of logical expressions x and y are true?\n\n\n~\n~x\nis the logical expression x not true?\n\n\n\n\n\n\n5.5.2 Connecting multiple logical expressions\nSometimes, you need to write more complex logical expressions to correctly describe the rows you are interested in. That is, when you combine multiple logical expressions together.\nAs an example, lets suppose you wanted all the rows in transf DataFrame from client of number 1297 where the transfer value is smaller than 1000, and the date of the transfer is after 20 of February 2022. These conditions are dependent, that is, they are connected to each other (the client number, the transfer value and the date of the transfer). That is why I used the and keyword between each condition in the example below (i.e. to connect these three conditions together).\n\ncondition = '''\n  transferValue < 1000\n  and clientNumber == 1297 \n  and dateTransfer > '2022-02-20'\n'''\n\ntransf\\\n  .filter(condition)\\\n  .show()\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|  2022-12-18|2022-12-18 08:45:30|        1297|       142.66|        dollar $|  20223467|       null|                  421|                 5420|               43088-1|\n|  2022-11-04|2022-11-04 20:00:34|        1297|        854.0|        dollar $|  20223194|       null|                  421|                 4078|               43478-6|\n|  2022-02-27|2022-02-27 13:27:44|        1297|       697.21|        dollar $|  20221505|       null|                  421|                 1100|               60414-7|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n\n\n\nI could translate this logical expression into the “pythonic” way (using the col() function). However, I would have to surround each individual expression by parentheses, and, use the & operator to substitute the and keyword.\n\ntransf\\\n  .filter(\n    (col('transferValue') < 1000) &\n    (col('clientNumber') == 1297) &\n    (col('dateTransfer') > '2022-02-20')\n  )\\\n  .show()\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|  2022-12-18|2022-12-18 08:45:30|        1297|       142.66|        dollar $|  20223467|       null|                  421|                 5420|               43088-1|\n|  2022-11-04|2022-11-04 20:00:34|        1297|        854.0|        dollar $|  20223194|       null|                  421|                 4078|               43478-6|\n|  2022-02-27|2022-02-27 13:27:44|        1297|       697.21|        dollar $|  20221505|       null|                  421|                 1100|               60414-7|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n\n\n\nThis a very important detail, because is very easy to forget it. When building your complex logical expressions, if you are using the col() function, always remember to surround each expression by a pair of parentheses. Otherwise, you will get a very confusing and useless error message, like this:\n\ntransf\\\n  .filter(\n    col('transferValue') < 1000 &\n    col('clientNumber') == 1297 &\n    col('dateTransfer') > '2022-02-20'\n  )\\\n  .show(5)\n\nPy4JError: An error occurred while calling o216.and. Trace:\npy4j.Py4JException: Method and([class java.lang.Integer]) does not exist\n    at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:318)\n    at py4j.reflection.ReflectionEngine.getMethod(ReflectionEngine.java:326)\n    at py4j.Gateway.invoke(Gateway.java:274)\n    at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n    at py4j.commands.CallCommand.execute(CallCommand.java:79)\n    at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n    at py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n    at java.base/java.lang.Thread.run(Thread.java:829)\nIn the above examples, we have logical expressions that are dependent on each other. But, lets suppose these conditions were independent. In this case, we would use the or keyword, instead of and. Now, Spark will look for every row of transf where transferValue is smaller than 1000, or, clientNumber is equal to 1297, or, dateTransfer is greater than 20 of February 2022.\n\ncondition = '''\n  transferValue < 1000\n  or clientNumber == 1297 \n  or dateTransfer > '2022-02-20'\n'''\n\ntransf\\\n  .filter(condition)\\\n  .show(5)\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|  2022-12-31|2022-12-31 14:00:24|        5516|      7794.31|          zing ƒ|  20223563|       null|                   33|                 4078|               72424-2|\n|  2022-12-31|2022-12-31 10:32:07|        4965|       7919.0|          zing ƒ|  20223562|       null|                  421|                 1979|               36441-5|\n|  2022-12-31|2022-12-31 07:37:02|        4608|       5603.0|        dollar $|  20223561|       null|                  666|                 4425|               41323-1|\n|  2022-12-31|2022-12-31 07:35:05|        1121|      4365.22|        dollar $|  20223560|       null|                  666|                 2400|               74120-4|\n|  2022-12-31|2022-12-31 02:53:44|        1121|       4620.0|        dollar $|  20223559|       null|                  421|                 1100|               39830-0|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\nonly showing top 5 rows\n\n\n\nTo translate this expression into the pythonic way, we have to substitute the or keyword by the | operator, and surround each expression by parentheses again:\n\ntransf\\\n  .filter(\n    (col('transferValue') < 1000) |\n    (col('clientNumber') == 1297) |\n    (col('dateTransfer') > '2022-02-20')\n  )\\\n  .show(5)\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|  2022-12-31|2022-12-31 14:00:24|        5516|      7794.31|          zing ƒ|  20223563|       null|                   33|                 4078|               72424-2|\n|  2022-12-31|2022-12-31 10:32:07|        4965|       7919.0|          zing ƒ|  20223562|       null|                  421|                 1979|               36441-5|\n|  2022-12-31|2022-12-31 07:37:02|        4608|       5603.0|        dollar $|  20223561|       null|                  666|                 4425|               41323-1|\n|  2022-12-31|2022-12-31 07:35:05|        1121|      4365.22|        dollar $|  20223560|       null|                  666|                 2400|               74120-4|\n|  2022-12-31|2022-12-31 02:53:44|        1121|       4620.0|        dollar $|  20223559|       null|                  421|                 1100|               39830-0|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\nonly showing top 5 rows\n\n\n\nYou can increase the complexity of your logical expressions by mixing dependent expressions with independent expressions. For example, to filter all the rows where dateTransfer is greater than or equal to 01 of October 2022, and clientNumber is either 2727 or 5188, you would have the following code:\n\ncondition = '''\n  (clientNumber == 2727 or clientNumber == 5188)\n  and dateTransfer >= '2022-10-01'\n'''\n\ntransf\\\n  .filter(condition)\\\n  .show(5)\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|  2022-12-29|2022-12-29 10:22:02|        2727|      4666.25|          euro €|  20223541|       null|                   33|                 5420|               83070-8|\n|  2022-12-27|2022-12-27 03:58:25|        5188|      7821.69|        dollar $|  20223522|       null|                   33|                 4078|               46571-3|\n|  2022-12-26|2022-12-25 23:45:02|        2727|      3261.73| british pound £|  20223515|       null|                  421|                 6317|               66040-9|\n|  2022-12-23|2022-12-23 05:32:49|        2727|       8042.0|        dollar $|  20223496|       null|                  290|                 5420|               37759-7|\n|  2022-12-22|2022-12-22 06:02:47|        5188|      8175.67|        dollar $|  20223490|       null|                  666|                 8800|               42657-8|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\nonly showing top 5 rows\n\n\n\nIf you investigate the above condition carefully, maybe, you will identify that this condition could be rewritten in a simpler format, by using the in keyword. This way, Spark will look for all the rows where clientNumber is equal to one of the listed values (2727 or 5188), and, that dateTransfer is greater than or equal to 01 of October 2022.\n\ncondition = '''\n  clientNumber in (2727, 5188)\n  and dateTransfer >= '2022-10-01'\n'''\n\ntransf\\\n  .filter(condition)\\\n  .show(5)\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|  2022-12-29|2022-12-29 10:22:02|        2727|      4666.25|          euro €|  20223541|       null|                   33|                 5420|               83070-8|\n|  2022-12-27|2022-12-27 03:58:25|        5188|      7821.69|        dollar $|  20223522|       null|                   33|                 4078|               46571-3|\n|  2022-12-26|2022-12-25 23:45:02|        2727|      3261.73| british pound £|  20223515|       null|                  421|                 6317|               66040-9|\n|  2022-12-23|2022-12-23 05:32:49|        2727|       8042.0|        dollar $|  20223496|       null|                  290|                 5420|               37759-7|\n|  2022-12-22|2022-12-22 06:02:47|        5188|      8175.67|        dollar $|  20223490|       null|                  666|                 8800|               42657-8|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\nonly showing top 5 rows\n\n\n\n\n\n5.5.3 Translating the in keyword to the pythonic way\nPython does have a in keyword just like SQL, but, this keyword does not work as expected in pyspark. To write a logical expression, using the pythonic way, that filters the rows where a column is equal to one of the listed values, you can use the isin() method.\nThis method belongs to the Column class, so, you should always use isin() after a column name or a col() function. In the example below, we are filtering the rows where destinationBankNumber is 290 or 666:\n\ntransf\\\n  .filter(col('destinationBankNumber').isin(290, 666))\\\n  .show(5)\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|  2022-12-31|2022-12-31 07:37:02|        4608|       5603.0|        dollar $|  20223561|       null|                  666|                 4425|               41323-1|\n|  2022-12-31|2022-12-31 07:35:05|        1121|      4365.22|        dollar $|  20223560|       null|                  666|                 2400|               74120-4|\n|  2022-12-31|2022-12-31 02:44:46|        1121|       7158.0|          zing ƒ|  20223558|       null|                  290|                 1100|               35424-4|\n|  2022-12-31|2022-12-31 01:02:06|        4862|       6714.0|        dollar $|  20223557|       null|                  666|                 1002|               71839-1|\n|  2022-12-31|2022-12-31 00:48:47|        3294|     10882.52|        dollar $|  20223556|       null|                  666|                 2231|               50190-5|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\nonly showing top 5 rows\n\n\n\n\n\n5.5.4 Negating logical conditions\n\nIn some cases, is easier to describe what rows you do not want in your filter. In this case, you want to negate (or invert) your logical expression. For this, SQL provides the not keyword, that you place before the logical expression you want to negate.\nFor example, we can filter all the rows of transf where clientNumber is not equal to 3284. Remember, the methods filter() and where() are equivalents or synonymous (they both mean the same thing).\n\ncondition = '''\n  not clientNumber == 3284\n'''\n\ntransf\\\n  .where(condition)\\\n  .show(5)\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|  2022-12-31|2022-12-31 14:00:24|        5516|      7794.31|          zing ƒ|  20223563|       null|                   33|                 4078|               72424-2|\n|  2022-12-31|2022-12-31 10:32:07|        4965|       7919.0|          zing ƒ|  20223562|       null|                  421|                 1979|               36441-5|\n|  2022-12-31|2022-12-31 07:37:02|        4608|       5603.0|        dollar $|  20223561|       null|                  666|                 4425|               41323-1|\n|  2022-12-31|2022-12-31 07:35:05|        1121|      4365.22|        dollar $|  20223560|       null|                  666|                 2400|               74120-4|\n|  2022-12-31|2022-12-31 02:53:44|        1121|       4620.0|        dollar $|  20223559|       null|                  421|                 1100|               39830-0|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\nonly showing top 5 rows\n\n\n\nTo translate this expression to the pythonic way, we use the ~ operator. However, because we are negating the logical expression as a whole, is important to surround the entire expression with parentheses.\n\ntransf\\\n  .where(~(col('clientNumber') == 3284))\\\n  .show(5)\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|  2022-12-31|2022-12-31 14:00:24|        5516|      7794.31|          zing ƒ|  20223563|       null|                   33|                 4078|               72424-2|\n|  2022-12-31|2022-12-31 10:32:07|        4965|       7919.0|          zing ƒ|  20223562|       null|                  421|                 1979|               36441-5|\n|  2022-12-31|2022-12-31 07:37:02|        4608|       5603.0|        dollar $|  20223561|       null|                  666|                 4425|               41323-1|\n|  2022-12-31|2022-12-31 07:35:05|        1121|      4365.22|        dollar $|  20223560|       null|                  666|                 2400|               74120-4|\n|  2022-12-31|2022-12-31 02:53:44|        1121|       4620.0|        dollar $|  20223559|       null|                  421|                 1100|               39830-0|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\nonly showing top 5 rows\n\n\n\nIf you forget to add the parentheses, Spark will think you are negating just the column (e.g. ~col('clientNumber')), and not the entire expression. That would not make sense, and, as a result, Spark would throw an error:\n\ntransf\\\n  .where(~col('clientNumber') == 3284)\\\n  .show(5)\n\nAnalysisException: cannot resolve '(NOT clientNumber)' due to data type mismatch: argument 1 requires boolean type, however, 'clientNumber' is of bigint type.;\n'Filter (NOT clientNumber#210L = 3284)\nBecause the ~ operator is a little discrete and can go unnoticed, I sometimes use a different approach to negate my logical expressions. I make the entire expression equal to False. This way, I get all the rows where that particular expression is False. This makes my intention more visible in the code, but, is harder to write it.\n\n# Filter all the rows where `user` is not equal to\n# Ana or Eduardo.\ntransf\\\n  .where( (col('clientNumber').isin(2727, 5188)) == False )\\\n  .show(5)\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|  2022-12-31|2022-12-31 14:00:24|        5516|      7794.31|          zing ƒ|  20223563|       null|                   33|                 4078|               72424-2|\n|  2022-12-31|2022-12-31 10:32:07|        4965|       7919.0|          zing ƒ|  20223562|       null|                  421|                 1979|               36441-5|\n|  2022-12-31|2022-12-31 07:37:02|        4608|       5603.0|        dollar $|  20223561|       null|                  666|                 4425|               41323-1|\n|  2022-12-31|2022-12-31 07:35:05|        1121|      4365.22|        dollar $|  20223560|       null|                  666|                 2400|               74120-4|\n|  2022-12-31|2022-12-31 02:53:44|        1121|       4620.0|        dollar $|  20223559|       null|                  421|                 1100|               39830-0|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\nonly showing top 5 rows\n\n\n\n\n\n5.5.5 Filtering null values\nSometimes, the null values play an important role in your filter. You either want to collect all these null values, so you can investigate why they are null in the first place, or, you want to completely eliminate them from your DataFrame.\nBecause this is a special kind of value in Spark, with a special meaning (the “absence” of a value), you need to use a special syntax to correctly filter these values in your DataFrame. In SQL, you can use the is keyword to filter these values:\n\ntransf\\\n  .where('transferLog is null')\\\n  .show(5)\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|  2022-12-31|2022-12-31 14:00:24|        5516|      7794.31|          zing ƒ|  20223563|       null|                   33|                 4078|               72424-2|\n|  2022-12-31|2022-12-31 10:32:07|        4965|       7919.0|          zing ƒ|  20223562|       null|                  421|                 1979|               36441-5|\n|  2022-12-31|2022-12-31 07:37:02|        4608|       5603.0|        dollar $|  20223561|       null|                  666|                 4425|               41323-1|\n|  2022-12-31|2022-12-31 07:35:05|        1121|      4365.22|        dollar $|  20223560|       null|                  666|                 2400|               74120-4|\n|  2022-12-31|2022-12-31 02:53:44|        1121|       4620.0|        dollar $|  20223559|       null|                  421|                 1100|               39830-0|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\nonly showing top 5 rows\n\n\n\nHowever, if you want to remove these values from your DataFrame, then, you can just negate (or invert) the above expression with the not keyword, like this:\n\ntransf\\\n  .where('not transferLog is null')\\\n  .show(5)\n\n+------------+-------------------+------------+-------------+----------------+----------+--------------------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|         transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+--------------------+---------------------+---------------------+----------------------+\n|  2022-12-05|2022-12-05 00:51:00|        2197|      8240.62|          zing ƒ|  20223383| 408 Request Timeout|                  666|                 1100|               58503-9|\n|  2022-09-20|2022-09-19 21:59:51|        5188|       7583.9|        dollar $|  20222912|500 Server Unavai...|                  290|                 1979|               85242-1|\n|  2022-09-03|2022-09-03 06:07:59|        3795|       3654.0|          zing ƒ|  20222814| 408 Request Timeout|                  290|                 9921|               60494-5|\n|  2022-07-02|2022-07-02 15:29:50|        4465|       5294.0|        dollar $|  20222408|500 Server Unavai...|                  421|                 2400|               39070-3|\n|  2022-06-14|2022-06-14 10:21:55|        1121|       7302.0|        dollar $|  20222273| 408 Request Timeout|                  666|                 5420|               47709-2|\n+------------+-------------------+------------+-------------+----------------+----------+--------------------+---------------------+---------------------+----------------------+\nonly showing top 5 rows\n\n\n\nThe is and not keywords in SQL have a special relation. Because you can create the same negation/inversion of the expression by inserting the not keyword in the middle of the expression (you can do this too in expressions with the in keyword). In other words, you might see, in someone else’s code, the same expression above written in this form:\n\ntransf\\\n  .where('transferLog is not null')\\\n  .show(5)\n\nBoth forms are equivalent and valid SQL logical expressions. But the latter is a strange version. Because we cannot use the not keyword in this manner on other kinds of logical expressions. Normally, we put the not keyword before the logical expression we want to negate, not in the middle of it. Anyway, just have in mind that this form of logical expression exists, and, that is a perfectly valid one.\nWhen we translate the above examples to the “pythonic” way, many people tend to use the null equivalent of python, that is, the None value, in the expression. But as you can see in the result below, this method does not work as expected:\n\ntransf\\\n  .where(col('transferLog') == None)\\\n  .show(5)\n\n+------------+----------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+----------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n+------------+----------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n\n\n\nThe correct way to do this in pyspark, is to use the isNull() method from the Column class.\n\ntransf\\\n  .where(col('transferLog').isNull())\\\n  .show(5)\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|  2022-12-31|2022-12-31 14:00:24|        5516|      7794.31|          zing ƒ|  20223563|       null|                   33|                 4078|               72424-2|\n|  2022-12-31|2022-12-31 10:32:07|        4965|       7919.0|          zing ƒ|  20223562|       null|                  421|                 1979|               36441-5|\n|  2022-12-31|2022-12-31 07:37:02|        4608|       5603.0|        dollar $|  20223561|       null|                  666|                 4425|               41323-1|\n|  2022-12-31|2022-12-31 07:35:05|        1121|      4365.22|        dollar $|  20223560|       null|                  666|                 2400|               74120-4|\n|  2022-12-31|2022-12-31 02:53:44|        1121|       4620.0|        dollar $|  20223559|       null|                  421|                 1100|               39830-0|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\nonly showing top 5 rows\n\n\n\nIf you want to eliminate the null values, just use the inverse method isNotNull().\n\ntransf\\\n  .where(col('transferLog').isNotNull())\\\n  .show(5)\n\n+------------+-------------------+------------+-------------+----------------+----------+--------------------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|         transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+--------------------+---------------------+---------------------+----------------------+\n|  2022-12-05|2022-12-05 00:51:00|        2197|      8240.62|          zing ƒ|  20223383| 408 Request Timeout|                  666|                 1100|               58503-9|\n|  2022-09-20|2022-09-19 21:59:51|        5188|       7583.9|        dollar $|  20222912|500 Server Unavai...|                  290|                 1979|               85242-1|\n|  2022-09-03|2022-09-03 06:07:59|        3795|       3654.0|          zing ƒ|  20222814| 408 Request Timeout|                  290|                 9921|               60494-5|\n|  2022-07-02|2022-07-02 15:29:50|        4465|       5294.0|        dollar $|  20222408|500 Server Unavai...|                  421|                 2400|               39070-3|\n|  2022-06-14|2022-06-14 10:21:55|        1121|       7302.0|        dollar $|  20222273| 408 Request Timeout|                  666|                 5420|               47709-2|\n+------------+-------------------+------------+-------------+----------------+----------+--------------------+---------------------+---------------------+----------------------+\nonly showing top 5 rows"
  },
  {
    "objectID": "Chapters/05-transforming.html#selecting-specific-columns-of-your-dataframe",
    "href": "Chapters/05-transforming.html#selecting-specific-columns-of-your-dataframe",
    "title": "5  Transforming your Spark DataFrame",
    "section": "5.6 Selecting specific columns of your DataFrame",
    "text": "5.6 Selecting specific columns of your DataFrame\nSometimes, you need manage or transform the columns you have. For example, you might need to change the order of these columns, or, to delete/rename some of them. To do this, you can use the select() and drop() methods of your DataFrame.\nThe select() method works very similarly to the SELECT statement of SQL. You basically list all the columns you want to keep in your DataFrame, in the specific order you want.\n\ntransf\\\n  .select(\n    'transferID', 'datetimeTransfer',\n    'clientNumber', 'transferValue'\n  )\\\n  .show(5)\n\n+----------+-------------------+------------+-------------+\n|transferID|   datetimeTransfer|clientNumber|transferValue|\n+----------+-------------------+------------+-------------+\n|  20223563|2022-12-31 14:00:24|        5516|      7794.31|\n|  20223562|2022-12-31 10:32:07|        4965|       7919.0|\n|  20223561|2022-12-31 07:37:02|        4608|       5603.0|\n|  20223560|2022-12-31 07:35:05|        1121|      4365.22|\n|  20223559|2022-12-31 02:53:44|        1121|       4620.0|\n+----------+-------------------+------------+-------------+\nonly showing top 5 rows\n\n\n\n\n5.6.1 Renaming your columns\nRealize in the example above, that the column names can be delivered directly as strings to select(). This makes life pretty easy, but, it does not give you extra options. You may want to rename some of the columns, and, to do this, you need to use the alias() method from Column class. Since this is a method from Column class, you need to use it after a col() function, or, after a column name.\n\ntransf\\\n  .select(\n    'datetimeTransfer',\n    col('transferID').alias('ID_of_transfer'),\n    transf.clientNumber.alias('clientID')\n  )\\\n  .show(5)\n\n+-------------------+--------------+--------+\n|   datetimeTransfer|ID_of_transfer|clientID|\n+-------------------+--------------+--------+\n|2022-12-31 14:00:24|      20223563|    5516|\n|2022-12-31 10:32:07|      20223562|    4965|\n|2022-12-31 07:37:02|      20223561|    4608|\n|2022-12-31 07:35:05|      20223560|    1121|\n|2022-12-31 02:53:44|      20223559|    1121|\n+-------------------+--------------+--------+\nonly showing top 5 rows\n\n\n\nBy using this alias() method, you can rename multiple columns within a single select() call. However, you can use the withColumnRenamed() method to rename just a single column of your DataFrame. The first argument of this method, is the current name of this column, and, the second argument, is the new name of this column.\n\ntransf\\\n  .withColumnRenamed('clientNumber', 'clientID')\\\n  .show(5)\n\n+------------+-------------------+--------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientID|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+--------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|  2022-12-31|2022-12-31 14:00:24|    5516|      7794.31|          zing ƒ|  20223563|       null|                   33|                 4078|               72424-2|\n|  2022-12-31|2022-12-31 10:32:07|    4965|       7919.0|          zing ƒ|  20223562|       null|                  421|                 1979|               36441-5|\n|  2022-12-31|2022-12-31 07:37:02|    4608|       5603.0|        dollar $|  20223561|       null|                  666|                 4425|               41323-1|\n|  2022-12-31|2022-12-31 07:35:05|    1121|      4365.22|        dollar $|  20223560|       null|                  666|                 2400|               74120-4|\n|  2022-12-31|2022-12-31 02:53:44|    1121|       4620.0|        dollar $|  20223559|       null|                  421|                 1100|               39830-0|\n+------------+-------------------+--------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\nonly showing top 5 rows\n\n\n\n\n\n5.6.2 Dropping unnecessary columns\nIn some cases, your DataFrame just have too many columns and you just want to eliminate a few of them. In a situation like this, you can list the columns you want to drop from your DataFrame, inside the drop() method, like this:\n\ntransf\\\n  .drop('dateTransfer', 'clientNumber')\\\n  .show(5)\n\n+-------------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|   datetimeTransfer|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+-------------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|2022-12-31 14:00:24|      7794.31|          zing ƒ|  20223563|       null|                   33|                 4078|               72424-2|\n|2022-12-31 10:32:07|       7919.0|          zing ƒ|  20223562|       null|                  421|                 1979|               36441-5|\n|2022-12-31 07:37:02|       5603.0|        dollar $|  20223561|       null|                  666|                 4425|               41323-1|\n|2022-12-31 07:35:05|      4365.22|        dollar $|  20223560|       null|                  666|                 2400|               74120-4|\n|2022-12-31 02:53:44|       4620.0|        dollar $|  20223559|       null|                  421|                 1100|               39830-0|\n+-------------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\nonly showing top 5 rows\n\n\n\n\n\n5.6.3 Casting columns to a different data type\nSpark try to do its best when guessing which is correct data type for the columns of your DataFrame. But, obviously, Spark can get it wrong, and, you end up deciding by your own which data type to use for a specific column.\nTo explicit transform a column to a specific data type, you can use cast() or astype() methods inside select(). The astype() method is just an alias to cast(). This cast() method is very similar to the CAST keyword in SQL, and belongs to the Column class, so, you should always use it after a column name, or, a col() function:\n\ntransf\\\n  .select(\n    'transferValue',\n    col('transferValue').cast('long').alias('value_as_integer'),\n    transf.transferValue.cast('boolean').alias('value_as_boolean')\n  )\\\n  .show(5)\n\n+-------------+----------------+----------------+\n|transferValue|value_as_integer|value_as_boolean|\n+-------------+----------------+----------------+\n|      7794.31|            7794|            true|\n|       7919.0|            7919|            true|\n|       5603.0|            5603|            true|\n|      4365.22|            4365|            true|\n|       4620.0|            4620|            true|\n+-------------+----------------+----------------+\nonly showing top 5 rows\n\n\n\nTo use cast() or astype() methods, you give the name of the data type (as a string) to which you want to cast the column. The main available data types to cast() or astype() are:\n\n'string': correspond to StringType();\n'int': correspond to IntegerType();\n'long': correspond to LongType();\n'double': correspond to DoubleType();\n'date': correspond to DateType();\n'timestamp': correspond to TimestampType();\n'boolean': correspond to BooleanType();\n'array': correspond to ArrayType();\n'dict': correspond to MapType();\n\n\n\n5.6.4 You can add new columns with select()\nWhen I said that select() works in the same way as the SELECT statement of SQL, I also meant that you can use select() to select columns that do not currently exist in your DataFrame, and add them to the final result.\nFor example, I can select a new column (called by_1000) containing value divided by 1000, like this:\n\ntransf\\\n  .select(\n    'transferValue',\n    (col('transferValue') / 1000).alias('by_1000')\n  )\\\n  .show(5)\n\n+-------------+-------+\n|transferValue|by_1000|\n+-------------+-------+\n|      7794.31|7.79431|\n|       7919.0|  7.919|\n|       5603.0|  5.603|\n|      4365.22|4.36522|\n|       4620.0|   4.62|\n+-------------+-------+\nonly showing top 5 rows\n\n\n\nThis by_1000 column do not exist in transf DataFrame. It was calculated and added to the final result by select(). The formula col('transferValue') / 1000 is the equation that defines what this by_1000 column is, or, how it should be calculated.\nBesides that, select() provides a useful shortcut to reference all the columns of your DataFrame. That is, the star symbol (*) from the SELECT statement in SQL. This shortcut is very useful when you want to maintain all columns, and, add a new column, at the same time.\nIn the example below, we are adding the same by_1000 column, however, we are bringing all the columns of transf together.\n\ntransf\\\n  .select(\n    '*',\n    (col('transferValue') / 1000).alias('by_1000')\n  )\\\n  .show(5)\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+-------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|by_1000|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+-------+\n|  2022-12-31|2022-12-31 14:00:24|        5516|      7794.31|          zing ƒ|  20223563|       null|                   33|                 4078|               72424-2|7.79431|\n|  2022-12-31|2022-12-31 10:32:07|        4965|       7919.0|          zing ƒ|  20223562|       null|                  421|                 1979|               36441-5|  7.919|\n|  2022-12-31|2022-12-31 07:37:02|        4608|       5603.0|        dollar $|  20223561|       null|                  666|                 4425|               41323-1|  5.603|\n|  2022-12-31|2022-12-31 07:35:05|        1121|      4365.22|        dollar $|  20223560|       null|                  666|                 2400|               74120-4|4.36522|\n|  2022-12-31|2022-12-31 02:53:44|        1121|       4620.0|        dollar $|  20223559|       null|                  421|                 1100|               39830-0|   4.62|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+-------+\nonly showing top 5 rows"
  },
  {
    "objectID": "Chapters/05-transforming.html#calculating-or-adding-new-columns-to-your-dataframe",
    "href": "Chapters/05-transforming.html#calculating-or-adding-new-columns-to-your-dataframe",
    "title": "5  Transforming your Spark DataFrame",
    "section": "5.7 Calculating or adding new columns to your DataFrame",
    "text": "5.7 Calculating or adding new columns to your DataFrame\nAlthough you can add new columns with select(), this method is not specialized to do that. As consequence, when you want to add many new columns, it can become pretty annoying to write select('*', new_column) over and over again. That is why pyspark provides a special method called withColumn().\nThis method has two arguments. First, is the name of the new column. Second, is the formula (or the equation) that represents this new column. As an example, I could reproduce the same by_1000 column like this:\n\ntransf\\\n  .withColumn('by_1000', col('transferValue') / 1000)\\\n  .show(5)\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+-------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|by_1000|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+-------+\n|  2022-12-31|2022-12-31 14:00:24|        5516|      7794.31|          zing ƒ|  20223563|       null|                   33|                 4078|               72424-2|7.79431|\n|  2022-12-31|2022-12-31 10:32:07|        4965|       7919.0|          zing ƒ|  20223562|       null|                  421|                 1979|               36441-5|  7.919|\n|  2022-12-31|2022-12-31 07:37:02|        4608|       5603.0|        dollar $|  20223561|       null|                  666|                 4425|               41323-1|  5.603|\n|  2022-12-31|2022-12-31 07:35:05|        1121|      4365.22|        dollar $|  20223560|       null|                  666|                 2400|               74120-4|4.36522|\n|  2022-12-31|2022-12-31 02:53:44|        1121|       4620.0|        dollar $|  20223559|       null|                  421|                 1100|               39830-0|   4.62|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+-------+\nonly showing top 5 rows\n\n\n\nA lot of the times we use the functions from pyspark.sql.functions module to produce such formulas used by withColumn(). You can checkout the complete list of functions present in this module, by visiting the official documentation of pyspark2.\nYou will see a lot more examples of formulas and uses of withColumn() throughout this book. For now, I just want you to know that withColumn() is a method that adds a new column to your DataFrame. The first argument is the name of the new column, and, the second argument is the calculation formula of this new column."
  },
  {
    "objectID": "Chapters/05-transforming.html#sorting-rows-of-your-dataframe",
    "href": "Chapters/05-transforming.html#sorting-rows-of-your-dataframe",
    "title": "5  Transforming your Spark DataFrame",
    "section": "5.8 Sorting rows of your DataFrame",
    "text": "5.8 Sorting rows of your DataFrame\nSpark, or, pyspark, provides the orderBy() and sort() DataFrame method to sort rows. They both work the same way: you just give the name of the columns that Spark will use to sort the rows.\nIn the example below, Spark will sort transf according to the values in the transferValue column. By default, Spark uses an ascending order while sorting your rows.\n\ntransf\\\n  .orderBy('transferValue')\\\n  .show(5)\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|  2022-07-22|2022-07-22 16:06:25|        3795|         60.0|        dollar $|  20222533|       null|                  290|                 1100|               39925-1|\n|  2022-05-09|2022-05-09 14:02:15|        3284|        104.0|        dollar $|  20222033|       null|                  666|                 2231|               74766-2|\n|  2022-09-16|2022-09-16 20:35:40|        3294|       129.09|          zing ƒ|  20222896|       null|                  290|                 3321|               60867-9|\n|  2022-12-18|2022-12-18 08:45:30|        1297|       142.66|        dollar $|  20223467|       null|                  421|                 5420|               43088-1|\n|  2022-08-20|2022-08-20 09:27:55|        2727|        160.0|        dollar $|  20222724|       null|                   33|                 1002|               75581-5|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\nonly showing top 5 rows\n\n\n\nJust to be clear, you can use the combination between multiple columns to sort your rows. Just give the name of each column (as strings) separated by commas. In the example below, Spark will sort the rows according to clientNumber column first, then, is going to sort the rows of each clientNumber according to transferValue column.\n\ntransf\\\n  .orderBy('clientNumber', 'transferValue')\\\n  .show(5)\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|  2022-03-30|2022-03-30 11:57:22|        1121|        461.0|          euro €|  20221738|       null|                  666|                 6552|               35568-9|\n|  2022-05-23|2022-05-23 11:51:02|        1121|       844.66| british pound £|  20222127|       null|                  421|                 1002|               32340-0|\n|  2022-08-24|2022-08-24 13:51:30|        1121|      1046.93|          euro €|  20222748|       null|                  421|                 6317|               38887-3|\n|  2022-09-23|2022-09-23 19:49:19|        1121|       1327.0| british pound £|  20222938|       null|                  290|                 5420|               77350-1|\n|  2022-06-25|2022-06-25 17:07:08|        1121|       1421.0|        dollar $|  20222361|       null|                  290|                 9921|               77258-7|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\nonly showing top 5 rows\n\n\n\nIf you want to use a descending order in a specific column, you need to use the desc() method from Column class. In the example below, Spark will sort the rows according to clientNumber column using an ascending order. However, it will use the values from transferValue column in a descending order to sort the rows in each clientNumber.\n\ntransf\\\n  .orderBy('clientNumber', col('transferValue').desc())\\\n  .show(5)\n\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|dateTransfer|   datetimeTransfer|clientNumber|transferValue|transferCurrency|transferID|transferLog|destinationBankNumber|destinationBankBranch|destinationBankAccount|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\n|  2022-08-18|2022-08-18 13:57:12|        1121|     11490.37|          zing ƒ|  20222712|       null|                  421|                 2400|               61244-9|\n|  2022-11-05|2022-11-05 08:00:37|        1121|     10649.59|        dollar $|  20223197|       null|                  421|                 3321|               40011-2|\n|  2022-05-17|2022-05-17 10:27:05|        1121|     10471.23| british pound £|  20222086|       null|                  666|                 8521|               26534-1|\n|  2022-05-15|2022-05-15 00:25:49|        1121|      10356.0|        dollar $|  20222075|       null|                   33|                 1979|               28234-7|\n|  2022-06-10|2022-06-09 23:51:39|        1121|      10142.0|        dollar $|  20222241|       null|                   33|                 2400|               36594-6|\n+------------+-------------------+------------+-------------+----------------+----------+-----------+---------------------+---------------------+----------------------+\nonly showing top 5 rows\n\n\n\nThis means that you can mix ascending orders with descending orders in orderBy(). Since the ascending order is the default, if you want to use a descending order in all of the columns, then, you need to apply the desc() method to all of them."
  },
  {
    "objectID": "Chapters/05-transforming.html#calculating-aggregates",
    "href": "Chapters/05-transforming.html#calculating-aggregates",
    "title": "5  Transforming your Spark DataFrame",
    "section": "5.9 Calculating aggregates",
    "text": "5.9 Calculating aggregates\nTo calculate aggregates of a Spark DataFrame we have two main paths: 1) we can use some standard DataFrame methods, like count() or sum(), to calculate a single aggregate; 2) or, we can use the agg() method to calculate multiple aggregates at the same time.\n\n5.9.1 Using standard DataFrame methods\nThe Spark DataFrame class by itself provides a single aggregate method, which is count(). With this method, you can find out how many rows your DataFrame have. In the example below, we can see that transf have 2421 rows.\n\ntransf.count()\n\n2421\n\n\nHowever, if you have a grouped DataFrame (we will learn more about these objects very soon), pyspark provides some more aggregate methods, which are listed below:\n\nmean(): calculate the average value of each numeric column;\nsum(): return the total sum of a column;\ncount(): count the number of rows;\nmax(): compute the maximum value of a column;\nmin(): compute the minimum value of a column;\n\nThis means that you can use any of the above methods after a groupby() call, to calculate aggregates per group in your Spark DataFrame. For now, lets forget about this “groupby” detail, and learn how to calculate different aggregates by using the agg() method.\n\n\n5.9.2 Using the agg() method\nWith the agg() method, we can calculate many different aggregates at the same time. In this method, you should provide a expression that describes what aggregate measure you want to calculate.\nIn most cases, this “aggregate expression” will be composed of functions from the pyspark.sql.functions module. So, having familiarity with the functions present in this module will help you to compose the formulas of your aggregations in agg().\nIn the example below, I am using the sum() and mean() functions from pyspark.sql.functions to calculate the total sum and the total mean of the transferValue column in the transf DataFrame. I am also using the countDistinct() function to calculate the number of distinct values in the clientNumber column.\n\nimport pyspark.sql.functions as F\n\ntransf.agg(\n    F.sum('transferValue').alias('total_value'),\n    F.mean('transferValue').alias('mean_value'),\n    F.countDistinct('clientNumber').alias('number_of_clients')\n  )\\\n  .show()\n\n+--------------------+-----------------+-----------------+\n|         total_value|       mean_value|number_of_clients|\n+--------------------+-----------------+-----------------+\n|1.5217690679999998E7|6285.704535315985|               26|\n+--------------------+-----------------+-----------------+\n\n\n\n\n\n5.9.3 Without groups, we calculate a aggregate of the entire DataFrame\nWhen we do not define any group for the input DataFrame, agg() always produce a new DataFrame with one single row (like in the above example). This happens because we are calculating aggregates of the entire DataFrame, that is, a set of single values (or single measures) that summarizes (in some way) the entire DataFrame.\nIn the other hand, when we define groups in a DataFrame (by using the groupby() method), the calculations performed by agg() are made inside each group in the DataFrame. In other words, instead of summarizing the entire DataFrame, agg() will produce a set of single values that describes (or summarizes) each group in the DataFrame.\nThis means that each row in the resulting DataFrame describes a specific group in the original DataFrame, and, agg() usually produces a DataFrame with more than one single row when its calculations are performed by group. Because our DataFrames usually have more than one single group.\n\n\n5.9.4 Calculating aggregates per group in your DataFrame\nBut how you define the groups inside your DataFrame? To do this, we use the groupby() and groupBy() methods. These methods are both synonymous (they do the same thing).\nThese methods, produce a grouped DataFrame as a result, or, in more technical words, a object of class pyspark.sql.group.GroupedData. You just need to provide, inside this groupby() method, the name of the columns that define (or “mark”) your groups.\nIn the example below, I am creating a grouped DataFrame per client defined in the clientNumber column. This means that each distinct value in the clientNumber column defines a different group in the DataFrame.\n\ntransf_per_client = transf.groupBy('clientNumber')\ntransf_per_client\n\n<pyspark.sql.group.GroupedData at 0x7f11d843fb80>\n\n\nAt first, it appears that nothing has changed. But the groupBy() method always returns a new object of class pyspark.sql.group.GroupedData. As a consequence, we can no longer use some of the DataFrame methods that we used before, like the show() method to see the DataFrame.\nThat’s ok, because we usually do not want to keep this grouped DataFrame for much time. This grouped DataFrame is just a passage (or a bridge) to the result we want, which is, to calculate aggregates per group of the DataFrame.\nAs an example, I can use the max() method, to find out which is the highest value that each user have tranfered, like this:\n\ntransf_per_client\\\n  .max('transferValue')\\\n  .show(5)\n\n+------------+------------------+\n|clientNumber|max(transferValue)|\n+------------+------------------+\n|        1217|           12601.0|\n|        2489|          12644.56|\n|        3284|          12531.84|\n|        4608|          10968.31|\n|        1297|           11761.0|\n+------------+------------------+\nonly showing top 5 rows\n\n\n\nRemember that, by using standard DataFrame methods (like max() in the example above) we can calculate only a single aggregate value. But, with agg() we can calculate more than one aggregate value at the same time. Since our transf_per_client object is a grouped DataFrame, agg() will calculate these aggregates per group.\nAs an example, if I apply agg() with the exact same expressions exposed on Section 5.9.2 with the transf_per_client DataFrame, instead of a DataFrame with one single row, I get a new DataFrame with nine rows. In each row, I have the total and mean values for a specific user in the input DataFrame.\n\ntransf_per_client\\\n  .agg(\n    F.sum('transferValue').alias('total_value'),\n    F.mean('transferValue').alias('mean_value')\n  )\\\n  .show(5)\n\n+------------+-----------------+------------------+\n|clientNumber|      total_value|        mean_value|\n+------------+-----------------+------------------+\n|        1217|575218.2099999998| 6185.142043010751|\n|        2489|546543.0900000001| 6355.152209302327|\n|        3284|581192.5700000001| 6054.089270833334|\n|        4608|        448784.44| 6233.117222222222|\n|        1297|594869.6699999999|6196.5590624999995|\n+------------+-----------------+------------------+\nonly showing top 5 rows\n\n\n\n\n\n\n\nChambers, Bill, and Matei Zaharia. 2018. Spark: The Definitive Guide: Big Data Processing Made Simple. Sebastopol, CA: O’Reilly Media."
  },
  {
    "objectID": "Chapters/07-import.html#introduction",
    "href": "Chapters/07-import.html#introduction",
    "title": "6  Importing data to Spark",
    "section": "6.1 Introduction",
    "text": "6.1 Introduction\nAnother way of creating Spark DataFrames, is to read (or import) data from somewhere and convert it to a Spark DataFrame. Spark can read a variety of file formats, including CSV, Parquet, JSON, ORC and Binary files. Furthermore, Spark can connect to other databases and import tables from them, using ODBC/JDBC connections.\nTo read (or import) any data to Spark, we use a “read engine”, and there are many different read engines available in Spark. Each engine is used to read a specific file format, or to import data from a specific type of data source, and we access these engines by using the read module from your Spark Session object."
  },
  {
    "objectID": "Chapters/07-import.html#reading-data-from-static-files",
    "href": "Chapters/07-import.html#reading-data-from-static-files",
    "title": "6  Importing data to Spark",
    "section": "6.2 Reading data from static files",
    "text": "6.2 Reading data from static files\nStatic files are probably the easiest way to transport data from one computer to another. Because you just need to copy and paste this file to the other machine, or download it from the internet.\nBut in order to Spark read any type of static file stored inside your computer, it always need to know the path to this file. Every OS have its own file system, and every file in your computer is stored in a specific address of this file system. The “path” to this file is the path (or “steps”) that your computer needs to follow to reach this specific address, where the file is stored.\nAs we pointed out earlier, to read any static file in Spark, you use one of the available “read engines”, which are in the spark.read module of your Spark Session. This means that, each read engine in this module is responsible for reading a specific file format.\nIf you want to read a CSV file for example, you use the spark.read.csv() engine. In contrast, if you want to read a JSON file, you use the spark.read.json() engine instead. But no matter what read engine you use, you always give the path to your file to any of these “read engines”.\nThe main read engines available in Spark for static files are:\n\nspark.read.json(): to read JSON files;\nspark.read.csv(): to read CSV files;\nspark.read.parquet(): to read Apache Parquet files;\nspark.read.orc(): to read ORC (Apache Optimized Row Columnar format) files;\n\nFor example, to read a JSON file called sales.json that is stored in my Data folder, I can do this:\n\njson_data = spark.read.json(\"../Data/sales.json\")\njson_data.show()\n\n+-----+----------+------------+-------+-------------------+-----+\n|price|product_id|product_name|sale_id|          timestamp|units|\n+-----+----------+------------+-------+-------------------+-----+\n| 3.12|       134| Milk 1L Mua| 328711|2022-02-01T22:10:02|    1|\n| 1.22|       110|  Coke 350ml| 328712|2022-02-03T11:42:09|    3|\n| 4.65|       117|    Pepsi 2L| 328713|2022-02-03T14:22:15|    1|\n| 1.22|       110|  Coke 350ml| 328714|2022-02-03T18:33:08|    1|\n| 0.85|       341|Trident Mint| 328715|2022-02-04T15:41:36|    1|\n+-----+----------+------------+-------+-------------------+-----+"
  },
  {
    "objectID": "Chapters/07-import.html#an-example-with-a-csv-file",
    "href": "Chapters/07-import.html#an-example-with-a-csv-file",
    "title": "6  Importing data to Spark",
    "section": "6.3 An example with a CSV file",
    "text": "6.3 An example with a CSV file\nAs an example, I have the following CSV file saved in my computer:\nname,age,job\nJorge,30,Developer\nBob,32,Developer\nThis CSV was saved in a file called people.csv, inside a folder called Data. So, to read this static file, Spark needs to know the path to this people.csv file. In other words, Spark needs to know where this file is stored in my computer, to be able to read it.\nIn my specific case, considering where this Data folder is in my computer, a relative path to it would be \"../Data/\". Having the path to the folder where people.csv is stored, I just need to add this file to the path, resulting in \"../Data/people.csv\". See in the example below, that I gave this path to the read.csv() method of my Spark Session. As a result, Spark will visit this address, and, read the file that is stored there:\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder.getOrCreate()\n\npath = \"../Data/people.csv\"\n\ndf = spark.read.csv(path)\ndf.show()\n\n+-----+---+---------+\n|  _c0|_c1|      _c2|\n+-----+---+---------+\n| name|age|      job|\n|Jorge| 30|Developer|\n|  Bob| 32|Developer|\n+-----+---+---------+\n\n\n\nIn the above example, I gave a relative path to the file I wanted to read. But you can provide an absolute path1 to the file, if you want to. The people.csv is located at a very specific folder in my Linux computer, so, the absolute path to this file is pretty long as you can see below. But, if I were in my Windows machine, this absolute path would be something like \"C:\\Users\\pedro\\Documents\\Projects\\...\".\n\n# The absolute path to `people.csv`:\npath = \"/home/pedro/Documents/Projets/Books/Introd-pyspark/Data/people.csv\"\n\ndf = spark.read.csv(path)\ndf.show()\n\n\n\n+-----+---+---------+\n|  _c0|_c1|      _c2|\n+-----+---+---------+\n| name|age|      job|\n|Jorge| 30|Developer|\n|  Bob| 32|Developer|\n+-----+---+---------+\n\n\n\nIf you give an invalid path (that is, a path that does not exist in your computer), you will get a AnalysisException. In the example below, I try to read a file called \"weird-file.csv\" that (in theory) is located at my current working directory. But when Spark looks inside my current directory, it does not find any file called \"weird-file.csv\". As a result, Spark raises a AnalysisException that warns me about this mistake.\n\ndf = spark.read.csv(\"weird-file.csv\")\n\nTraceback (most recent call last):\n  ...\npyspark.sql.utils.AnalysisException: Path does not exist: file:/home/pedro/Documents/Projects/Books/Introd-pyspark/weird-file.csv\nEvery time you face this “Path does not exist” error, it means that Spark did not found the file that you described in the path you gave to spark.read. In this case, is very likely that you have a typo or a mistake in your path. Maybe your forgot to add the .csv extension to the name of your file. Or maybe you forgot to use the right angled slash (/) instead of the left angled slash (\\). Or maybe, you gave the path to folder \\(x\\), when in fact, you wanted to reach the folder \\(y\\).\nSometimes, is useful to list all the files that are stored inside the folder you are trying to access. This way, you can make sure that you are looking at the right folder of your file system. To do that, you can use the listdir() function from os module of python. As an example, I can list all the files that are stored inside of the Data folder in this way:\n\nfrom os import listdir\nlistdir(\"../Data/\")\n\n['accounts.csv',\n 'user-events.json',\n 'transf.csv',\n 'people.csv',\n 'penguins.csv',\n 'livros.txt',\n 'transf_reform.csv',\n 'sales.json',\n 'books.txt']"
  },
  {
    "objectID": "Chapters/07-import.html#import-options",
    "href": "Chapters/07-import.html#import-options",
    "title": "6  Importing data to Spark",
    "section": "6.4 Import options",
    "text": "6.4 Import options\nWhile reading and importing data from any type of data source, Spark will always use the default values for each import option defined by the read engine you are using, unless you explicit ask it to use a different value. Each read engine has its own read/import options.\nFor example, the spark.read.orc() engine has a option called mergeSchema. With this option, you can ask Spark to merge the schemas collected from all the ORC part-files. In contrast, the spark.read.csv() engine does not have such option. Because this functionality of “merging schemas” does not make sense with CSV files.\nThis means that, some import options are specific (or characteristic) of some file formats. For example, the sep option (where you define the separator character) is used only in the spark.read.csv() engine. Because you do not have a special character that behaves as the “separator” in the other file formats (like ORC, JSON, Parquet…). So it does not make sense to have such option in the other read engines.\nIn the other hand, some import options can co-exist in multiple read engines. For example, the spark.read.json() and spark.read.csv() have both an encoding option. The encoding is a very important information, and Spark needs it to correctly interpret your file. By default, Spark will always assume that your files use the UTF-8 encoding system. Although, this may not be true for your specific case, and for these cases you use this encoding option to tell Spark which one to use.\nIn the next sections, I will break down some of the most used import options for each file format. If you want to see the complete list of import options, you can visit the Data Source Option section in the specific part of the file format you are using in the Spark SQL Guide2.\nTo define, or, set a specific import option, you use the option() method from a DataFrameReader object. To produce this kind of object, you use the spark.read module, like in the example below. Each call to the option() method is used to set a single import option.\nNotice that the “read engine” of Spark (i.e. csv()) is the last method called at this chain (or sequence) of steps. In other words, you start by creating a DataFrameReader object, then, set the import options, and lastly, you define which “read engine” you want to use.\n\n# Creating a `DataFrameReader` object:\ndf_reader = spark.read\n# Setting the import options:\ndf_reader = df_reader\\\n  .option(\"sep\", \"$\")\\\n  .option(\"locale\", \"pt-BR\")\n  \n# Setting the \"read engine\" to be used with `.csv()`:\nmy_data = df_reader\\\n  .csv(\"../Data/a-csv-file.csv\")\n\nIf you prefer, you can also merge all these calls together like this:\n\nspark.read\\ # a `DataFrameReader` object\n  .option(\"sep\", \"$\")\\ # Setting the `sep` option\n  .option(\"locale\", \"pt-BR\")\\ # Setting the `locale` option\n  .csv(\"../Data/a-csv-file.csv\") # The \"read engine\" to be used\n\nThere are many different import options for each read engine, and you can see the full list in the official documentation for Spark3. But lets just give you a brief overview of the probably most popular import options:\n\nsep: sets the separator character for each field and value in the CSV file (defaults to \",\");\nencoding: sets the character encoding of the file to be read (defaults to \"UTF-8\");\nheader: boolean (defaults to False), should Spark consider the first line of the file as the header of the DataFrame (i.e. the name of the columns) ?\ndateFormat and timestampFormat: sets the format for dates and timestamps in the file (defaults to \"yyyy-MM-dd\" and \"yyyy-MM-dd'T'HH:mm:ss[.SSS][XXX]\" respectively);"
  },
  {
    "objectID": "Chapters/07-import.html#setting-the-separator-character-for-csv-files",
    "href": "Chapters/07-import.html#setting-the-separator-character-for-csv-files",
    "title": "6  Importing data to Spark",
    "section": "6.5 Setting the separator character for CSV files",
    "text": "6.5 Setting the separator character for CSV files\nIn this section, we will use the transf_reform.csv file to demonstrate how to set the separator character of a CSV file. This file, contains some data of transfers made in a fictitious bank. Is worth mentioning that this sep import option is only available for CSV files.\nLets use the peek_file() function defined below to get a quick peek at the first 5 lines of this file. If you look closely to these lines, you can identify that this CSV files uses the \";\" character to separate fields and values, and not the american standard \",\" character.\n\ndef peek_file(path, n_lines = 5):\n  with open(path) as file:\n    lines = [next(file) for i in range(n_lines)]\n  text = ''.join(lines)\n  print(text)\n  \npeek_file(\"../Data/transf_reform.csv\")\n\ndatetime;user;value;transferid;country;description\n2018-12-06T22:19:19Z;Eduardo;598.5984;116241629;Germany;\n2018-12-06T22:10:34Z;Júlio;4610.955;115586504;Germany;\n2018-12-06T21:59:50Z;Nathália;4417.866;115079280;Germany;\n2018-12-06T21:54:13Z;Júlio;2739.618;114972398;Germany;\n\n\n\nThis is usually the standard adopted by countries that uses a comma to define decimal places in real numbers. In other words, in some countries, the number 3.45 is usually written as 3,45.\nAnyway, we know now that the transf_reform.csv file uses a different separator character, so, to correctly read this CSV file into Spark, we need to set the sep import option. Since this file comes with the column names in the first line, I also set the header import option to read this first line as the column names as well.\n\ntransf = spark.read\\\n  .option(\"sep\", \";\")\\\n  .option(\"header\", True)\\\n  .csv(\"../Data/transf_reform.csv\")\n  \ntransf.show(5)\n\n+--------------------+--------+--------+----------+-------+-----------+\n|            datetime|    user|   value|transferid|country|description|\n+--------------------+--------+--------+----------+-------+-----------+\n|2018-12-06T22:19:19Z| Eduardo|598.5984| 116241629|Germany|       null|\n|2018-12-06T22:10:34Z|   Júlio|4610.955| 115586504|Germany|       null|\n|2018-12-06T21:59:50Z|Nathália|4417.866| 115079280|Germany|       null|\n|2018-12-06T21:54:13Z|   Júlio|2739.618| 114972398|Germany|       null|\n|2018-12-06T21:41:27Z|     Ana|1408.261| 116262934|Germany|       null|\n+--------------------+--------+--------+----------+-------+-----------+\nonly showing top 5 rows"
  },
  {
    "objectID": "Chapters/07-import.html#setting-the-encoding-of-the-file",
    "href": "Chapters/07-import.html#setting-the-encoding-of-the-file",
    "title": "6  Importing data to Spark",
    "section": "6.6 Setting the encoding of the file",
    "text": "6.6 Setting the encoding of the file\nSpark will always assume that your static files use the UTF-8 encoding system. But, that might not be the case for your specific file. In this situation, you have to tell Spark which is the appropriate encoding system to be used while reading the file. This encoding import option is available both for CSV and JSON files.\nTo do this, you can set the encoding import option, with the name of the encoding system to be used. As an example, lets look at the file books.txt, which is a CSV file encoded with the ISO-8859-1 system (i.e. the Latin 1 system).\nIf we use the defaults in Spark, you can see in the result below that some characters in the Title column are not correctly interpreted. Remember, this problem occurs because of a mismatch in encoding systems. Spark thinks books.txt is using the UTF-8 system, but, in reality, it uses the ISO-8859-1 system:\n\nbooks = spark.read\\\n  .option(\"header\", True)\\\n  .csv(\"../Data/books.txt\")\n  \nbooks.show()\n\n+--------------------+--------------------+------+\n|               Title|              Author| Price|\n+--------------------+--------------------+------+\n|            O Hobbit|    J. R. R. Tolkien| 40.72|\n|Matem�tica para E...|Carl P. Simon and...|139.74|\n|Microeconomia: um...|       Hal R. Varian| 141.2|\n|      A Luneta �mbar|      Philip Pullman| 42.89|\n+--------------------+--------------------+------+\n\n\n\nBut if we tell Spark to use the ISO-8859-1 system while reading the file, then, all problems are solved, and all characters in the file are correctly interpreted, as you see in the result below:\n\nbooks = spark.read\\\n  .option(\"header\", True)\\\n  .option(\"encoding\", \"ISO-8859-1\")\\\n  .csv(\"../Data/books.txt\")\n  \nbooks.show()\n\n+--------------------+--------------------+------+\n|               Title|              Author| Price|\n+--------------------+--------------------+------+\n|            O Hobbit|    J. R. R. Tolkien| 40.72|\n|Matemática para E...|Carl P. Simon and...|139.74|\n|Microeconomia: um...|       Hal R. Varian| 141.2|\n|      A Luneta Âmbar|      Philip Pullman| 42.89|\n+--------------------+--------------------+------+"
  },
  {
    "objectID": "Chapters/07-import.html#setting-the-format-of-dates-and-timestamps",
    "href": "Chapters/07-import.html#setting-the-format-of-dates-and-timestamps",
    "title": "6  Importing data to Spark",
    "section": "6.7 Setting the format of dates and timestamps",
    "text": "6.7 Setting the format of dates and timestamps\nThe format that humans write dates and timestamps vary drastically over the world. By default, Spark will assume that the dates and timestamps stored in your file are in the format described by the ISO-8601 standard. That is, the “YYYY-mm-dd”, or, “year-month-day” format.\nBut this standard might not be the case for your file. For example: the brazilian people usually write dates in the format “dd/mm/YYYY”, or, “day/month/year”; some parts of Spain write dates in the format “YYYY/dd/mm”, or, “year/day/month”; on Nordic countries (i.e. Sweden, Finland) dates are written in “YYYY.mm.dd” format.\nEvery format of a date or timestamp is defined by using a string with the codes of each part of the date/timestamp, like the letter ‘Y’ which represents a 4-digit year, or the letter ‘d’ which represents a 2-digit day. You can see the complete list of codes and their description in the official documentation of Spark4.\nAs an example, lets look into the user-events.json file. We can see that the dates and timestamps in this file are using the “dd/mm/YYYY” and “dd/mm/YYYY HH:mm:ss” formats respectively.\n\npeek_file(\"../Data/user-events.json\", n_lines=3)\n\n{\"dateOfEvent\":\"15/06/2022\",\"timeOfEvent\":\"15/06/2022 14:33:10\",\"userId\":\"b902e51e-d043-4a66-afc4-a820173e1bb4\",\"nameOfEvent\":\"entry\"}\n{\"dateOfEvent\":\"15/06/2022\",\"timeOfEvent\":\"15/06/2022 14:40:08\",\"userId\":\"b902e51e-d043-4a66-afc4-a820173e1bb4\",\"nameOfEvent\":\"click: shop\"}\n{\"dateOfEvent\":\"15/06/2022\",\"timeOfEvent\":\"15/06/2022 15:48:41\",\"userId\":\"b902e51e-d043-4a66-afc4-a820173e1bb4\",\"nameOfEvent\":\"select: payment-method\"}\n\n\nDate variables are usually interpreted by Spark as string variables. In other words, Spark usually do not convert data that contains dates to the date type of Spark. In order to Spark\n\nfrom pyspark.sql.types import StructType, StructField\nfrom pyspark.sql.types import DateType, StringType, TimestampType\nspark.conf.set(\"spark.sql.legacy.timeParserPolicy\",\"LEGACY\")\nschema = StructType([\n  StructField('dateOfEvent', DateType(), True),\n  StructField('timeOfEvent', TimestampType(), True),\n  StructField('userId', StringType(), True),\n  StructField('nameOfEvent', StringType(), True)\n])\n\nuser_events = spark.read\\\n  .option(\"dateFormat\", \"dd/mm/YYYY\")\\\n  .option(\"timestampFormat\", \"dd/mm/YYYY HH:mm:ss\")\\\n  .json(\"../Data/user-events.json\", schema = schema)\n  \nuser_events.show()"
  },
  {
    "objectID": "Chapters/06-dataframes-sql.html#introduction",
    "href": "Chapters/06-dataframes-sql.html#introduction",
    "title": "7  Working with SQL in pyspark",
    "section": "7.1 Introduction",
    "text": "7.1 Introduction\nAs we discussed in Chapter 2, Spark is a multi-language engine for large-scale data processing. This means that we can build our Spark application using many different languages (like Java, Scala, Python and R). Furthermore, you can also use the Spark SQL module of Spark to translate all of your transformations into pure SQL queries.\nIn more details, Spark SQL is a Spark module for structured data processing (Apache Spark Official Documentation 2022). As a result, you can use it to translate all transformations that you build with the DataFrame API, which is the main structured API of Spark. This means that virtually all transformations exposed throughout this book, can be translated into a SQL query in Spark.\nHowever, this also means that the Spark SQL module does not handle the transformations produced by the unstructured APIs of Spark, i.e. the Dataset API. Since the Dataset API is not available in pyspark, it is not covered in this book.\nDue to the multi-language nature of Spark, you can mix python code that uses the DataFrame API with pure SQL queries to build your transformations. We will focus on this exchange between Python and SQL in this chapter."
  },
  {
    "objectID": "Chapters/06-dataframes-sql.html#the-sql-method-as-the-main-entrypoint",
    "href": "Chapters/06-dataframes-sql.html#the-sql-method-as-the-main-entrypoint",
    "title": "7  Working with SQL in pyspark",
    "section": "7.2 The sql() method as the main entrypoint",
    "text": "7.2 The sql() method as the main entrypoint\nThe main entrypoint to Spark SQL is the sql() method of your Spark Session. This method accepts a SQL query inside a string as input, and will always output a new Spark DataFrame as result. That is why I used the show() method right after sql(), in the example below, to see what this new Spark DataFrame looked like.\nAs a first example, lets run a very basic SQL query, that just select a list of code values:\n\nsql_query = '''\nSELECT *\nFROM (\n  VALUES (11), (31), (24), (35)\n) AS List(Codes)\n'''\n\nspark.sql(sql_query).show()\n\n+-----+\n|Codes|\n+-----+\n|   11|\n|   31|\n|   24|\n|   35|\n+-----+\n\n\n\nIf you want to execute a very short SQL query, is fine to write it inside a single pair of quotation marks (for example \"SELECT * FROM sales.per_day\"). However, since SQL queries usually take multiple lines, you can write your SQL query inside a python docstring (created by a pair of three quotation marks), like in the example above.\nHaving this in mind, every time you want to execute a SQL query, you can use this sql() method from the object you stored your Spark Session. The sql() method is the bridge between pyspark and SQL. You give it a pure SQL query inside a string, and, Spark will execute it, considering your Spark SQL context."
  },
  {
    "objectID": "Chapters/06-dataframes-sql.html#creating-sql-tables-in-spark",
    "href": "Chapters/06-dataframes-sql.html#creating-sql-tables-in-spark",
    "title": "7  Working with SQL in pyspark",
    "section": "7.3 Creating SQL Tables in Spark",
    "text": "7.3 Creating SQL Tables in Spark\nIn real life jobs at the industry, is very likely that your data will be allocated inside a SQL-like database. Spark can connect to a external SQL database through JDBC/ODBC connections, or, read tables from Apache Hive. This way, you can sent your SQL queries to this external database.\nHowever, to expose more simplified examples throughout this chapter, we will use pyspark to create a simple temporary SQL table in our Spark SQL context, and use this temporary SQL table in our examples of SQL queries. This way, we avoid the work to connect to some existing SQL database, and, still get to learn how to use SQL queries in pyspark.\nFirst, lets create our Spark Session. You can see below that I used the config() method to set a specific option of the session called spark.sql.catalogImplementation, to the value \"hive\". This option controls the implementation of the Spark SQL Catalog, which is a core part of the SQL functionality of Spark 1.\nSpark usually complain with a AnalysisException error when you try to create SQL tables with this option undefined (or not configured). If you decide to follow the examples of this chapter, please always set this option right at the start of your script2.\n\nfrom pyspark.sql import SparkSession\nspark = SparkSession\\\n  .builder\\\n  .config(\"spark.sql.catalogImplementation\",\"hive\")\\\n  .getOrCreate()\n\n\n7.3.1 TABLEs versus VIEWs\nTo run a complete SQL query over any Spark DataFrame, you must register this DataFrame in the Spark SQL Catalog of your Spark Session. You can register a Spark DataFrame into this catalog as a physical SQL TABLE, or, as a SQL VIEW.\nIf you are familiar with SQL in other platforms, you probably already heard of these two types (TABLE or VIEW) of tables. But if not, we will explain each one in this section. Is worth pointing out that choosing between these two types does not affect your code, or your transformations in any way. It just affect the way that Spark SQL stores the table/DataFrame itself.\n\n7.3.1.1 VIEWs are stored as SQL queries or memory pointers\nWhen you register a DataFrame as a SQL VIEW, the query to produce this DataFrame is stored, not the DataFrame itself. There are also cases where Spark store a memory pointer instead, that points to the memory adress where this DataFrame is stored in memory. In this perspective, Spark SQL use this pointer every time it needs to access this DataFrame.\nTherefore, when you call (or access) this SQL VIEW inside your SQL queries (for example, with a SELECT * FROM statement), Spark SQL will automatically get this SQL VIEW “on the fly” (or “on runtime”), by running the query necessary to build the initial DataFrame that you stored inside this VIEW, or, if this DataFrame is already stored in memory, Spark will look at the specific memory address it is stored.\nIn other words, when you create a SQL VIEW, Spark SQL do not store any physical data or rows of the table/DataFrame. It just stores the SQL query necessary to build your table/DataFrame, or, for temporary SQL VIEWs, a memory pointer to the Spark DataFrame that you stored in this VIEW. In some sense, you can interpret any SQL VIEWs as an abbreviation to a SQL query, or a nickname to an already existing DataFrame.\nAs a consequence, for most “use case scenarios”, SQL VIEWs are easier to manage inside your data pipelines. Because you usually do not have to update them. Since they are calculated “on the fly”, a SQL VIEW will always translate the most recent version of the data.\nIn pyspark, you can register a Spark DataFrame as a SQL VIEW with createTempView and createOrReplaceTempView() methods. These methods register your Spark DataFrame as a temporary SQL VIEW, and have a single input, which is the name you want to give to this new SQL VIEW you are creating inside a string:\n\n# To save the `df` DataFrame as a SQL VIEW, use one of the methods below:\ndf.createTempView('example_view')\ndf.createOrReplaceTempView('example_view')\n\nAfter you registered your DataFrame, you can use it in any SQL query, like in the example below:\n\nsql_query = '''\nSELECT *\nFROM example_view\nWHERE value > 20\n'''\n\nspark.sql(sql_query).show()\n\n[Stage 0:>                                                          (0 + 1) / 1]\n\n\n                                                                                \n\n\n+---+-----+----------+\n| id|value|      date|\n+---+-----+----------+\n|  1| 28.3|2021-01-01|\n|  3| 20.1|2021-01-02|\n+---+-----+----------+\n\n\n\nYou could also save a specific SQL query as a persistent SQL query, with a normal CREATE VIEW statement through the sql() method. In the example below, I am saving the simple query that I showed at the beginning of this chapter inside a VIEW called list_of_codes.\n\nsql_query = '''\nCREATE OR REPLACE VIEW list_of_codes AS\nSELECT *\nFROM (\n  VALUES (11), (31), (24), (35)\n) AS List(Codes)\n'''\n\nspark.sql(sql_query)\n\nDataFrame[]\n\n\nNow, every time I want to use this SQL query that selects a list of codes, I can use this list_of_codes as a shortcut.\n\nspark.sql(\"SELECT * FROM list_of_codes\").show()\n\n+-----+\n|Codes|\n+-----+\n|   11|\n|   31|\n|   24|\n|   35|\n+-----+\n\n\n\n\n\n7.3.1.2 TABLEs are stored as physical tables\nIn the other hand, SQL TABLEs are the “opposite” of SQL VIEWs. That is, SQL TABLEs are stored as physical tables inside the SQL database. In other words, each one of the rows of your table are stored inside the SQL database.\nBecause of this characteristic, when dealing with huges amounts of data, SQL TABLEs are usually faster to load and transform. Because you just have to read the data stored on the database, you do not need to calculate it from scratch every time you use it. But, as a collateral effect, you usually have to physically update the data inside this TABLE, by using, for example, INSERT INTO statements.\nIn pyspark, you can save a Spark DataFrame as a SQL TABLE with the write.saveAsTable() method. This method accepts, as first input, the name you want to give to this SQL TABLE inside a string.\n\n# To save the `df` DataFrame as a SQL TABLE:\ndf.write.saveAsTable('example_table')\n\nThere are other arguments that you might want to use in this write.saveAsTable() method, like the mode argument. This argument controls if you want to rewrite/replace the entire table with the current data of your DataFrame (mode = 'overwrite'), or, if you just want to append (or insert) this data into the table (mode = 'append'). You can see the full list of arguments and their description by looking at the documentation3.\nAs you expect, after we registered the DataFrame as a SQL table, we can now run any SQL query over example_table, like in the example below:\n\nspark.sql(\"SELECT SUM(value) FROM example_table\").show()\n\n+----------+\n|sum(value)|\n+----------+\n|      76.8|\n+----------+\nYou can also use pure SQL queries to create a empty SQL TABLE from scratch, and then, feed this table with data by using INSERT INTO statements. In the example below, we create a new database called examples, and, inside of it, a table called code_brazil_states. Then, we populated it with a few rows of data.\n\nspark.sql('CREATE DATABASE `examples`')\nspark.sql('USE `examples`')\nspark.sql('''\n  CREATE TABLE `code_brazil_states` (\n    `code` INT,\n    `state_name` STRING\n  )\n''')\nspark.sql('INSERT INTO `code_brazil_states` VALUES (31, \"Minas Gerais\")')\nspark.sql('INSERT INTO `code_brazil_states` VALUES (15, \"Pará\")')\nspark.sql('INSERT INTO `code_brazil_states` VALUES (41, \"Paraná\")')\nspark.sql('INSERT INTO `code_brazil_states` VALUES (25, \"Paraíba\")')\n\nWe can see now this new physical SQL table using a simple query like this:\n\nspark\\\n  .sql('SELECT * FROM examples.code_brazil_states')\\\n  .show()\n\n+----+------------+\n|code|  state_name|\n+----+------------+\n|  41|      Paraná|\n|  31|Minas Gerais|\n|  15|        Pará|\n|  25|     Paraíba|\n+----+------------+\n\n\n\n7.3.2 Spark SQL Catalog is the bridge between SQL and pyspark\nRemember, to run SQL queries over any Spark DataFrame, you have to register this DataFrame into the Spark SQL Catalog. Because of it, this Spark SQL Catalog works almost as the bridge that connects the python objects that hold your Spark DataFrames to the Spark SQL context. Without it, Spark SQL will not find your Spark DataFrames. As a result, it can not run any SQL query over it.\nThe methods saveAsTable(), createTempView and createOrReplaceTempView() are the main methods to register your Spark DataFrame into this Spark SQL Catalog. This means that you have to use one of these methods before you run any SQL query over your Spark DataFrame.\n\n\n7.3.3 Temporary versus Persistent sources\nWhen you register any Spark DataFrame as a SQL TABLE, it becomes a persistent source. Because the contents, the data, the rows of the table are stored on disk, inside a database, and can be accessed any time, even after you close or restart your computer (or your Spark Session). In other words, it becomes “persistent” as in the sense of “it does not die”.\nHowever, with methods createTempView and createOrReplaceTempView() you register your Spark DataFrame as a temporary SQL VIEW. This means that the life (or time of existence) of this VIEW is tied to your Spark Session. In other words, it will exist in your Spark SQL Catalog only for the duration of your Spark Session. When you close your Spark Session, this VIEW just dies. When you start a new Spark Session it does not exist anymore. As a result, you have to register your DataFrame again at the catalog to use it one more time."
  },
  {
    "objectID": "Chapters/06-dataframes-sql.html#the-penguins-table",
    "href": "Chapters/06-dataframes-sql.html#the-penguins-table",
    "title": "7  Working with SQL in pyspark",
    "section": "7.4 The penguins table",
    "text": "7.4 The penguins table\nIn the example below, I am reading a CSV file from my computer called penguins.csv (remember that this CSV can be downloaded from the book repository4), then, I create a SQL temporary view (called penguins_view) from this penguins DataFrame with the createOrReplaceTempView() method.\n\npath = \"../Data/penguins.csv\"\npenguins = spark.read\\\n  .csv(path, header = True)\n  \npenguins.createOrReplaceTempView('penguins_view')\n\nAfter these commands, I have now a SQL view called penguins_view registered in my Spark SQL context, which I can query it, using pure SQL:\n\nspark.sql('SELECT * FROM penguins_view').show(5)\n\n+-------+---------+--------------+-------------+-----------------+-----------+------+----+\n|species|   island|bill_length_mm|bill_depth_mm|flipper_length_mm|body_mass_g|   sex|year|\n+-------+---------+--------------+-------------+-----------------+-----------+------+----+\n| Adelie|Torgersen|          39.1|         18.7|              181|       3750|  male|2007|\n| Adelie|Torgersen|          39.5|         17.4|              186|       3800|female|2007|\n| Adelie|Torgersen|          40.3|           18|              195|       3250|female|2007|\n| Adelie|Torgersen|          null|         null|             null|       null|  null|2007|\n| Adelie|Torgersen|          36.7|         19.3|              193|       3450|female|2007|\n+-------+---------+--------------+-------------+-----------------+-----------+------+----+\nonly showing top 5 rows"
  },
  {
    "objectID": "Chapters/06-dataframes-sql.html#selecting-spark-sql-tables",
    "href": "Chapters/06-dataframes-sql.html#selecting-spark-sql-tables",
    "title": "7  Working with SQL in pyspark",
    "section": "7.5 Selecting Spark SQL tables",
    "text": "7.5 Selecting Spark SQL tables\nAn obvious way to access any SQL TABLE or VIEW registered in your Spark SQL context, is to select it, through a simple SELECT * FROM statement, like we saw in the previous examples. However, it can be quite annoying to type “SELECT * FROM” every time you want to use a Spark SQL table.\nThat is why Spark offers a shortcut to us, which is the table() method of your Spark session. In other words, the code spark.table(\"table_name\") is a shortcut to spark.sql(\"SELECT * FROM table_name\"). They both mean the same thing. For example, we could access penguins_view as:\n\nspark\\\n  .table('penguins_view')\\\n  .show(5)\n\n+-------+---------+--------------+-------------+-----------------+-----------+------+----+\n|species|   island|bill_length_mm|bill_depth_mm|flipper_length_mm|body_mass_g|   sex|year|\n+-------+---------+--------------+-------------+-----------------+-----------+------+----+\n| Adelie|Torgersen|          39.1|         18.7|              181|       3750|  male|2007|\n| Adelie|Torgersen|          39.5|         17.4|              186|       3800|female|2007|\n| Adelie|Torgersen|          40.3|           18|              195|       3250|female|2007|\n| Adelie|Torgersen|          null|         null|             null|       null|  null|2007|\n| Adelie|Torgersen|          36.7|         19.3|              193|       3450|female|2007|\n+-------+---------+--------------+-------------+-----------------+-----------+------+----+\nonly showing top 5 rows"
  },
  {
    "objectID": "Chapters/06-dataframes-sql.html#executing-sql-expressions",
    "href": "Chapters/06-dataframes-sql.html#executing-sql-expressions",
    "title": "7  Working with SQL in pyspark",
    "section": "7.6 Executing SQL expressions",
    "text": "7.6 Executing SQL expressions\nAs I noted at Section 4.2, columns of a Spark DataFrame are closely related to expressions. As a result, you usually use and execute expressions in Spark when you want to transform (or mutate) columns of a Spark DataFrame.\nThis is no different for SQL expressions. A SQL expression is basically any expression you would use on the SELECT statement of your SQL query. As you probably guessed, since these expresisons are used on the SELECT statement, you can use them to transform columns of a Spark DataFrame.\nThere are many column transformations that are particularly verbose and expensive to write in pyspark. But you can use a SQL expression in your favor, to translate this transformation into a more short and concise form. Virtually every expression you write in pyspark can be translated into a SQL expression.\nTo execute a SQL expression, you give this expression inside a string to the expr() function from the pyspark.sql.functions module. Since expressions are used to transform columns, you usually use expr() inside a withColumn() method.\n\nfrom pyspark.sql.functions import expr\n\nspark\\\n  .table('penguins_view')\\\n  .withColumn(\n    'specie_island',\n    expr(\"CONCAT(species, island)\")\n  )\\\n  .withColumn(\n    'sex_short',\n    expr(\"CASE WHEN sex == 'male' THEN 'M' ELSE 'F' END\")\n  )\\\n  .select('specie_island', 'sex_short')\\\n  .show(5)\n\n+---------------+---------+\n|  specie_island|sex_short|\n+---------------+---------+\n|AdelieTorgersen|        M|\n|AdelieTorgersen|        F|\n|AdelieTorgersen|        F|\n|AdelieTorgersen|        F|\n|AdelieTorgersen|        F|\n+---------------+---------+\nonly showing top 5 rows\n\n\n\n\n\n\n\nApache Spark Official Documentation. 2022. Documentation for Apache Spark 3.2.1; Available at: https://spark.apache.org/docs/latest/."
  },
  {
    "objectID": "Chapters/references.html",
    "href": "Chapters/references.html",
    "title": "References",
    "section": "",
    "text": "Apache Spark Official Documentation. 2022. Documentation for\nApache Spark 3.2.1; Available at: https://spark.apache.org/docs/latest/.\n\n\nChambers, Bill, and Matei Zaharia. 2018. Spark: The Definitive\nGuide: Big Data Processing Made Simple. Sebastopol, CA: O’Reilly\nMedia.\n\n\nDamji, Jules, Brooke Wenig, Tathagata Das, and Denny Lee. 2020.\nLearning Spark: Lightning-Fast Data Analytics. Sebastopol, CA:\nO’Reilly Media.\n\n\nKarau, Holden, Andy Konwinski, Patrick Wendell, and Matei Zaharia. 2015.\nLearning Spark: Lightning-Fast Data Analytics. Sebastopol, CA:\nO’Reilly Media."
  },
  {
    "objectID": "Chapters/00-terminal.html",
    "href": "Chapters/00-terminal.html",
    "title": "Appendix A — Opening the terminal of your OS",
    "section": "",
    "text": "Every OS comes with a terminal (or a command prompt). A terminal is usually just a black screen where you can send commands to be executed by your OS. The Figure A.1 shows a print screen of the terminal in Ubuntu. The terminal on Windows is very similar to this.\n\n\n\nFigure A.1: A terminal on Ubuntu\n\n\nTerminals are incredibely flexible tools, and you will find this tool very useful for a number of things. However, since they are just a black and empty screen, they give you no clue of what you should do, or what commands are available to you.\nThat is ok, for now, you should not worry about that. I will expose many terminal commands in this book, and these commands should be used inside these terminals that comes with your OS. The next sub-sections will show you how to open a terminal in each one of the OS’s where Spark is available. Lets begin with Windows.\n\nA.0.1 Opening a terminal on Windows\nThere are some different approaches to do this, but, the one that I find the most useful, is to open a terminal from inside a folder, using the default File Explorer program of Windows.\nThis is very useful, because we normally use terminal commands to affect a set of files that are stored inside a specific folder in our computer.\nAs a result, when we open a terminal from inside a folder, using the File Explorer, the terminal opened is already rooted inside the folder where our files are stored. In other words, we already have easy access to the files that we want to affect/use in our command, and we do not have the work to change or adjust directories in this terminal.\nFor example, lets suppose that we want to use a terminal command to use a file called hello.py, and, that this hello.py file is stored inside a folder called HelloPython. You can see in Figure A.2, that this folder is in path C:\\Users\\pedro\\Documents\\HelloPython. So, the first step, is to use the File Explorer of Windows, to open this folder, like in Figure A.2.\n\n\n\nFigure A.2: Opening the HelloPython folder in Windows\n\n\nAfter you opened this folder, substitute the path to the folder in the search box with the word “cmd”, like in Figure A.3, and them, press Enter in the keyboard.\n\n\n\nFigure A.3: Opening a terminal inside a Windows folder - Part 1\n\n\nAs a result, a new terminal will open. See in Figure A.4, that this new terminal is already looking to (or is already rooted on) this HelloPython folder. This way, we can easily access the files stored inside this folder, like the hello.py file.\n\n\n\nFigure A.4: Opening a terminal inside a Windows folder - Part 2\n\n\n\n\nA.0.2 Opening a terminal on Linux\nIs fairly easy to open a terminal on a Linux distribution. Again, is very useful when you open the terminal from inside the folder you are interested in. Because you will have an easier access to all the files that are stored inside this folder.\nTo do this in Linux, you use the built-in File Explorer to open the folder where you want to root your terminal. At the moment, I am using an Ubuntu distribution. I just opened the same HelloPython folder, with the same hello.py file, in the File Explorer of Linux. As shown in Figure A.5:\n\n\n\nFigure A.5: Opening the HelloPython folder in Linux\n\n\nAfter you opened the folder, just click with the right button of your mouse, and select the “Open in Terminal” option, and a new terminal should appear on your screen. See in Figure A.6, that the new terminal is already looking to the HelloPython folder, as we expected.\n\n\n\nFigure A.6: Opening the terminal in Linux\n\n\n\n\nA.0.3 Opening a terminal in MacOs\nUnfortunately, I do not have a Mac machine in my possession, so I cannot easily show you how to open a terminal in MacOs. But there a lot of articles available in the internet discussing how to open a terminal in MacOs. For example, there is a article from the support of Apple1, or this other article from iDownloadBlog2.\n\n\n\n\n\n\nhttps://support.apple.com/en-ie/guide/terminal/apd5265185d-f365-44cb-8b09-71a064a42125/mac↩︎\nhttps://www.idownloadblog.com/2019/04/19/ways-open-terminal-mac/↩︎"
  },
  {
    "objectID": "Chapters/00-install-spark.html#what-are-the-steps",
    "href": "Chapters/00-install-spark.html#what-are-the-steps",
    "title": "Appendix B — How to install Spark and pyspark",
    "section": "B.1 What are the steps?",
    "text": "B.1 What are the steps?\nIn short, the next steps for installing Spark are:\n\nInstall Java;\nInstall Python;\nInstall pyspark;\nDownload and extract Apache Spark;\nSet a few environment variables;\n\nThe next sections describes these steps for each operating system."
  },
  {
    "objectID": "Chapters/00-install-spark.html#on-windows",
    "href": "Chapters/00-install-spark.html#on-windows",
    "title": "Appendix B — How to install Spark and pyspark",
    "section": "B.2 On Windows",
    "text": "B.2 On Windows\n\nB.2.1 Install Java SDK\nApache Spark is written in Scala, which is a fairly modern programming language that have powerful interoperability with the Java programming language. Because of this characteristic, some of the functionalities of Spark require you to have Java SDK (Software Development Kit) installed in your machine.\nIn other words, you must have Java SDK installed to use Spark. If you do not have Java SDK, Spark will likely fail when you try to start it. However, since Java is a very popular tecnology across the world, is possible that you already have it installed in your machine. To check if Java is already installed, you can run the following command in your OS terminal:\n#| eval: false\nTerminal$ java -version\nIf the above command outputs something similar to the text exposed below, than, you already have Java installed in your machine, and you can proceed to the next step.\njava version \"19.0.1\" 2022-10-18\nJava(TM) SE Runtime Environment (build 19.0.1+10-21)\nJava HotSpot(TM) 64-Bit Server VM (build 19.0.1+10-21, mixed mode, sharing)\nBut, if something different comes up in your terminal, than, is likely that you do not have Java installed. To fix this, download Java from the official website1, and install it.\n\n\nB.2.2 Install Python\nYou can easily install Python on Windows, by downloading the installer available at the official website of the language2, and executing it.\n\n\nB.2.3 Install pyspark\nInstalling the pyspark python package is pretty straightforward. Just open a terminal (if you need help to open the terminal check Appendix A), and use the pip command to do it:\n#| eval: false\nTerminal$ pip install pyspark\nIf you try to run the above command (inside a terminal of any OS), and a message like pip: command not found appears, this means that you do not have the pip tool installed on your machine. Hence, if you face this kind of message, you need to install pip before you even install pyspark.\nThe pip tool is automatically installed with Python on Windows. So, if you face this message (pip: command not found), then, is very likely that you do not have Python correctly installed on your machine. Or maybe, Python is not installed at all in any shape or size in your system. So, you should comeback to previous section, and re install it.\n\n\nB.2.4 Download and extract the files of Apache Spark\nFirst, you need to download Apache Spark from the official website of the project3. Currently, the Apache Spark does not offers installers or package programs to install the software for you.\nWe usually install external software on Windows by using installers (i.e. executable files - .exe) that perform all the necessary steps to install the software in your machine. However, currently, the Apache Spark project does not offers such installers. This means that you have to install it yourself.\nWhen you download Apache Spark from the official website, you will get all files of the program compacted inside a TAR file (.tgz), which is similar to a ZIP file (.zip). After you downloaded Spark to your machine, you need to extract all files from the TAR file (.tgz) to a specific location of your computer. It can be anywhere, just choose a place. As an example, I will extract the files to a folder called Spark directly at my hard disk C:/.\nTo extract these files, you can use very popular UI tools like 7zip4 or WinRAR5. However, if you use a modern version of Windows, there is a tar command line tool available at the terminal that you can use to do this process in a programatic fashion. As an example, I can extract and move all files of Spark with these commands:\n#| eval: false\nTerminal$ tar -x -f spark-3.3.1-bin-hadoop3.tgz\nTerminal$ mv spark-3.3.1-bin-hadoop3 C:/Spark/spark-3.3.1-bin-hadoop3\n\n\nB.2.5 Set environment variables\n\n\nApache Spark will always look for two specific environment variables in your system: SPARK_HOME and JAVA_HOME. This means that you must have these two environment variables defined and correctly configured in your machine. To configure an environment variable on Windows, I recommend you to use the menu that you can find by searching for “environment variables” in the Windows search box. At Figure B.1 you can see the path to this menu on Windows:\n\n\n\n\n\n\n\n(a) Path to Windows Environment Variables Menu - Part 1\n\n\n\n\n\n(b) Path to Windows Environment Variables Menu - Part 2\n\n\n\n\n\n(c) Path to Windows Environment Variables Menu - Part 3\n\n\nFigure B.1: Path to Windows Environment Variables Menu\n\n\n\nDuring the process of scheduling and running your Spark application, Spark will execute a set of scripts located at the home directory of Spark itself. Spark does this by looking for a environment variable called SPARK_HOME in your system. If you do not have this variable configured, Spark will not be able to locate its home directory, and as a consequence, it throws an runtime error.\nIn other words, you need to set a variable with name SPARK_HOME in your system. Its value should be the path to the home (or “root”) directory where you installed (or unzipped) the Spark files. In my case I unzipped the files into a folder called C:/Spark/spark-3.3.1-bin-hadoop3, and that is the folder that I going to use in SPARK_HOME, as you can see at Figure B.2:\n\n\n\nFigure B.2: Setting the SPARK_HOME environment variable\n\n\n\n\nNow, a lot Spark functionality is closely related to Java SDK, and to find it, Spark will look for a environment variable called JAVA_HOME. On my machine, I have Java SDK installed at the folder C:\\Program Files\\Java\\jdk-19. This might be not your case, and you may find the “jdk” folder for Java at a different location of your computer. Just find where it is, and use this path while setting this JAVA_HOME variable, like I did at Figure B.3:\n\n\n\nFigure B.3: Setting the JAVA_HOME environment variable\n\n\nWith these two environment variables setted, you should be able to start and use Spark already. Just open a brand new terminal, and type the spark command. An interactive Spark Session will initiate after this command, and give you a command prompt where you can start coding the Spark application you want to execute."
  },
  {
    "objectID": "Chapters/00-install-spark.html#on-linux",
    "href": "Chapters/00-install-spark.html#on-linux",
    "title": "Appendix B — How to install Spark and pyspark",
    "section": "B.3 On Linux",
    "text": "B.3 On Linux\n\nB.3.1 Install pyspark\nIn Linux systems, installing pip is very easy, because you can use the built-in package manager to do this for you. In Debian like distros (e.g. Ubuntu), you use the apt tool, and, in Arch-Linux like distros (e.g. Arch Linux, Manjaro) you would use the pacman tool. Both possibilities are exposed below:\n{terminal, eval = FALSE} # If you are in a Debian like distro of Linux # and need to install `pip`, use this command: Terminal$ sudo apt install python3-pip # If you are in a Arch-Linux like distro of Linux # and need to install `pip`, use this command: Terminal$ pacman -S python-pip"
  }
]
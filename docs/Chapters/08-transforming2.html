<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-99.9.9">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Introduction to pyspark - 8&nbsp; Transforming your Spark DataFrame - Part 2</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../Chapters/09-strings.html" rel="next">
<link href="../Chapters/06-dataframes-sql.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Introduction to <code>pyspark</code></span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="https://pedro-faria.netlify.app/" rel="" target="">
 <span class="menu-text">Visit the author’s blog</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/pedropark99/Introd-pyspark" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
      <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../Chapters/08-transforming2.html"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Transforming your Spark DataFrame - Part 2</span></a></li></ol></nav>
      <a class="flex-grow-1" role="button" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
      </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">About this website</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Chapters/01-intro.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Preface</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Chapters/02-python.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Key concepts of python</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Chapters/03-spark.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Introducing Apache Spark</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Chapters/04-dataframes.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Introducing Spark DataFrames</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Chapters/04-columns.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Introducing the <code>Column</code> class</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Chapters/05-transforming.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Transforming your Spark DataFrame - Part 1</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Chapters/07-import.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Importing data to Spark</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Chapters/06-dataframes-sql.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Working with SQL in <code>pyspark</code></span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Chapters/08-transforming2.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Transforming your Spark DataFrame - Part 2</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Chapters/09-strings.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Tools for string manipulation</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Chapters/references.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">References</span></a>
  </div>
</li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true">
 <span class="menu-text">Appendices</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Chapters/00-terminal.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">A</span>&nbsp; <span class="chapter-title">Opening the terminal of your OS</span></span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../Chapters/00-install-spark.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">B</span>&nbsp; <span class="chapter-title">How to install Spark and <code>pyspark</code></span></span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span class="header-section-number">8.1</span> Introduction</a></li>
  <li><a href="#removing-duplicated-values-from-your-dataframe" id="toc-removing-duplicated-values-from-your-dataframe" class="nav-link" data-scroll-target="#removing-duplicated-values-from-your-dataframe"><span class="header-section-number">8.2</span> Removing duplicated values from your DataFrame</a></li>
  <li><a href="#other-techniques-for-dealing-with-null-values" id="toc-other-techniques-for-dealing-with-null-values" class="nav-link" data-scroll-target="#other-techniques-for-dealing-with-null-values"><span class="header-section-number">8.3</span> Other techniques for dealing with <code>null</code> values</a>
  <ul class="collapse">
  <li><a href="#replacing-null-values" id="toc-replacing-null-values" class="nav-link" data-scroll-target="#replacing-null-values"><span class="header-section-number">8.3.1</span> Replacing <code>null</code> values</a></li>
  <li><a href="#dropping-all-null-values" id="toc-dropping-all-null-values" class="nav-link" data-scroll-target="#dropping-all-null-values"><span class="header-section-number">8.3.2</span> Dropping all <code>null</code> values</a></li>
  </ul></li>
  <li><a href="#applying-union-operations" id="toc-applying-union-operations" class="nav-link" data-scroll-target="#applying-union-operations"><span class="header-section-number">8.4</span> Applying UNION operations</a></li>
  <li><a href="#applying-join-operations" id="toc-applying-join-operations" class="nav-link" data-scroll-target="#applying-join-operations"><span class="header-section-number">8.5</span> Applying JOIN operations</a>
  <ul class="collapse">
  <li><a href="#what-is-a-join" id="toc-what-is-a-join" class="nav-link" data-scroll-target="#what-is-a-join"><span class="header-section-number">8.5.1</span> What is a JOIN ?</a></li>
  <li><a href="#the-different-types-of-join" id="toc-the-different-types-of-join" class="nav-link" data-scroll-target="#the-different-types-of-join"><span class="header-section-number">8.5.2</span> The different types of JOIN</a></li>
  <li><a href="#a-cross-join-as-the-seventh-type" id="toc-a-cross-join-as-the-seventh-type" class="nav-link" data-scroll-target="#a-cross-join-as-the-seventh-type"><span class="header-section-number">8.5.3</span> A cross JOIN as the seventh type</a></li>
  </ul></li>
  <li><a href="#pivot-operations" id="toc-pivot-operations" class="nav-link" data-scroll-target="#pivot-operations"><span class="header-section-number">8.6</span> Pivot operations</a>
  <ul class="collapse">
  <li><a href="#transforming-columns-into-rows" id="toc-transforming-columns-into-rows" class="nav-link" data-scroll-target="#transforming-columns-into-rows"><span class="header-section-number">8.6.1</span> Transforming columns into rows</a></li>
  <li><a href="#transforming-rows-into-columns" id="toc-transforming-rows-into-columns" class="nav-link" data-scroll-target="#transforming-rows-into-columns"><span class="header-section-number">8.6.2</span> Transforming rows into columns</a></li>
  </ul></li>
  <li><a href="#implode-and-explode-operations" id="toc-implode-and-explode-operations" class="nav-link" data-scroll-target="#implode-and-explode-operations"><span class="header-section-number">8.7</span> Implode and explode operations</a></li>
  <li><a href="#more-operations-in-arrays-and-map" id="toc-more-operations-in-arrays-and-map" class="nav-link" data-scroll-target="#more-operations-in-arrays-and-map"><span class="header-section-number">8.8</span> More operations in Arrays and Map</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Transforming your Spark DataFrame - Part 2</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="introduction" class="level2" data-number="8.1">
<h2 data-number="8.1" class="anchored" data-anchor-id="introduction"><span class="header-section-number">8.1</span> Introduction</h2>
<p>At <a href="05-transforming.html"><span>Chapter&nbsp;5</span></a> I introduced six core types of transformations over Spark DataFrames. In this chapter, I will expand your knowledge by introducing five more types of transformations available to Spark DataFrames, which are:</p>
<ul>
<li>Replacing null values;</li>
<li>Removing duplicated values;</li>
<li>Merging multiple DataFrames with UNION operations;</li>
<li>Merging multiple DataFrames with JOIN operations;</li>
<li>Rows to columns with Pivot operations;</li>
<li>Collecting and explode operations;</li>
</ul>
</section>
<section id="removing-duplicated-values-from-your-dataframe" class="level2" data-number="8.2">
<h2 data-number="8.2" class="anchored" data-anchor-id="removing-duplicated-values-from-your-dataframe"><span class="header-section-number">8.2</span> Removing duplicated values from your DataFrame</h2>
<p>Removing duplicated values from DataFrames is a very commom operation in ETL pipelines. In <code>pyspark</code> you have two options to remove duplicated values, which are:</p>
<ul>
<li><code>distinct()</code> which removes all duplicated values considering the combination of all current columns in the DataFrame;</li>
<li><code>drop_duplicates()</code> or <code>dropDuplicates()</code> which removes all duplicated values considering a specific combination (or set) of columns in the DataFrame;</li>
</ul>
<p>These three methods above are all DataFrames methods. Furthermore, the methods <code>drop_duplicates()</code> and <code>dropDuplicates()</code> are equivalent. They both mean the same thing, and have the same arguments and perform the same operation.</p>
<p>When you run <code>drop_duplicates()</code> or <code>dropDuplicates()</code> without any argument, they automatically use by default the combination of all columns available in the DataFrame to identify the duplicated values. As a consequence, over this specific situation, the methods <code>drop_duplicates()</code> or <code>dropDuplicates()</code> become equivalent to the <code>distinct()</code> method. Because they use the combination of all columns in the DataFrame.</p>
<p>Lets pick the <code>supermarket_sales</code> DataFrame exposed below as an example. You can see below, that this DataFrame contains some duplicated values, specifically on the transaction IDs “T001” e “T004”. We also have some “degree of duplication” on the transaction ID “T006”. But the two rows describing this ID “T006” are not precisely identical, since they have a small difference on the <code>quantity</code> column.</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.sql.types <span class="im">import</span> (</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    StructType, StructField,</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    StringType, IntegerType,</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    FloatType</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>schema <span class="op">=</span> StructType([</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>    StructField(<span class="st">"transaction_id"</span>, StringType(), <span class="va">True</span>),</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>    StructField(<span class="st">"product_name"</span>, StringType(), <span class="va">True</span>),</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>    StructField(<span class="st">"quantity"</span>, IntegerType(), <span class="va">True</span>),</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>    StructField(<span class="st">"price"</span>, FloatType(), <span class="va">True</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>])</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> [</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"T001"</span>, <span class="st">"Apple"</span>, <span class="dv">5</span>, <span class="fl">1.2</span>),</span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"T001"</span>, <span class="st">"Apple"</span>, <span class="dv">5</span>, <span class="fl">1.2</span>),</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"T002"</span>, <span class="st">"Banana"</span>, <span class="dv">3</span>, <span class="fl">0.8</span>),</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"T004"</span>, <span class="st">"Mango"</span>, <span class="dv">2</span>, <span class="fl">2.0</span>),</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"T004"</span>, <span class="st">"Mango"</span>, <span class="dv">2</span>, <span class="fl">2.0</span>),</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"T004"</span>, <span class="st">"Mango"</span>, <span class="dv">2</span>, <span class="fl">2.0</span>),</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"T005"</span>, <span class="st">"Grapes"</span>, <span class="dv">1</span>, <span class="fl">3.5</span>),</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"T006"</span>, <span class="st">"Apple"</span>, <span class="dv">2</span>, <span class="fl">1.2</span>),</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"T006"</span>, <span class="st">"Apple"</span>, <span class="dv">1</span>, <span class="fl">1.2</span>),</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"T007"</span>, <span class="st">"Banana"</span>, <span class="dv">4</span>, <span class="fl">0.8</span>),</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>    (<span class="st">"T008"</span>, <span class="st">"Apple"</span>, <span class="dv">3</span>, <span class="fl">1.2</span>)</span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>supermarket_sales <span class="op">=</span> spark.createDataFrame(data, schema)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>supermarket_sales.show(<span class="dv">6</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+--------------+------------+--------+-----+
|transaction_id|product_name|quantity|price|
+--------------+------------+--------+-----+
|          T001|       Apple|       5|  1.2|
|          T001|       Apple|       5|  1.2|
|          T002|      Banana|       3|  0.8|
|          T004|       Mango|       2|  2.0|
|          T004|       Mango|       2|  2.0|
|          T004|       Mango|       2|  2.0|
+--------------+------------+--------+-----+
only showing top 6 rows
</code></pre>
</div>
</div>
<p>We can remove these duplicated values by using the <code>distinct()</code> method. In the example of transaction ID “T004”, all duplicated rows of this ID contains the same values <code>("T004", "Mango", 2, 2.0)</code>, precisely in this order. Because of that, the <code>distinct()</code> method is enough to remove all of these duplicated values from the table.</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>supermarket_sales<span class="op">\</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    .distinct()<span class="op">\</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    .show(<span class="dv">6</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+--------------+------------+--------+-----+
|transaction_id|product_name|quantity|price|
+--------------+------------+--------+-----+
|          T001|       Apple|       5|  1.2|
|          T002|      Banana|       3|  0.8|
|          T004|       Mango|       2|  2.0|
|          T005|      Grapes|       1|  3.5|
|          T006|       Apple|       2|  1.2|
|          T006|       Apple|       1|  1.2|
+--------------+------------+--------+-----+
only showing top 6 rows
</code></pre>
</div>
</div>
<p>However, the two rows describing the transaction ID “T006” have some difference on the <code>quantity</code> column, and as a result, the <code>distinct()</code> method does not identify these two rows as “duplicated values”, and they are not removed from the input DataFrame.</p>
<p>Now, if we needed a DataFrame that contained one row for each transaction ID (that is, the values on <code>transaction_id</code> column must be unique), we could use the <code>drop_duplicates()</code> method with only the column <code>transaction_id</code> as the key to remove all duplicated values of this column. This way, we get a slightly different output as you can see below.</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>supermarket_sales<span class="op">\</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    .drop_duplicates([<span class="st">'transaction_id'</span>])<span class="op">\</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>    .show(<span class="dv">8</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+--------------+------------+--------+-----+
|transaction_id|product_name|quantity|price|
+--------------+------------+--------+-----+
|          T001|       Apple|       5|  1.2|
|          T002|      Banana|       3|  0.8|
|          T004|       Mango|       2|  2.0|
|          T005|      Grapes|       1|  3.5|
|          T006|       Apple|       2|  1.2|
|          T007|      Banana|       4|  0.8|
|          T008|       Apple|       3|  1.2|
+--------------+------------+--------+-----+
</code></pre>
</div>
</div>
<p>In the example above, the duplicated values of IDs “T001” and “T004” were removed as we expected. But we also removed the second value for ID “T006”. Because we did not listed the <code>quantity</code> column on <code>drop_duplicates()</code>, and, as a result, the <code>drop_duplicates()</code> method was not concerned with the differences on the <code>quantity</code> column. In other words, it used solely the <code>transaction_id</code> column to identify the duplicated values.</p>
</section>
<section id="other-techniques-for-dealing-with-null-values" class="level2" data-number="8.3">
<h2 data-number="8.3" class="anchored" data-anchor-id="other-techniques-for-dealing-with-null-values"><span class="header-section-number">8.3</span> Other techniques for dealing with <code>null</code> values</h2>
<p>At <a href="05-transforming.html#sec-filter-null-values"><span>Section&nbsp;5.6.5</span></a> I showed how you can use <code>filter()</code> or <code>where()</code> DataFrame methods to remove all rows that contained a null value on some column. There are two other DataFrames methods available in Spark that you might want use to deal with null values. In essence, you can either remove or replace these null values.</p>
<section id="replacing-null-values" class="level3" data-number="8.3.1">
<h3 data-number="8.3.1" class="anchored" data-anchor-id="replacing-null-values"><span class="header-section-number">8.3.1</span> Replacing <code>null</code> values</h3>
<p>Instead of removing the null values, and pretending that they never existed, maybe, you prefer to replace these null values by a more useful or representative value, such as <code>0</code>, or an empty string (<code>''</code>), or a <code>False</code> value, etc. To do that in <code>pyspark</code>, we can use the <code>na.fill()</code> and <code>fillna()</code> DataFrame methods.</p>
<p>Both methods mean the same thing, and they work the exact same way. The most popular way of using this methods, is to provide a python dict as input. Inside this dict you have key-value pairs, where the key represents the column name, and the value represents the static value that will replace all null values that are found on the column specified by the key.</p>
<p>In the example below, I created a simple <code>df</code> DataFrame which contains some null values on the <code>age</code> column. By providing the dict <code>{'age': 0}</code> to <code>fillna()</code>, I am asking <code>fillna()</code> to replace all null values found on the <code>age</code> column by the value <code>0</code> (zero).</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> [</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">1</span>, <span class="st">"John"</span>, <span class="va">None</span>, <span class="st">"2023-04-05"</span>),</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">2</span>, <span class="st">"Alice"</span>, <span class="dv">25</span>, <span class="st">"2023-04-09"</span>),</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">3</span>, <span class="st">"Bob"</span>, <span class="va">None</span>, <span class="st">"2023-04-12"</span>),</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">4</span>, <span class="st">"Jane"</span>, <span class="dv">30</span>, <span class="va">None</span>),</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">5</span>, <span class="st">"Mike"</span>, <span class="dv">35</span>, <span class="va">None</span>)</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>columns <span class="op">=</span> [<span class="st">"id"</span>, <span class="st">"name"</span>, <span class="st">"age"</span>, <span class="st">"date"</span>]</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> spark.createDataFrame(data, columns)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Or `df.na.fill({'age': 0}).show()`</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="co"># It is the same thing</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>df.fillna({<span class="st">'age'</span>: <span class="dv">0</span>}).show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+---+-----+---+----------+
| id| name|age|      date|
+---+-----+---+----------+
|  1| John|  0|2023-04-05|
|  2|Alice| 25|2023-04-09|
|  3|  Bob|  0|2023-04-12|
|  4| Jane| 30|      null|
|  5| Mike| 35|      null|
+---+-----+---+----------+
</code></pre>
</div>
</div>
<p>You can see in the above example, that the null values present in the <code>date</code> column were maintained intact on the result. Because we did not asked to <code>fillna()</code> to replace the values of this column, by including it on the input dict that we provided.</p>
<p>If we do include this <code>date</code> column on the input dict, then, <code>fillna()</code> will take care of this column as well:</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>df.fillna({<span class="st">'age'</span>: <span class="dv">0</span>, <span class="st">'date'</span>: <span class="st">'2023-01-01'</span>})<span class="op">\</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>    .show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+---+-----+---+----------+
| id| name|age|      date|
+---+-----+---+----------+
|  1| John|  0|2023-04-05|
|  2|Alice| 25|2023-04-09|
|  3|  Bob|  0|2023-04-12|
|  4| Jane| 30|2023-01-01|
|  5| Mike| 35|2023-01-01|
+---+-----+---+----------+
</code></pre>
</div>
</div>
</section>
<section id="dropping-all-null-values" class="level3" data-number="8.3.2">
<h3 data-number="8.3.2" class="anchored" data-anchor-id="dropping-all-null-values"><span class="header-section-number">8.3.2</span> Dropping all <code>null</code> values</h3>
<p>Spark also offers the <code>na.drop()</code> and <code>dropna()</code> DataFrames methods, which you can use to easily remove any row that contains a null value on any column of the DataFrame. This is different from <code>filter()</code> and <code>where()</code>, because on these two methods you have to build a logical expression that translate “not-null values”.</p>
<p>In contrast, on <code>na.drop()</code> and <code>dropna()</code> methods you do not have a logical expression. You just call these methods, and they do the heavy work for you. They search trough the entire DataFrame. When it identify a null value on the DataFrame, it removes the entire row that contains such null value.</p>
<p>For example, if we apply these methods on the <code>df</code> DataFrame that we used on the previous section, this is the end result:</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>df.na.drop()<span class="op">\</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    .show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+---+-----+---+----------+
| id| name|age|      date|
+---+-----+---+----------+
|  2|Alice| 25|2023-04-09|
+---+-----+---+----------+
</code></pre>
</div>
</div>
</section>
</section>
<section id="applying-union-operations" class="level2" data-number="8.4">
<h2 data-number="8.4" class="anchored" data-anchor-id="applying-union-operations"><span class="header-section-number">8.4</span> Applying UNION operations</h2>
<p>When you have many individual DataFrames that have the same columns, and you want to unify them into a single big DataFrame that have all the rows from these different DataFrames, you want to perform an UNION operation.</p>
<p>An UNION operation works on a pair of DataFrames. It returns the row-wise union of these two DataFrames. In <code>pyspark</code>, we perform UNION operations by using the <code>union()</code> DataFrame method. To use this method, you just provide the other DataFrame you want to make the union with. So the expression <code>df1.union(df2)</code> creates a new DataFrame which contains all the rows from both the <code>df1</code> and <code>df2</code> DataFrames.</p>
<p>In commom SQL engines there are usually two kinds of UNION operations, which are: <em>union all</em> and <em>union distinct</em>. When you use an <em>union all</em> operation, you saying that you just want to unifiy the two DataFrames, no matter what data is in each one of them. You do not care if duplicated values are generated in the process, because an observation “x” might be present both on <code>df1</code> and <code>df2</code>.</p>
<p>In contrast, an <em>union distinct</em> operation is the exact opposite of that. It merges the rows from both DataFrames together, and then, it removes all duplicated values from the result. So you use an <em>union distinct</em> operation when you want a single DataFrame that contains all rows from both DataFrames <code>df1</code> and <code>df2</code>, but, you do not want any duplicated rows into this single DataFrame.</p>
<p>By default, the <code>union()</code> method always perform an <em>union all</em> operation. However, to do an <em>union distinct</em> operation in pyspark, you actually have to use the <code>union()</code> method in conjunction with the <code>distinct()</code> or <code>drop_duplicates()</code> methods. In other words, there is not a direct method in pyspark that performs an <em>union distinct</em> operation on a single command.</p>
<p>Look at the example below with <code>df1</code> and <code>df2</code> DataFrames.</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>df1 <span class="op">=</span> [</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">1</span>, <span class="st">'Anne'</span>, <span class="st">'F'</span>),</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">5</span>, <span class="st">'Mike'</span>, <span class="st">'M'</span>),</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">2</span>, <span class="st">'Francis'</span>, <span class="st">'M'</span>),</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>df2 <span class="op">=</span> [</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">5</span>, <span class="st">'Mike'</span>, <span class="st">'M'</span>),</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">7</span>, <span class="st">'Arthur'</span>, <span class="st">'M'</span>),</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">1</span>, <span class="st">'Anne'</span>, <span class="st">'F'</span>),</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>df1 <span class="op">=</span> spark.createDataFrame(df1, [<span class="st">'ID'</span>, <span class="st">'Name'</span>, <span class="st">'Sex'</span>])</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>df2 <span class="op">=</span> spark.createDataFrame(df2, [<span class="st">'ID'</span>, <span class="st">'Name'</span>, <span class="st">'Sex'</span>])</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a><span class="co"># An example of UNION ALL operation:</span></span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>df1.union(df2).show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+---+-------+---+
| ID|   Name|Sex|
+---+-------+---+
|  1|   Anne|  F|
|  5|   Mike|  M|
|  2|Francis|  M|
|  5|   Mike|  M|
|  7| Arthur|  M|
|  1|   Anne|  F|
+---+-------+---+
</code></pre>
</div>
</div>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># An example of UNION DISTINCT operation</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>df1<span class="op">\</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>    .union(df2)<span class="op">\</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>    .distinct()<span class="op">\</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>    .show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+---+-------+---+
| ID|   Name|Sex|
+---+-------+---+
|  1|   Anne|  F|
|  5|   Mike|  M|
|  2|Francis|  M|
|  7| Arthur|  M|
+---+-------+---+
</code></pre>
</div>
</div>
<p>Because an UNION operation merges the two DataFrames in a vertical way, the columns between the two DataFrames must match. If the columns between the two DataFrames are not in the same places, a mismatch happens during the operation, and Spark will do nothing to fix your mistake.</p>
<p>Most programming languages would issue an error at this point, warning you about this conflict between the columns found on each DataFrame and their respective positions. However, in Spark, if the columns are out of order, Spark will continue with the UNION operation, as if nothing was wrong. Spark will not even raise a warning for you. Since this problem can easily pass unnotice, be aware of it.</p>
<p>In the example below, we have a third DataFrame called <code>df3</code>. Notice that the columns in <code>df3</code> are the same of <code>df1</code> and <code>df2</code>. However, the columns from <code>df3</code> are in a different order than in <code>df1</code> and <code>df2</code>.</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> [</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'Marla'</span>, <span class="st">'F'</span>, <span class="dv">9</span>),</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'Andrew'</span>, <span class="st">'M'</span>, <span class="dv">15</span>),</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'Peter'</span>, <span class="st">'M'</span>, <span class="dv">12</span>)</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>df3 <span class="op">=</span> spark.createDataFrame(data, [<span class="st">'Name'</span>, <span class="st">'Sex'</span>, <span class="st">'ID'</span>])</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a>df3.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+------+---+---+
|  Name|Sex| ID|
+------+---+---+
| Marla|  F|  9|
|Andrew|  M| 15|
| Peter|  M| 12|
+------+---+---+
</code></pre>
</div>
</div>
<p>If we try to perform an UNION operation between, let’s say, <code>df2</code> and <code>df3</code>, the operations just works. But, the end result of this operation is not correct, as you can see in the example below.</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>df2.union(df3).show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+------+------+---+
|    ID|  Name|Sex|
+------+------+---+
|     5|  Mike|  M|
|     7|Arthur|  M|
|     1|  Anne|  F|
| Marla|     F|  9|
|Andrew|     M| 15|
| Peter|     M| 12|
+------+------+---+
</code></pre>
</div>
</div>
<p>Although this might be problematic, Spark provides an easy-to-use solution when the columns are in different places between each DataFrame. This solution is the <code>unionByName()</code> method.</p>
<p>The difference between <code>union()</code> and <code>unionByName()</code> methods, is that the <code>unionByName()</code> method makes an matching by column name, before if performs the UNION. In other words, it compares the column names found on each DataFrame and it matches each column by its name. This way, the columns present on each DataFrame of the UNION must have the same name, but they do not need to be in the same positions on both DataFrames.</p>
<p>If we use this method on the same example as above, you can see below that we get a different result, and a correct one this time.</p>
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>df2.unionByName(df3)<span class="op">\</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    .show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+---+------+---+
| ID|  Name|Sex|
+---+------+---+
|  5|  Mike|  M|
|  7|Arthur|  M|
|  1|  Anne|  F|
|  9| Marla|  F|
| 15|Andrew|  M|
| 12| Peter|  M|
+---+------+---+
</code></pre>
</div>
</div>
<p>Therefore, if you want to make an UNION operation between two DataFrames, you can generally use the <code>union()</code> method. But if you suspect the columns from these DataFrames might be in different positions on each DataFrame, you can change to the <code>unionByName()</code> method.</p>
<p>In contrast, if the columns are different not only on position, but also, on column name, then, <code>unionByName()</code> will not work. The two DataFrames involved on an UNION operation must be very similar. If they are not similar, then, you will have a hard time trying to do the operation.</p>
<p>Another problem that you might face is if you try to unify two DataFrames that have different numbers of columns between them. In this situation, it means that the two DataFrames have “different widths”, and, as a result of that, an <code>AnalysisException</code> error will be raised by Spark if you try to unify them with an UNION operation, like in the example below:</p>
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.sql.types <span class="im">import</span> (</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>    StructField,</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>    StructType,</span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>    LongType</span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>schema <span class="op">=</span> StructType([StructField(<span class="st">'ID'</span>, LongType(), <span class="va">False</span>)])</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>df4 <span class="op">=</span> [</span>
<span id="cb23-9"><a href="#cb23-9" aria-hidden="true" tabindex="-1"></a>    (<span class="dv">19</span>,), (<span class="dv">17</span>,), (<span class="dv">16</span>,)</span>
<span id="cb23-10"><a href="#cb23-10" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb23-11"><a href="#cb23-11" aria-hidden="true" tabindex="-1"></a>df4 <span class="op">=</span> spark.createDataFrame(df4, schema)</span>
<span id="cb23-12"><a href="#cb23-12" aria-hidden="true" tabindex="-1"></a>df3.union(df4).show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<pre><code>AnalysisException: Union can only be performed on tables
with the same number of columns, but the first table has
3 columns and the second table has 1 columns;
'Union false, false
:- LogicalRDD [Name#703, Sex#704, ID#705L], false
+- LogicalRDD [ID#762L], false</code></pre>
</section>
<section id="applying-join-operations" class="level2" data-number="8.5">
<h2 data-number="8.5" class="anchored" data-anchor-id="applying-join-operations"><span class="header-section-number">8.5</span> Applying JOIN operations</h2>
<p>A JOIN operation is another very commom operation that is also used to bring data from scattered sources into a single unified DataFrame. In pyspark, we can build JOIN operations by using the <code>join()</code> DataFrame method. This method accepts three arguments, which are:</p>
<ul>
<li><code>other</code>: the DataFrame you want to JOIN with (i.e.&nbsp;the DataFrame on the right side of the JOIN);</li>
<li><code>on</code>: a column name, or a list of column names, that represents the key (or keys) of the JOIN;</li>
<li><code>how</code>: the kind of JOIN you want to perform (inner, full, left, right);</li>
</ul>
<p>As a first example, let’s use the <code>info</code> and <code>band_instruments</code> DataFrames. With the source code below, you can quickly re-create these two DataFrames in your session:</p>
<div class="cell" data-execution_count="14">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>info <span class="op">=</span> [</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'Mick'</span>, <span class="st">'Rolling Stones'</span>, <span class="st">'1943-07-26'</span>, <span class="va">True</span>),</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'John'</span>, <span class="st">'Beatles'</span>, <span class="st">'1940-09-10'</span>, <span class="va">True</span>),</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'Paul'</span>, <span class="st">'Beatles'</span>, <span class="st">'1942-06-18'</span>, <span class="va">True</span>),</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'George'</span>, <span class="st">'Beatles'</span>, <span class="st">'1943-02-25'</span>, <span class="va">True</span>),</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'Ringo'</span>, <span class="st">'Beatles'</span>, <span class="st">'1940-07-07'</span>, <span class="va">True</span>)</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>info <span class="op">=</span> spark.createDataFrame(</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>    info,</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>    [<span class="st">'name'</span>, <span class="st">'band'</span>, <span class="st">'born'</span>, <span class="st">'children'</span>]</span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-14"><a href="#cb25-14" aria-hidden="true" tabindex="-1"></a>band_instruments <span class="op">=</span> [</span>
<span id="cb25-15"><a href="#cb25-15" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'John'</span>, <span class="st">'guitar'</span>),</span>
<span id="cb25-16"><a href="#cb25-16" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'Paul'</span>, <span class="st">'bass'</span>),</span>
<span id="cb25-17"><a href="#cb25-17" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'Keith'</span>, <span class="st">'guitar'</span>)</span>
<span id="cb25-18"><a href="#cb25-18" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb25-19"><a href="#cb25-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-20"><a href="#cb25-20" aria-hidden="true" tabindex="-1"></a>band_instruments <span class="op">=</span> spark.createDataFrame(</span>
<span id="cb25-21"><a href="#cb25-21" aria-hidden="true" tabindex="-1"></a>    band_instruments,</span>
<span id="cb25-22"><a href="#cb25-22" aria-hidden="true" tabindex="-1"></a>    [<span class="st">'name'</span>, <span class="st">'plays'</span>]</span>
<span id="cb25-23"><a href="#cb25-23" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>If you look closely to these two DataFrames, you will probably notice that they both describe musicians from two famous rock bands from 60’s and 70’s. The <code>info</code> DataFrame have more personal or general informations about the musicians, while the <code>band_instruments</code> DataFrame have only data about the main musical instruments that they play.</p>
<div class="cell" data-execution_count="15">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>info.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+------+--------------+----------+--------+
|  name|          band|      born|children|
+------+--------------+----------+--------+
|  Mick|Rolling Stones|1943-07-26|    true|
|  John|       Beatles|1940-09-10|    true|
|  Paul|       Beatles|1942-06-18|    true|
|George|       Beatles|1943-02-25|    true|
| Ringo|       Beatles|1940-07-07|    true|
+------+--------------+----------+--------+
</code></pre>
</div>
</div>
<div class="cell" data-execution_count="16">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>band_instruments.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+-----+------+
| name| plays|
+-----+------+
| John|guitar|
| Paul|  bass|
|Keith|guitar|
+-----+------+
</code></pre>
</div>
</div>
<p>It might be of your interest, to have a single DataFrame that contains both the personal information and the musical instrument of each musician. In this case, you can build a JOIN operation between these DataFrames to get this result. An example of this JOIN in pyspark would be:</p>
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>info.join(band_instruments, on <span class="op">=</span> <span class="st">'name'</span>, how <span class="op">=</span> <span class="st">'left'</span>)<span class="op">\</span></span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>    .show(<span class="dv">5</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+------+--------------+----------+--------+------+
|  name|          band|      born|children| plays|
+------+--------------+----------+--------+------+
|  Mick|Rolling Stones|1943-07-26|    true|  null|
|  John|       Beatles|1940-09-10|    true|guitar|
|  Paul|       Beatles|1942-06-18|    true|  bass|
|George|       Beatles|1943-02-25|    true|  null|
| Ringo|       Beatles|1940-07-07|    true|  null|
+------+--------------+----------+--------+------+
</code></pre>
</div>
</div>
<p>In the example above, we are performing a <em>left join</em> between the two DataFrames, using the <code>name</code> column as the JOIN key. Now, we have a single DataFrame with all 5 columns from both DataFrames (<code>plays</code>, <code>children</code>, <code>name</code>, <code>band</code> and <code>born</code>).</p>
<section id="what-is-a-join" class="level3" data-number="8.5.1">
<h3 data-number="8.5.1" class="anchored" data-anchor-id="what-is-a-join"><span class="header-section-number">8.5.1</span> What is a JOIN ?</h3>
<p>I imagine you are already familiar with JOIN operations. However, in order to build good and precise JOIN operations, is very important to know what a JOIN operation actually is. So let’s revisit it.</p>
<p>A JOIN operation merges two different DataFrames together into a single unified DataFrame. It does this by using a column (or a set of columns) as keys to identify the observations of both DataFrames, and connects these observations together.</p>
<p>A JOIN (like UNION) is also an operation that works on a pair of DataFrames. It is very commom to refer to this pair as “the sides of the JOIN”. That is, the DataFrame on the left side of the JOIN, and the DataFrame on the right side of the JOIN. Or also, the DataFrames “A” (left side) and “B” (right side).</p>
<p>The main idea (or objective) of the JOIN is to bring all data from the DataFrame on the right side, into the DataFrame on the left side. In other words, a JOIN between DataFrames A and B results into a DataFrame C which contains all columns and rows from both DataFrames A and B.</p>
<p>In an UNION operation, both DataFrames must have the same columns, because in an UNION operation you are concatenating both DataFrames together vertically, so the number of columns (or the “width” of the tables) need to match. However, in a JOIN operation, both DataFrames only need to have at least one column in commom. Apart from that, in a JOIN, both DataFrames can have very different structure and columns from each other.</p>
<p>One key characteristic of JOIN operations is it’s key matching mechanism. A JOIN uses the columns you provide to <strong>build a key</strong>. This key is used to identify rows (or “observations”) in both DataFrames. In other words, these keys identifies relationships between the two DataFrames. These relations are vital to the JOIN.</p>
<p>If we go back to <code>info</code> and <code>band_instruments</code> DataFrames, and analyse them for a bit more, we can see that they both have a <code>name</code> column which contains the name of the musician being described on the current row. This <code>name</code> column can be used as <strong>the key</strong> of the JOIN. Because this column is available on both DataFrames, and it can be used to identify a single observation (or a single musician) present in each DataFrame.</p>
<p>So the JOIN key is a column (or a combination of columns) that can identify what observations are (and are not) present on both DataFrames. At <a href="#fig-keys-comparison">Figure&nbsp;<span>8.1</span></a>, we can see the observations from <code>info</code> and <code>band_instruments</code> in a visual manner. You see in the figure that both Paul and John are described in both DataFrames. At the same time, Ringo, Mick and George are present only on <code>info</code>, while <code>Keith</code> is only at <code>band_instruments</code>.</p>
<div id="fig-keys-comparison" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="./../Figures/keys_comparacao.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;8.1: The relations between <code>info</code> and <code>band_instruments</code> DataFrames</figcaption><p></p>
</figure>
</div>
<p>In a certain way, you can see the JOIN key as a way to <strong>identify relationships between the two DataFrames</strong>. A JOIN operation use these relationships to merge your DataFrames in a precise way. A JOIN does not simply horizontally glue two DataFrames together. It uses the JOIN key to perform a matching process between the observations of the two DataFrames.</p>
<p>This matching process ensures that the data present DataFrame “B” is correctly transported to the DataFrame “A”. In other words, it ensures that the oranges are paired with oranges, apples with apples, bananas with bananas, you got it.</p>
<p>Just to describe visually what this matching process is, we have the <a href="#fig-join-matching">Figure&nbsp;<span>8.2</span></a> below. In this figure, we have two DataFrames on the left and center of the image, which represents the inputs of the JOIN. We also have a third DataFrame on the right side of the image, which is the output (or the result) of the JOIN.</p>
<p>In this specific example, the column that represents the JOIN key is the <code>ID</code> column. Not only this column is present on both DataFrames, but it also represents an unique identifier to each person described in both tables. And that is precisely the job of a JOIN key. It represents a way to identify observations (or “persons”, or “objects”, etc.) on both tables.</p>
<p>You can see at <a href="#fig-join-matching">Figure&nbsp;<span>8.2</span></a>, that when the <code>ID</code> 100 is found on the 1st row of the left DataFrame, the JOIN initiates a lookup/matching process on the center DataFrame, looking for a row in the DataFrame that matches this <code>ID</code> 100. When it finds this <code>ID</code> 100 (on the 4th row of the center DataFrame), it captures and connects these two rows on both DataFrames, because these rows describes the same person (or observation), and because of that, they should be connected. This same matching process happens for all remaining <code>ID</code> values.</p>
<div id="fig-join-matching" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="./../Figures/pareamento1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;8.2: The matching process of a JOIN operation</figcaption><p></p>
</figure>
</div>
</section>
<section id="the-different-types-of-join" class="level3" data-number="8.5.2">
<h3 data-number="8.5.2" class="anchored" data-anchor-id="the-different-types-of-join"><span class="header-section-number">8.5.2</span> The different types of JOIN</h3>
<p>JOIN operations actually comes in different flavours (or types). The four main known types of JOINs are: <em>full</em>, <em>left</em>, <em>right</em> and <em>inner</em>. All of these different types of JOIN perform the same steps and matching processes that we described on the previous section. But they differ on the treatment they do to unmatched observations. In other words, these different types of JOINs differ on <strong>what they do in cases when an observation is not found on both DataFrames of the JOIN</strong> (e.g.&nbsp;when an observation is found only on table A).</p>
<p>In other words, all these four types will perform the same matching process between the two DataFrames, and will connect observations that are found in both DataFrames. However, which rows are included in the final output is what changes between each type (or “flavour”) of JOIN.</p>
<p>In this situation, the words “left” and “right” are identifiers to the DataFrames involved on the JOIN operation. That is, the word <em>left</em> refers to the DataFrame on the left side of the JOIN, while the word <em>right</em> refers to the DataFrame on the right side of the JOIN.</p>
<p>A very useful way of understanding these different types of JOINs is to represent both DataFrames as numerical sets (as we learn in mathematics). The <a href="#fig-join-sets">Figure&nbsp;<span>8.3</span></a> gives you a visual representation of each type of JOIN using this “set model” of representing JOINs. Remember, all of these different types of JOIN work the same way, they just do different actions when an observation is not found on both tables.</p>
<p>The most “complete” and “greedy” type of JOIN is the <em>full join</em>. Because this type returns all possible combinations of both DataFrames. In other words, this type of JOIN will result in a DataFrame that have all observations from both DataFrames. It does not matter if an observation is present only on table A, or only on table B, or maybe, on both tables. A <em>full join</em> will always try to connect as much observation as it can.</p>
<div id="fig-join-sets" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="./../Figures/join-sets.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;8.3: A visual representation for types of JOIN using numerical sets</figcaption><p></p>
</figure>
</div>
<p>That is why the <em>full join</em> is represented on <a href="#fig-join-sets">Figure&nbsp;<span>8.3</span></a> as the union between the two tables (or the two sets). In contrast, an <em>inner join</em> is the intersection of the two tables (or two sets). That is, an <em>inner join</em> will result in a new DataFrame which contains solely the observations that could be found on both tables. If a specific observation is found only on one table of the JOIN, this observation will be automatically removed from the result of the <em>inner join</em>.</p>
<p>If we go back to the <code>info</code> and <code>band_instruments</code> DataFrames, and use them as an example, you can see that only Paul and John are included on the result of an <em>inner join</em>. While in a <em>full join</em>, all musicians are included on the resulting DataFrame.</p>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="co"># An inner join between `info` and `band_instruments`:</span></span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>info.join(band_instruments, on <span class="op">=</span> <span class="st">'name'</span>, how <span class="op">=</span> <span class="st">'inner'</span>)<span class="op">\</span></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a>    .show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+----+-------+----------+--------+------+
|name|   band|      born|children| plays|
+----+-------+----------+--------+------+
|John|Beatles|1940-09-10|    true|guitar|
|Paul|Beatles|1942-06-18|    true|  bass|
+----+-------+----------+--------+------+
</code></pre>
</div>
</div>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># A full join between `info` and `band_instruments`:</span></span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>info.join(band_instruments, on <span class="op">=</span> <span class="st">'name'</span>, how <span class="op">=</span> <span class="st">'full'</span>)<span class="op">\</span></span>
<span id="cb34-3"><a href="#cb34-3" aria-hidden="true" tabindex="-1"></a>    .show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+------+--------------+----------+--------+------+
|  name|          band|      born|children| plays|
+------+--------------+----------+--------+------+
|George|       Beatles|1943-02-25|    true|  null|
|  John|       Beatles|1940-09-10|    true|guitar|
| Keith|          null|      null|    null|guitar|
|  Mick|Rolling Stones|1943-07-26|    true|  null|
|  Paul|       Beatles|1942-06-18|    true|  bass|
| Ringo|       Beatles|1940-07-07|    true|  null|
+------+--------------+----------+--------+------+
</code></pre>
</div>
</div>
<p>On the other hand, the <em>left join</em> and <em>right join</em> are kind of self-explanatory. On a <em>left join</em>, all the observations from the left DataFrame are kept intact on the resulting DataFrame of the JOIN, regardless of whether these observations were found or not on the right DataFrame. In contrast, an <em>right join</em> is the opposite of that. So, all observations from the right DataFrame are kept intact on the resulting DataFrame of the JOIN.</p>
<p>In pyspark, you can define the type of JOIN you want to use by setting the <code>how</code> argument at <code>join()</code> method. This argument accepts a string with the type of JOIN you want to use as input.</p>
<ul>
<li><code>how = 'left'</code>: make a <em>left join</em>;</li>
<li><code>how = 'right'</code>: make a <em>right join</em>;</li>
<li><code>how = 'full'</code>: make a <em>full join</em>;</li>
<li><code>how = 'inner'</code>: make an <em>inner join</em>;</li>
<li><code>how = 'semi'</code>: make a <em>semi join</em>;</li>
<li><code>how = 'anti'</code>: make an <em>anti join</em>;</li>
</ul>
<p>You can see on the list above, that <code>pyspark</code> do have two more types of JOINs, which are the <em>semi join</em> and <em>anti join</em>. These are “filtering types” of JOINs. Because they perform the matching process, and only filter the rows from table A (i.e.&nbsp;the DataFrame on the left side of the JOIN) based on the matches found on table B (i.e.&nbsp;the DataFrame on the right side of the JOIN).</p>
<p>In other words, these both types are used as a filter mechanism, and not as a merge mechanism. When you use these two types, instead of merging two DataFrames together, you are interested in filtering the rows of DataFrame A based on the existence of these rows in DataFrame B.</p>
<p>This is different from what we learned on <em>left</em>, <em>right</em>, <em>full</em> and <em>inner</em> types, because they do not only change which rows are included in the final result, but they also add the columns from table B into table A. Because of this behavior, these four main types are usually called as “additive types” of JOIN, since they are always adding data from table B into table A, i.e.&nbsp;they are merging the two tables together.</p>
<p>In more details, an <em>anti join</em> perform the exact opposite matching process of an <em>inner join</em>. This means that an <em>anti join</em> will always result in a new DataFrame that contains solely the observations that exists only on one DataFrame of the JOIN. In other words, the observations that are found on both tables are automatically removed from the resulting DataFrame of the JOIN. If we look at the example below, we can see that both John and Paul were removed from the resulting DataFrame of the <em>anti join</em>, because these two musicians are present on both DataFrames:</p>
<div class="cell" data-execution_count="20">
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>info.join(band_instruments, on <span class="op">=</span> <span class="st">'name'</span>, how <span class="op">=</span> <span class="st">'anti'</span>)<span class="op">\</span></span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>    .show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+------+--------------+----------+--------+
|  name|          band|      born|children|
+------+--------------+----------+--------+
|  Mick|Rolling Stones|1943-07-26|    true|
|George|       Beatles|1943-02-25|    true|
| Ringo|       Beatles|1940-07-07|    true|
+------+--------------+----------+--------+
</code></pre>
</div>
</div>
<p>In contrast, a <em>semi join</em> is equivalent to an <em>inner join</em>, with the difference that it does not adds the column from table B into table A. So this type of JOIN filter the rows from DataFrame A that also exists in DataFrame B. If an observation is found on both tables, this observation will appear on the resulting DataFrame.</p>
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1" aria-hidden="true" tabindex="-1"></a>info.join(band_instruments, on <span class="op">=</span> <span class="st">'name'</span>, how <span class="op">=</span> <span class="st">'semi'</span>)<span class="op">\</span></span>
<span id="cb38-2"><a href="#cb38-2" aria-hidden="true" tabindex="-1"></a>    .show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+----+-------+----------+--------+
|name|   band|      born|children|
+----+-------+----------+--------+
|John|Beatles|1940-09-10|    true|
|Paul|Beatles|1942-06-18|    true|
+----+-------+----------+--------+
</code></pre>
</div>
</div>
<p>Just to keep using our visual model of sets, on <a href="#fig-join-sets2">Figure&nbsp;<span>8.4</span></a> you can see the <em>semi</em> and <em>anti</em> JOIN types represented as numerical sets.</p>
<div id="fig-join-sets2" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="./../Figures/join-sets2.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;8.4: The two “filter types” of JOIN</figcaption><p></p>
</figure>
</div>
</section>
<section id="a-cross-join-as-the-seventh-type" class="level3" data-number="8.5.3">
<h3 data-number="8.5.3" class="anchored" data-anchor-id="a-cross-join-as-the-seventh-type"><span class="header-section-number">8.5.3</span> A cross JOIN as the seventh type</h3>
<p>We described six different types of JOINs on the previous section. But Spark also offers a seventh type of JOIN called <em>cross join</em>. This is a special type of JOIN that you can use by calling the <code>crossJoin()</code> DataFrame method.</p>
<p>In essence, a <em>cross join</em> returns, as output, the cartesian product between two DataFrames. It is similar to R functions <a href="https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/expand.grid"><code>base::expand.grid()</code></a> or <a href="https://tidyr.tidyverse.org/reference/expand.html"><code>dplyr::expand()</code></a>, and also, the Python equivalent <a href="https://docs.python.org/2/library/itertools.html#itertools.product"><code>itertools.product()</code></a>.</p>
<p>This is a type of JOIN that you should avoid to use, specially if one (or both) of the DataFrames involved is a big DataFrame with thousands/millions of rows. Because a <em>cross join</em> will always produce a cartesian product between the two DataFrames involved. This means that, if DataFrame A contains <span class="math inline">\(x\)</span> rows, and DataFrame B contains <span class="math inline">\(y\)</span> rows, the end result of the <em>cross join</em> is a new DataFrame C that contains <span class="math inline">\(x \times y\)</span> rows.</p>
<p>In other words, the number of rows in the output of a <em>cross join</em> can grow exponentially. For example, a <em>cross join</em> between a DataFrame of 1 thousand rows, and another DataFrame of 10 thousand of rows (both are small DataFrames for the scale and sizes of a real-world big data environment), would produce a DataFrame with <span class="math inline">\(10^3 \times 10^4 = 10^7\)</span>, that is, 10 milion of rows as output.</p>
<p>In a big data environment, dealing with something that grows exponentially… it is never a good idea. So try to avoid a <em>cross join</em> and use him solely on very small DataFrames.</p>
<p>As an example, to apply a <em>cross join</em> between <code>info</code> and <code>band_instruments</code> DataFrames we can use the <code>crossJoin()</code> method, like in the example below:</p>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a>info.crossJoin(band_instruments)<span class="op">\</span></span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a>    .show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stderr">
<pre><code>[Stage 260:============&gt;                                        (24 + 12) / 100][Stage 260:=================&gt;                                   (33 + 12) / 100]</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>[Stage 260:====================&gt;                                (39 + 12) / 100][Stage 260:=========================&gt;                           (49 + 12) / 100]</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>[Stage 260:===============================&gt;                     (60 + 12) / 100][Stage 260:=====================================&gt;               (70 + 12) / 100]</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>[Stage 260:==========================================&gt;          (80 + 12) / 100][Stage 260:==============================================&gt;      (87 + 12) / 100]</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>                                                                                </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>+------+--------------+----------+--------+-----+------+
|  name|          band|      born|children| name| plays|
+------+--------------+----------+--------+-----+------+
|  Mick|Rolling Stones|1943-07-26|    true| John|guitar|
|  Mick|Rolling Stones|1943-07-26|    true| Paul|  bass|
|  Mick|Rolling Stones|1943-07-26|    true|Keith|guitar|
|  John|       Beatles|1940-09-10|    true| John|guitar|
|  John|       Beatles|1940-09-10|    true| Paul|  bass|
|  John|       Beatles|1940-09-10|    true|Keith|guitar|
|  Paul|       Beatles|1942-06-18|    true| John|guitar|
|  Paul|       Beatles|1942-06-18|    true| Paul|  bass|
|  Paul|       Beatles|1942-06-18|    true|Keith|guitar|
|George|       Beatles|1943-02-25|    true| John|guitar|
|George|       Beatles|1943-02-25|    true| Paul|  bass|
|George|       Beatles|1943-02-25|    true|Keith|guitar|
| Ringo|       Beatles|1940-07-07|    true| John|guitar|
| Ringo|       Beatles|1940-07-07|    true| Paul|  bass|
| Ringo|       Beatles|1940-07-07|    true|Keith|guitar|
+------+--------------+----------+--------+-----+------+
</code></pre>
</div>
</div>
<p>A <em>cross join</em> is a special type of JOIN because it does not use “keys” and a matching process. It just computes every possible combination between the rows from both DataFrames. Because of the absence of these keys characteristics of a JOIN, many data analysts and engineers would not call a <em>cross join</em> as a type of JOIN (in other words, they would call it a type of something else). But regardless of our opinions, Spark decided to call this process as the <em>cross join</em>, so this is the way we are calling this process on this book.</p>
</section>
</section>
<section id="pivot-operations" class="level2" data-number="8.6">
<h2 data-number="8.6" class="anchored" data-anchor-id="pivot-operations"><span class="header-section-number">8.6</span> Pivot operations</h2>
<p>Pivot operations are extremely useful, and they are probably the main operation you can use to completely reformat your table. What these operations do is basically change the dimensions of your table, or in other words, these operations seek to transform columns into rows, or vice versa.</p>
<p>As a parallel with other frameworks, a pivot operation in Spark is the same operation performed by R functions <a href="https://tidyr.tidyverse.org/reference/pivot_longer.html"><code>tidyr::pivot_longer()</code></a> and <a href="https://tidyr.tidyverse.org/reference/pivot_wider.html"><code>tidyr::pivot_wider()</code></a> from the famous R framework <code>tidyverse</code>; or, the same as the <a href="https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.pivot.html"><code>pivot()</code></a> and <a href="https://pandas.pydata.org/docs/reference/api/pandas.melt.html"><code>melt()</code></a> methods from the Python framework <code>pandas</code>.</p>
<p>In Spark, pivot operations are performed by the <code>pivot()</code> DataFrame method, and by the <code>stack()</code> Spark SQL function. Pivot transformations are available in both directions. That is, you can transform either rows into columns (corresponds to <code>pivot()</code>), or, columns into rows (corresponds to <code>stack()</code>). Let’s begin with <code>stack()</code>, and after that, we explain the <code>pivot()</code> method.</p>
<section id="transforming-columns-into-rows" class="level3" data-number="8.6.1">
<h3 data-number="8.6.1" class="anchored" data-anchor-id="transforming-columns-into-rows"><span class="header-section-number">8.6.1</span> Transforming columns into rows</h3>
<p>The <code>stack()</code> Spark SQL function allows you to transform columns into rows. In other words, you can make your DataFrame “longer” with this kind of operation, because you remove columns (“width”) from the table, and adds new rows (“heigth”). This gives an aspect of “longer” to your table, because after this operation, you table usually have more rows than columns.</p>
<p>As a first example, lets use the <code>religion</code> DataFrame, which you can re-create in yuour session with the source code below:</p>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb47"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb47-1"><a href="#cb47-1" aria-hidden="true" tabindex="-1"></a>data <span class="op">=</span> [</span>
<span id="cb47-2"><a href="#cb47-2" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'Agnostic'</span>, <span class="dv">27</span>, <span class="dv">34</span>, <span class="dv">60</span>),</span>
<span id="cb47-3"><a href="#cb47-3" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'Atheist'</span>, <span class="dv">12</span>, <span class="dv">27</span>, <span class="dv">37</span>),</span>
<span id="cb47-4"><a href="#cb47-4" aria-hidden="true" tabindex="-1"></a>    (<span class="st">'Buddhist'</span>, <span class="dv">27</span>, <span class="dv">21</span>, <span class="dv">30</span>)</span>
<span id="cb47-5"><a href="#cb47-5" aria-hidden="true" tabindex="-1"></a>]</span>
<span id="cb47-6"><a href="#cb47-6" aria-hidden="true" tabindex="-1"></a>cols <span class="op">=</span> [<span class="st">'religion'</span>, <span class="st">'&lt;$10k'</span>, <span class="st">'$10k-$20k'</span>, <span class="st">'$20k-$30k'</span>]</span>
<span id="cb47-7"><a href="#cb47-7" aria-hidden="true" tabindex="-1"></a>religion <span class="op">=</span> spark.createDataFrame(data, cols)</span>
<span id="cb47-8"><a href="#cb47-8" aria-hidden="true" tabindex="-1"></a>religion.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+--------+-----+---------+---------+
|religion|&lt;$10k|$10k-$20k|$20k-$30k|
+--------+-----+---------+---------+
|Agnostic|   27|       34|       60|
| Atheist|   12|       27|       37|
|Buddhist|   27|       21|       30|
+--------+-----+---------+---------+
</code></pre>
</div>
</div>
<p>This DataFrame is showing us the average salary of people belonging to different religions. See that in each column of this DataFrame, you have data for a specific salary level (or range). This is a structure that can be easy and intuitive at times, but it also might impose some limitations, specially if you need to apply a vectorised operation over these salary ranges.</p>
<p>The basic unit of this DataFrame are the religious groups, and the salary range represents a characteristic of these groups. The different salary levels are distributed across different columns. But what if we transformed these multiple columns into multiple rows? How can we accomplish that?</p>
<p>What we need to do, is to concentrate the labels (or the column names) of salary levels into a single column, and move the respective values of the salary levels into another column. In other words, we need to create a column that contains the labels, and another column that contains the values. At <a href="#fig-pivot1">Figure&nbsp;<span>8.5</span></a> we show a visual representation of this process:</p>
<div id="fig-pivot1" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="./../Figures/pivot1.png" class="img-fluid figure-img"></p>
<p></p><figcaption class="figure-caption">Figure&nbsp;8.5: A visual representation of a pivot operation</figcaption><p></p>
</figure>
</div>
<p>Lets build this transformation in <code>pyspark</code>. First, remember that <code>stack()</code> is not a DataFrame method. It is a Spark SQL function. However, it is not an exported Spark SQL function, which means that you cannot import this function from the <code>pyspark.sql.function</code> module. This means that <code>stack()</code> will never be directly available in your python session to use.</p>
<p>So how do you use it? The answer is: use it inside Spark SQL! The <code>stack()</code> function is not available directly in python, but it is always available in Spark SQL, so all you need to do, is to use <code>stack()</code> inside functions and methods such as <code>expr()</code> (that I introduced at <a href="06-dataframes-sql.html#sec-sql-expr"><span>Section&nbsp;7.7.2</span></a>), or <code>sql()</code> to access Spark SQL functionality.</p>
<p>Now, the <code>stack()</code> function have two main arguments, which are the number of columns to transform into rows, and a sequence of key-value pairs that describes which columns will be transformed into rows, and the label values that corresponds to each column being transformed.</p>
<p>As a first example, the source code below replicates the transformation exposed at <a href="#fig-pivot1">Figure&nbsp;<span>8.5</span></a>:</p>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb49"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb49-1"><a href="#cb49-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.sql.functions <span class="im">import</span> expr</span>
<span id="cb49-2"><a href="#cb49-2" aria-hidden="true" tabindex="-1"></a>stack_expr <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb49-3"><a href="#cb49-3" aria-hidden="true" tabindex="-1"></a><span class="st">stack(3,</span></span>
<span id="cb49-4"><a href="#cb49-4" aria-hidden="true" tabindex="-1"></a><span class="st">    '&lt;$10k', `&lt;$10k`,</span></span>
<span id="cb49-5"><a href="#cb49-5" aria-hidden="true" tabindex="-1"></a><span class="st">    '$10k-$20k', `$10k-$20k`,</span></span>
<span id="cb49-6"><a href="#cb49-6" aria-hidden="true" tabindex="-1"></a><span class="st">    '$20k-$30k', `$20k-$30k`</span></span>
<span id="cb49-7"><a href="#cb49-7" aria-hidden="true" tabindex="-1"></a><span class="st">) AS (salary_range, avg_salary)</span></span>
<span id="cb49-8"><a href="#cb49-8" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb49-9"><a href="#cb49-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-10"><a href="#cb49-10" aria-hidden="true" tabindex="-1"></a>longer_religion <span class="op">=</span> religion<span class="op">\</span></span>
<span id="cb49-11"><a href="#cb49-11" aria-hidden="true" tabindex="-1"></a>    .select(<span class="st">'religion'</span>, expr(stack_expr))</span>
<span id="cb49-12"><a href="#cb49-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb49-13"><a href="#cb49-13" aria-hidden="true" tabindex="-1"></a>longer_religion.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+--------+------------+----------+
|religion|salary_range|avg_salary|
+--------+------------+----------+
|Agnostic|       &lt;$10k|        27|
|Agnostic|   $10k-$20k|        34|
|Agnostic|   $20k-$30k|        60|
| Atheist|       &lt;$10k|        12|
| Atheist|   $10k-$20k|        27|
| Atheist|   $20k-$30k|        37|
|Buddhist|       &lt;$10k|        27|
|Buddhist|   $10k-$20k|        21|
|Buddhist|   $20k-$30k|        30|
+--------+------------+----------+
</code></pre>
</div>
</div>
<p>An important aspect about the <code>stack()</code> function, is that it always outputs two new columns (one column for the labels - or the keys, and another for the values). In the example above, these new columns are <code>salary_range</code> and <code>avg_salary</code>.</p>
<p>The first column identifies from which column (before the <code>stack()</code> operation) the value present at <code>avg_salary</code> came from. This means that this first column produced by <code>stack()</code> works as a column of labels or identifiers. These labels identify from which of the three transformed columns (<code>&lt;$10k</code>, <code>$10k-$20k</code> and <code>$20k-$30k</code>) the row value came from. In the visual representation exposed at <a href="#fig-pivot1">Figure&nbsp;<span>8.5</span></a>, this “labels column” is the <code>income</code> column.</p>
<p>In contrast, the second column in the output of <code>stack()</code> contains the actual values that were present on the columns that were transformed. This “values column” in the example above corresponds to the column <code>avg_salary</code>, while in the visual representation exposed at <a href="#fig-pivot1">Figure&nbsp;<span>8.5</span></a>, it is the <code>values</code> column.</p>
<p>The first argument in <code>stack()</code> is always the number of columns that will be transformed by the function into rows. In our example, we have three columns that we want to transform, which are <code>&lt;$10k</code>, <code>$10k-$20k</code> and <code>$20k-$30k</code>. That is why we have the number 3 as the first argument to <code>stack()</code>.</p>
<p>After that, we have a sequence of key-value pairs. In each pair, the value side (i.e.&nbsp;the right side) of the pair contains the name of the column that will be transformed, and the key side (i.e.&nbsp;the left side) of the pair contains the “label value”, or, in other words, which value represents, marks, label, or identifies the values that came from the column described in the right side of the pair.</p>
<p>Normally, you set the label value to be equivalent to the column name. That is, both sides of each pair are usually pretty much the same. But you can change this behaviour if you want. In the example below, all values that came from the <code>&lt;$10k</code> are labeled as <code>"Below $10k"</code>, while the values from the <code>$10k-$20k</code> column, are labeled in the output as <code>"Between $10k-$20k"</code>, etc.</p>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb51"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb51-1"><a href="#cb51-1" aria-hidden="true" tabindex="-1"></a>stack_expr <span class="op">=</span> <span class="st">"""</span></span>
<span id="cb51-2"><a href="#cb51-2" aria-hidden="true" tabindex="-1"></a><span class="st">stack(3,</span></span>
<span id="cb51-3"><a href="#cb51-3" aria-hidden="true" tabindex="-1"></a><span class="st">    'Below $10k', `&lt;$10k`,</span></span>
<span id="cb51-4"><a href="#cb51-4" aria-hidden="true" tabindex="-1"></a><span class="st">    'Between $10k-$20k', `$10k-$20k`,</span></span>
<span id="cb51-5"><a href="#cb51-5" aria-hidden="true" tabindex="-1"></a><span class="st">    'Between $20k-$30k', `$20k-$30k`</span></span>
<span id="cb51-6"><a href="#cb51-6" aria-hidden="true" tabindex="-1"></a><span class="st">) AS (salary_range, avg_salary)</span></span>
<span id="cb51-7"><a href="#cb51-7" aria-hidden="true" tabindex="-1"></a><span class="st">"""</span></span>
<span id="cb51-8"><a href="#cb51-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb51-9"><a href="#cb51-9" aria-hidden="true" tabindex="-1"></a>religion<span class="op">\</span></span>
<span id="cb51-10"><a href="#cb51-10" aria-hidden="true" tabindex="-1"></a>    .select(<span class="st">'religion'</span>, expr(stack_expr))<span class="op">\</span></span>
<span id="cb51-11"><a href="#cb51-11" aria-hidden="true" tabindex="-1"></a>    .show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+--------+-----------------+----------+
|religion|     salary_range|avg_salary|
+--------+-----------------+----------+
|Agnostic|       Below $10k|        27|
|Agnostic|Between $10k-$20k|        34|
|Agnostic|Between $20k-$30k|        60|
| Atheist|       Below $10k|        12|
| Atheist|Between $10k-$20k|        27|
| Atheist|Between $20k-$30k|        37|
|Buddhist|       Below $10k|        27|
|Buddhist|Between $10k-$20k|        21|
|Buddhist|Between $20k-$30k|        30|
+--------+-----------------+----------+
</code></pre>
</div>
</div>
<p>Furthermore, because the <code>stack()</code> function always outputs two new columns, if you want to rename these two new columns being created, to give them more readable and meaningful names, you always need to provide two new column names at once, inside a tuple, to the <code>AS</code> keyword.</p>
<p>In the example above, this tuple is <code>(salary_range, avg_salary)</code>. The first value in the tuple is the new name for the “labels column”, while the second value in the tuple, is the new name for the “values column”.</p>
<p>Now, differently from other Spark SQL functions, the <code>stack()</code> function should not be used inside a <code>withColumn()</code>, and the reason for this is very simple: <code>stack()</code> always returns two new columns as output, but the <code>withColumn()</code> method can only create one column at a time.</p>
<p>This is why you get an <code>AnalysisException</code> error when you try to use <code>stack()</code> inside <code>withColumn()</code>, like in the example below:</p>
<div class="cell" data-execution_count="26">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1" aria-hidden="true" tabindex="-1"></a>religion<span class="op">\</span></span>
<span id="cb53-2"><a href="#cb53-2" aria-hidden="true" tabindex="-1"></a>    .withColumn(<span class="st">'salary_ranges'</span>, stack_expr)<span class="op">\</span></span>
<span id="cb53-3"><a href="#cb53-3" aria-hidden="true" tabindex="-1"></a>    .show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<pre><code>AnalysisException: The number of aliases supplied in the AS clause
does not match the number of columns output by the UDTF expected
2 aliases but got salary_ranges </code></pre>
</section>
<section id="transforming-rows-into-columns" class="level3" data-number="8.6.2">
<h3 data-number="8.6.2" class="anchored" data-anchor-id="transforming-rows-into-columns"><span class="header-section-number">8.6.2</span> Transforming rows into columns</h3>
<p>On the other side, if you want to transform rows into columns on your Spark DataFrame, you can use the <code>pivot()</code> method. One key aspect of the <code>pivot()</code> method, is that it must always be used in conjunction with the <code>groupby()</code> method that we introduced at <a href="05-transforming.html#sec-group-by"><span>Section&nbsp;5.11.4</span></a>. In other words, <code>pivot()</code> does not work without <code>groupby()</code>.</p>
<p>You can think (or interpret) that the <code>groupby()</code> method does the job of defining (or identifying) which columns will be present in the output of <code>pivot()</code>. For example, if your DataFrame contains five columns, which are <code>A</code>, <code>B</code>, <code>C</code>, <code>D</code> and <code>E</code>; but you only listed columns <code>A</code> and <code>C</code> inside <code>groupby()</code>, this means that if you perform a pivot operation after that, the columns <code>B</code>, <code>D</code> and <code>E</code> will not be present in the output of <code>pivot()</code>. These three columns will be automatically dropped during the pivot operation.</p>
<p>In contrast, the <code>pivot()</code> method does the job of identifying a single column containing the values that will be transformed into new columns. In other words, if you list the column <code>car_brands</code> inside <code>pivot()</code>, and, this column contains four unique values, for example, <code>Audi</code>, <code>BMW</code>, <code>Jeep</code> and <code>Fiat</code>, this means that, in the output of <code>pivot()</code>, four new columns will be created, named as <code>Audi</code>, <code>BMW</code>, <code>Jeep</code> and <code>Fiat</code>.</p>
<p>Now that we know how we define the columns that will be kept in the output of the pivot operation, and, how to mark the column that contains the rows that will be transformed into new columns, we must learn how to specify how these rows will be agrouped during the pivot operation. And for that, we need to use an aggregating function.</p>
<p>This is really important, you can not do a pivot operation without aggregating the values you are converting into columns. Without it, Spark will not let you do the pivot operation.</p>
<p>As an example, we can return to the <code>religion</code> DataFrame. More specifically, to the <code>longer_religion</code> DataFrame, which is the pivoted version of the <code>religion</code> DataFrame that we created on the previous section, using the <code>stack()</code> function.</p>
<div class="cell" data-execution_count="27">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1" aria-hidden="true" tabindex="-1"></a>longer_religion.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+--------+------------+----------+
|religion|salary_range|avg_salary|
+--------+------------+----------+
|Agnostic|       &lt;$10k|        27|
|Agnostic|   $10k-$20k|        34|
|Agnostic|   $20k-$30k|        60|
| Atheist|       &lt;$10k|        12|
| Atheist|   $10k-$20k|        27|
| Atheist|   $20k-$30k|        37|
|Buddhist|       &lt;$10k|        27|
|Buddhist|   $10k-$20k|        21|
|Buddhist|   $20k-$30k|        30|
+--------+------------+----------+
</code></pre>
</div>
</div>
<p>We can use this <code>longer_religion</code> and the <code>pivot()</code> method to perform the inverse operation described at <a href="#fig-pivot1">Figure&nbsp;<span>8.5</span></a>. In other words, we can re-create the <code>religion</code> DataFrame trough <code>longer_religion</code>. The source code below demonstrates how we could do such thing:</p>
<div class="cell" data-execution_count="28">
<div class="sourceCode cell-code" id="cb57"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb57-1"><a href="#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pyspark.sql.functions <span class="im">import</span> first</span>
<span id="cb57-2"><a href="#cb57-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb57-3"><a href="#cb57-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Equivalent to the `religion` DataFrame:</span></span>
<span id="cb57-4"><a href="#cb57-4" aria-hidden="true" tabindex="-1"></a>longer_religion<span class="op">\</span></span>
<span id="cb57-5"><a href="#cb57-5" aria-hidden="true" tabindex="-1"></a>    .groupby(<span class="st">'religion'</span>)<span class="op">\</span></span>
<span id="cb57-6"><a href="#cb57-6" aria-hidden="true" tabindex="-1"></a>    .pivot(<span class="st">'salary_range'</span>)<span class="op">\</span></span>
<span id="cb57-7"><a href="#cb57-7" aria-hidden="true" tabindex="-1"></a>    .agg(first(<span class="st">'avg_salary'</span>))<span class="op">\</span></span>
<span id="cb57-8"><a href="#cb57-8" aria-hidden="true" tabindex="-1"></a>    .show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>+--------+---------+---------+-----+
|religion|$10k-$20k|$20k-$30k|&lt;$10k|
+--------+---------+---------+-----+
|Agnostic|       34|       60|   27|
|Buddhist|       21|       30|   27|
| Atheist|       27|       37|   12|
+--------+---------+---------+-----+
</code></pre>
</div>
</div>
</section>
</section>
<section id="implode-and-explode-operations" class="level2" data-number="8.7">
<h2 data-number="8.7" class="anchored" data-anchor-id="implode-and-explode-operations"><span class="header-section-number">8.7</span> Implode and explode operations</h2>
</section>
<section id="more-operations-in-arrays-and-map" class="level2" data-number="8.8">
<h2 data-number="8.8" class="anchored" data-anchor-id="more-operations-in-arrays-and-map"><span class="header-section-number">8.8</span> More operations in Arrays and Map</h2>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../Chapters/06-dataframes-sql.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Working with SQL in <code>pyspark</code></span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../Chapters/09-strings.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Tools for string manipulation</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>